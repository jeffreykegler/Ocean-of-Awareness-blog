What are the reasonable computer languages?
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <blockquote>
    "You see things; and you say 'Why?' But I dream things that never were; and I say 'Why not?'"
    -- <a href="http://www.bartleby.com/73/465.html">George Bernard Shaw</a>
    </blockquote>
    <p>
      In the 1960's and 1970's computer languages were evolving rapidly.
      It was not clear which way they were headed.
      Would most programming be done with
      general-purpose languages?
      Or would programmers create a language for every task domain?
      Or even for every project?
      And, if lots of languages were going to be created,
      what kinds of languages would be needed?
    </p>
    <p>
      It was in that context that &#268;ulik and Cohen,
      in
      <a href="http://www.sciencedirect.com/science/article/pii/S0022000073800509">a 
      1973 paper</a>,
      outlined what they thought programmers would want and should have.
      In keeping with the spirit of the time,
      it was quite a lot:
      <ul>
      <li>Programmers would want to extend their grammars with new syntax,
      including new
      kinds of expressions.</li>
      <li>Programmers would also want to use tools that automatically generated new syntax.</li>
      <li>Programmers would not want to, and especially
      in the case of auto-generated syntax
      would usually not be able to,
      massage the syntax into very restricted forms.
      Instead, programmers would create grammars and languages
      which required unlimited lookahead to disambiguate,
      and they would require parsers which could handle these grammars.</li>
      <li>Finally, programmers would need to be able to rely on
      all of this parsing being done in linear time.</li>
      </ul>
      Today, we think we know that
      &#268;ulik and Cohen's vision was naive,
      because we think we know that parsing technology cannot support it.
      We think we know that parsing is much harder than they thought.
    </p>
    <h2>The eyeball grammars</h2>
    <p>As a thought problem, consider the "eyeball" class of grammars.
      The "eyeball" class of grammars contains all the grammars that a human
      can parse at a glance.
      If a grammar is in the eyeball class,
      but a computer cannot parse it,
      it presents an interesting choice.  Either,
    </p><ul>
      <li>your computer is not using the strongest practical algorithm; or
      </li><li>your mind is using some power which cannot be reduced to a machine computation.
      </li></ul><p>
      There are some people out there (I am one of them)
      who don't believe that everything the mind can do reduces
      to a machine computation.
      But even those people
      will tend to go for the choice in this case:
      There must be some practical computer parsing algorithm which
      can do at least as well at parsing as a human
      can do by "eyeball".
      In other words, the class of "reasonable grammars" should
      contain the eyeball class.
    </p>
    <p>
      &#268;ulik and Cohen's candidate for the class of "reasonable grammars"
      were the grammars that
      a deterministic parse engine
      could parse if it had a lookahead that was infinite,
      but restricted to distinguishing between regular expressions.
      They called these the LR-regular, or LRR, grammars.
      And the LRR grammars
      <b>do</b>
      in fact seem to be a good first approximation
      to the eyeball class.
      They do not allow lookahead that contains things
      that you have to count, like palindromes.
      And, while I'd be hard put to eyeball every possible string for every possible regular expression,
      intuitively the concept of scanning for a regular expression
      does seem close to capturing the idea of glancing through a text looking for a telltale pattern.
    </p>
    <h2>So what happened?</h2>
    <p>
      Alas, the algorithm in the &#268;ulik and Cohen paper turned out to be impractical.
      But in 1991, Joop Leo discovered a way to adopt Earley's algorithm to parse the LRR grammars
      in linear time, without doing the lookahead.
      And Leo's algorithm does have a practical implementation:
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">Marpa</a>.
    </p>
    <h2>References, comments, etc.</h2>
    <p>
      To learn more about Marpa,
      there's
      <a href="http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
