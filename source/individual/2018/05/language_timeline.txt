A timeline of the parsing term "language"
<html>
  <head>
  </head>
  <body>
    <p>The term "language" has a special and unusual meaning within
    Parsing Theory, one it inherited from a now nearly-extinct
    school of linguistics.
    In a previous post,
    I said that this would turn out to be more than
    just a matter of textbooks definition falling short of ideal,
    but would, because of its major effect on Parsing Theory,
    shape the daily practice of programming.
    This post supplies the evidence.
    </p>
    <p>
    This post takes the form of a timeline<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    The earlier entires in this timeline borrow heavily from
    <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html">
    a previous blog post</a>.
    </p>
    <h2>"Language" as of 1929</h2>
    <p>In 1929 Leonard Bloomfield,
    as part of his effort to put American linguistics on a rigorous footing,
    publishes his "Postulates".
    They include his definition of language:
    <blockquote>
    The totality of utterances that can be made in a speech
    community is the <b>language</b>
    of that speech-community.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    </blockquote>
    There is no reference in this definition to the more usual view,
    that the utterances of a language "mean" something.
    This omission is not accidental:
    <blockquote>
    The statement of meanings is therefore the weak point in
    language-study, and will remain so until human knowledge
    advances very far beyond its present state. In practice, we define the
    meaning of a linguistic form, wherever we can, in terms of some
    other science.<a id="footnote-3-ref" href="#footnote-3">[3]</a>
    </blockquote>
    Bloomfield is passing the buck in this way,
    because he wants linguistics to become regarded as a science,
    and the positivist science of his time abhors
    unverifiable statements,
    and therefore any claims about mental states.
    A claim to know what someone "means" can be read
    as a claim to know what's in their mind,
    and mind-reading has a poor reputation among positivists.
    </p>
    <p>
    Draconian as the exclusion of meaning is,
    it is a big success.
    Known as structural linguistics,
    Bloomfieldian linguistics dominates lingustics for
    the next couple of decades.
    </p>
    <h2>1955: Noam Chomsky graduates</h2>
    <p>
    Noam Chomsky earns his PhD at the Universtity of Pennsylvania.
    His teacher, Zelig Harris, is a prominent Bloomfieldian,
    and Chomsky's early work is thought to be in the Bloomfield school.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    Chomsky becomes a professor at MIT.
    MIT does not have a linguistics department,
    and Chomsky is free to teach his own approach to the subject.
    </p>
    <h2>The term "language" as of 1956</h2>
    <p>Chomsky publishes his "Three models" paper,
    one of the most important papers of all time.
    His definition of language now uses the terminology
    of set theory,
    but its substance comes from Bloomfield:
    <blockquote>
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of sysbols.  If A is an alphabet, we shall say that
      anything formed by concatenating the symbols of A is a string in
      A. By a grammar of the language L we mean a device of some sort that
      produces all of the strings that are sentences of L and only these.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </blockquote>
    Chomsky does not intend to
    follow in the Bloomfieldian tradition of avoiding considerations
    of meaning, aka semantics:
    <blockquote>
    [...] it would be absurd to develop
    a general syntactic theory
    without assigning an absolutely
    crucial role to semantic considerations,
    since obviously the necessity to support
    semantic interpretation is one of the primary
    requirements
    that the structures
    generated by the syntactic component of a grammar
    must meet.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </blockquote>
    Already in "Three Models",
    Chomsky brings in semantics,
    when it is useful to show
    that his model is superior:
    When an utterance is ambiguous,
    Chomsky's new model produces multiple derivations
    each of which "look" like the natural representation
    of one of the meanings.
    Chomsky points out that this is a very
    desirable property for a model to have.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </p>
    <h2>Oettinger 1961</h2>
    <p>
    While the stack itself goes back to Turing<a id="footnote-8-ref" href="#footnote-8">[8]</a>,
    its significance for parsing becomes an object
    of interest in itself with
    Samuelson and Bauer's 1959 paper<a id="footnote-9-ref" href="#footnote-9">[9]</a>.
    Mathematical study of stacks as models of computing begins with Anthony Oettinger's 1961 paper.<a id="footnote-10-ref" href="#footnote-10">[10]</a>
    </blockquote>
    Oettinger hopes his pushdown store model of computing --
    what will eventually be called
    deterministic pushdown automata (DPDA's) --
    will become the basis of a theory of language
    computing encompassing both natural language
    (including the Russian which is the object of his own research)
    and computing languages like ALGOL.
    For natural language translation,
    DPDA's will prove totally inadequate.
    But DPDA's will continute to be the basis
    of hopes for a theory of computer language parsing.
    And the quote above more than hints at an expectation
    of One Stack Parsing Algorithm to Rule Them All.
    </p>
    <h2>Knuth 1965</h2>
    <p>It is with expectations like those of Oettinger 1961 in mind
    that Donald Knuth publishes his pivotal LR(k) paper.
    </p>
    <p>
    Knuth is also aware that DPDA determinism and
    linear time behavior are not the same thing<a id="footnote-11-ref" href="#footnote-11">[11]</a>
    While all DPDA's are indeed linear in the length of their input,
    many linear algorithms have more power than DPDA's.
    </p>
    <h2>Comments, etc.</h2>
    <p>
      The background material for this post is in my
    <a href="https://jeffreykegler.github.io/personal/timeline_v3>
    Parsing: a timeline 3.0</a>,
    and this post may be considered a supplement to "Timelime".
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
    It is intended to be incorporated in my
    <a href="https://jeffreykegler.github.io/personal/timeline_v3>
    Parsing: a timeline</a>.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
    Bloomfield, Leonard,
    "A set of Postulates
    for the Science of Language",
    <cite>Language</cite>, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
    The quote is definition 4 on p. 154.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
    Bloomfield, Leonard.
    <cite>Language</cite>.
    Holt, Rinehart and Winston, 1933, p. 140.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
	Harris, Randy Allen,
	<cite>The Linguistics Wars</cite>,
	Oxford University Press, 1993,
	pp 31-34, p. 37.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
      The quote is on p. 114 of
      Chomsky, Noam.
      "Three models for the description of language."
      <cite>IRE Transactions on information theory</cite>,
      vol. 2, issue 3, September 1956, pp. 113-124.
      In case there is any doubt Chomsky's "strings"
      are Bloomfield's utterances,
      Chomsky also calls his strings,
      "utterances".
      For example in Chomsky, Noam, <cite>Syntactic Structures</cite>,
    2nd ed.,
    Mouton de Gruyter, 2002, on p. 15:
    "Any grammar of a language will project the finite and somewhat accidental
    corpus of observed utterances to a set (presumably infinite) 
    of grammatical utterances."
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
    Chomsky, Noam.
    <cite>Topics in the Theory of Generative Grammar</cite>.
    De Gruyter, 1978, p. 20.
    (The quote occurs in footnote 7 starting on p. 19.)
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
    Chomsky 1956, p. 118, p. 123.
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
    Carpenter, Brian E., and Robert W. Doran.
    "The other Turing machine."
    <cite>The Computer Journal</cite>, vol. 20, issue 3, 1 January 1977, pp. 269-279.
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9">9.
    Samelson, Klaus, and Friedrich L. Bauer. "Sequentielle formel√ºbersetzung." it-Information Technology 1.1-4 (1959): 176-182.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10">10.
    <footnote>
    Oettinger, Anthony.
    "Automatic Syntactic Analysis and the Pushdown Store"
    <cite>Proceedings of Symposia in Applied Mathematics</cite>,
    Volume 12,
    American Mathematical Society, 1961.
    </p>
    <p>Oettinger 1961 is full of evidence that stacks
    (which he calls "pushdown stores") are very new.
    Oettinger, for example, does not use the terms "push" or "pop",
    but instead describes operations on his pushdown stores using
    a set of vector operations which will later form the basis
    of the APL language.
    As of 1961, all algorithms with acceptable speed are using
    stacks with various modifications.
    Oettinger express a hope:
    </p>
    <blockquote>
    The development of a theory of pushdown algorithms should
    hopefully lead to systematic techniques for generating
    algorithms satisfying given requirements to replace
    the ad hoc invention of each new algorithm.<footnote>
    Oettinger 1961, p. 127.
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11">11.
    Knuth 1965, p. 607: "execution time at worst
    proportional to the length of the string being parsed."
 <a href="#footnote-11-ref">&#8617;</a></p>
  </body>
</html>
