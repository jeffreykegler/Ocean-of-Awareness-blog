A timeline of parsing
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p><b>1960</b>:
      The ALGOL 60 spec comes out.
      It specifies, for the first time, a block structured
      language.
      The ALGOL committee is well aware
      that
      nobody knows how to parse such a language.
      But they believe that if they specify a block-structured
      language, a parser for it will be invented.
      Risky as this approach is, it pays off ...
    </p>
    <p><b>1961</b>: Ned Irons publishes his ALGOL parser.
      In fact, the Irons parser
      is the first parser of any kind to be described
      in print.
      Ned's algorithm was a form of recursive descent,
      but unlike modern
      recursive descent,
      the Irons algorithm
      is general and syntax-driven.
      "General" means it can parse anything written in BNF.
      "Syntax-driven" (aka declarative) means that parser is
      actually created from the BNF -- an Irons parser does not need
      to be hand-written.
    </p>
    <p><b>1965</b>: Don Knuth invents LR parsing.
      Knuth is primarily interested
      in the mathematics.
      He describes a parsing algorithm,
      but it is not thought practical.
    </p>
    <p><b>1968</b>: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is also syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's core idea was to
      track everything about the parse in tables.
      Earley's algorithm is enticing, but it has three major issues:
      <ul>
      <li>First, there is a bug in the handling of null productions.
      <li>Second, it is quadratic for right recursions.
      <li>Third, the bookkeeping required to set up the tables is,
      by the standards of 1968 hardware, daunting.
      </ul>
    <p>The textbooks continue to discuss Earley's algorithm.
      In
      <b>1972</b>, Aho and Ullmann describe
      a straightforward fix to the null-production bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    <p>Meanwhile, while Irons first left parser was syntax-driven,
      hand-coding emerges as the more attractive approach to left parsing.
      On one hand hand-coding is necessary to overcome the limits of a left-parsing approach.
      On the other hand, left parsing and hand-coding do fit well together.
      The approach to hand-writing parsers that emerges is now called
      recursive descent.
      The search for an efficient, powerful, syntax-driven algorithm
      seem to be at a dead-end until ...
    </p>
    <p <b>1969</b>:
      a PhD thesis by Frank DeRemer described a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
      Gradually,
      the world takes notice.
    </p>
    <p><b>1975</b>:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    </p>
    <p><b>1977</b>:
      The first "Dragon book" comes out.
      The nickname of this classic textbooks comes from the drawing
      on the front cover, in which a knight takes on a dragon.
      Emblazoned on his lance are the letters "LALR".
      From here on,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    </p>
    <p><b>1987</b>:
    If you want to date the high point of LALR's reign,
    this might be the right year.
      It is the year that
      Larry Wall introduced Perl 1.
      Perl embraced complexity like no previous language,
      and to do it, Larry used LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    </p>
    <p><b>1991</b>:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Not noticed at the time, this is a major discovery.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      And in 1991 hardware was six orders of magnitude faster
      than 1968 hardware, so the bookkeeping issue had receded
      in importance.
      When it comes to the speed,
      the game has changed, and very much in the Earley algorithm's
      favor.
      But when Leo publishes, Earley parsing has been forgotten by most.
      It takes 20 years for anyone to write a practical
      implementation of Leo's algorithm.
    </p>
    <p>
      So, since Earley's is forgotten, everyone in
      LALR-land is content.  Right?
      Wrong. Far from it, in fact.
      Users of LALR had made some nasty discoveries about it.
      While LALR does automatically
      generates a parser for you,
      it is so hard to debug that it's
      usually just as easy to write one by hand.
      Once written, an LALR parser is fast for correct inputs.
      But almost all it can tell the users about incorrect inputs,
      is that they are incorrect --
      the LALR tables provide little clue as to why.
      In Larry's words, LALR was "fast but stupid".
    </p><p>In
      <b>2000</b>, Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
      Discontent with LALR
      was also spreading inside academia ...
    </p>
    <p <b>2002</b>:
      Aycock&Horspool describe their attempt to speed up Earley's in practice.
      They do not include Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and introduces complications.
      But buried in their paper is a solution to the null-production bug --
      this time a solution that requires no additional bookkeeping.
    </p>
    <p>
      The Aycock&Horpsool has little effect on practical parsing.
      But LALR's plummet from favor is by now unstoppable.
      completely rewritten.
    </p>
    <p><b>2006</b>:
      For three decades,
      the industry's flagship C compilers had been
      parsed with LALR --
      proof of the claim that LALR and serious
      parsing were equivalent.
      But in 2006,
      GNU announces that the GCC compiler's parser has been rewritten.
      LALR's replacement is the technology that
      it replaced a quarter century earlier:
      recursive descent.
    </p>
    <p>With the retreat from LALR comes a collapse in the
      prestige of parsing theory.
      After a half century of
      research, it looks as if we wound up almost
      exactly where we started.
      In fact,
      if you took Ned Iron's original 1961 algorithm,
      changed the names and dates,
      and translated from code from the mix of assembler and
      ALGOL into Haskell,
      you would easily republish it as 
      as new and ground-breaking.
    </p>
    <p><b>2010</b>:
      Over the years, I'd come back to Earley's algorithm again and again.
      Around 2010, I realized
      that the vision
      of an efficient, practical, general and syntax-driven parser --
      abandoned so long it was almost 
      was now, in fact, quite possible.
      Over the years, unnoticed, the pieces had fallen into place.
    </p><p>
      Aycock&Hospool has solved the null-production bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated.
      Machines operations are now a billion times faster than in 1968,
      and probably no longer relevant in any case --
      caches misses are more important.
    </p>
    <p>But while the original issues with Earley's disappeared,
      a new issue emerged.
      With a parsing algorithm as powerful as Earley's behind it,
      a syntax-driven approach can do much more than it can with
      a left parser.
      But with the experience with LALR in their collective consciousness,
      few modern programmers are prepared
      to trust a purely declarative parser.
      As Lincoln said, "Once a cat's been burned,
      he won't even sit on a cold stove."
    </p>
    <p>
      In the process of combining
      the improvements from Aycock&Horspool and Leo,
      I found that the Earley's parse engine can be written so
      that it's easy to pause at any token.
      While paused,
      the application can switch to procedural logic
      and single-step forward token by token.
      The procedural logic can hand control back
      over to syntax-driven parsing at any point it likes.
      Earley's algorithm is actually a better companion
      for hand-written procedural logic than recursive descent is.
      The Earley tables can provide the procedural logic with
      full knowledge of the state of the
      parse so far:
      all rules recognized
      in all possible parses so far,
      and all symbols expected.
    </p>
    <h3>For more</h3>
    <p>
      For more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
      official web site maintained by Ron Savage</a>:
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
