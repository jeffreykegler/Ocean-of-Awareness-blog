What makes a parsing algorithm successful?
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>What makes a parsing algorithm successful?
      Two factors, I think.
      First, does the algorithm parse a workably-defined set of grammars in linear time?
      Second, does it allow the application to intervene in the parse
      with custom code?
      When parsing algorithms are compared,
      typically neither of these gets much attention.
      But the successful algorithms do one or the other.
    </p>
    <h3>Does the algorithm parse a workably-defined set of grammars in linear time?</h3>
    <p>By "workably-defined" I do not just mean well-defined
      in the mathematical sense.
      I mean something that goes beyond that --
      the set of grammars has to have a definiton that is workable.
      Workable means that the
      definition is something that,
      with reasonable effort,
      a programmer can use in practice.
    </p><p>
      The algorithms in regular expression engines are workably-defined.
      A regular expression, in the pure sense consists of a sequence of symbols,
      usually shown by concatenation:
    </p><blockquote><pre>a b c</pre></blockquote><p>
      or a choice among sequences, usually shown by a vertical bar:
    </p><blockquote><pre>a | b | c</pre></blockquote><p>
      or a repetition of any of the, typically shown with a star:
    </p><blockquote><pre>a*</pre></blockquote><p>
      or any recursive combination of these.
      True, if this definition is new to you, it can take time to get
      used to.
      But vast numbers of working programming are very much "used to it",
      can think in terms of regular expressions,
      and can determine if a particular problem will yield to treatment
      as a regular expression, or not.
      <p>
      Parsers in the LALR family (yacc, bison, etc.)
      do <b>not</b>
      have a workably defined set of grammars.
      LALR is perfectly well-defined mathematically,
      but even experts in parsing theory are hard put to decide
      if a particular grammar is LALR.
    </p><p>
      Recursive descent also does not have a workably defined
      set of grammars.
      Recursive descent doesn't even have a precise mathematical description --
      you can say they are LL, but in practice LL tables are rarely used.
      Also in practice, the LL logic is extended with every other trick
      imaginable, up to and including switching to other parsing algorithms.
    </p>
    <h3>Does it allow the user to intervene in the parse?</h3>
    <p>It is not easy for users to intervene in the processing
      of a regular expression, though some implementations attempt to
      allow such efforts.
      LALR parsers are notoriously opaque.
      Those who maintain the Perl parser have tried
      to supplement its abilities with
      custom code, with results that will not encourage
      others making the same attempt.
    </p><p>Recursive descent, on the other hand, has no parse engine --
      it is 100% custom code.
      You don't get much friendlier than that.
    </p><h3>Conclusion</h3><p>
      Regular expressions are a success,
      and will remain so,
      because the set of grammars
      they handle is very workably-defined.
      Applications using regular expressions have to take what the algorithm
      gives them, but what it gives them is very predictable.
      <p>
      For example, an application can write regular expressions on the fly, and
      the programmer can be confident they will run as long as they are well-formed.
      And it is easy to determine if the regular expression is well-formed.
      (Whether it actually does what you want is a separate issue.)
      <p>
      Recursive descent does not handle a workably-defined set of grammars,
      and it also has to be hand-written.
      But it makes up for this by allowing the user to step into the parsing process
      anywhere, and "get his hands dirty".
      Recursive descent does nothing for you, but it does allow you complete control.
      This is enough to mkae recursive descent the current algorithm of choice
      for major parsing projects.
      <p>
      As I have
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html">chronicled
      elsewhere</a>,
      LALR was once,
      on highly convincing theoretical grounds,
      seen as
      <b>the</b> solution to the parsing problem.
      But while mathematically well-defined,
      LALR was not workably defined,
      it was very hostile to applications that tried to alter,
      or even examine, its syntax-driven workings.
      After decades of trying to make it work,
      the profession has abandoned LALR almost totally.
    </p>
    <h3>What about Marpa?</h3>
    <p>Marpa has both properties:
      its set of grammars is workably-defined.
      And, while Marpa is syntax-driven like LALR and regular expressions,
      it also allows the user to stop the parse engine,
      communicate with it about the state of the parse,
      do its own parsing for a while,
      and restart the parse engine at any point it wants.
    </p>
    <p>Marpa's workable definition has a nuance that the one
    for regular expressions does not.
    For regular expression linearity is a given --
    they parse in linear time or fail.
      Marpa parses a much larger class of grammars, the context-free grammars --
      anything that can be written in BNF.
      BNF is used to describe languages in standards,
      and is therefore the "gold standard" for a workable definition of a
      set of grammars.
      However, Marpa does <b>not</b>
      parse everything that can be written in BNF in linear time.
    </p>
    <p>Marpa linearly-parsed set of grammars is smaller than the context-free
    grammars, but it is still very large, and it is still workably-defined.
      Marpa will parse any unambiguous language in linear time,
      unless it contains unmarked middle recursions.
      An example of a "marked" middle recursion is the language described
      by
    </p><blockquote><pre>S ::= a S a | x</pre></blockquote><p>
    a string of which is "<tt>aaaxaaa</tt>",
      where the "<tt>x</tt>" marks the middle.
      An example of an "unmarked" middle recursion is the language described
      by
    </p><blockquote><pre>S ::= a S a | a</pre></blockquote><p>
    a string of which is "<tt>aaaaaaa</tt>",
      where nothing marks the middle, so that you don't know until the end where the
      middle of the recursion is.
      If a human can reliably find the middle by eyeball, the middle recursion is marked.
      If a human can't, then the middle recursion might be unmarked.
    </p>
    <p>Marpa also parses a large set of unambiguous grammars linearly,
      and this set of grammars is also workably-defined.
      Marpa parses an ambiguous grammar in linear time if
    </p><ul>
      <li>It has no unmarked middle recursions.
      </li>
      <li>All right recursions are unambiguous.
      </li>
      <li>There are no cycles.
      A cycle occurs, when example, if there is a rule <tt>A ::= A</tt>
      in the grammar.
      </li>
      <li>Marpa's level of ambiguity at any location is bounded by a constant.
        There can be as many rules "in play" as you like.
        The key question is at how many points,
        earlier in the parse, can these rules have begun?
        That is, can a rule currently in play
        have begun only at 20 previous locations,
        and could it have started at every location so far?
        If the answer is 20 or some other constant, the level of
        ambiguity is "bounded".
        If not, the level of ambiguity is unbounded.
      </li>
      </ul>
    <p>For the unambiguous case, Marpa's workable definition encompasses
    a much larger class of grammars, and is no more
      complex than that for regular expressions.
      If you want to extend even further into ambiguous grammars,
      working with the definition is still quite manageable.
      Of the 4 restrictions needed to ensure linearity,
      typically the only one for bounded
      ambiguity will ever require careful watching.
      Cycles never occur in a practical grammars,
      so that you just fix them when they happen.
      Most recursions will be left recursions,
      which are unrestricted.
      For the rest, you just note where your middle
      recursions and right recursions are,
      and make sure the middle ones are marked
      and the right ones are unambiguous.
    </p>
    <h3>More about Marpa</h3><p>
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p><h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
