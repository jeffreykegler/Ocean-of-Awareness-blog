Parsing right and left
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>Parsing has a reputation of being an esoteric
        discipline -- which it can be,
        and which is how it is usually presented.
        But its basic concepts are simple and intuitive.
        </p>
	<h3>Left parsing</h3>
        <p>Certainly the basic idea behind being left parsing is
	as about as simple as a programming strategy gets:
        you look at the next symbol and make your parsing decision
        based on it.
        This is simple but limited, so left parsing is almost
        combined with with lookahead.
        Lookahead of one character helps a lot, but longer lookahead
	very rapidly becomes less useful,
        at the same time as they make the size of the parser grows exponentially.
        <p>Left parsing approaches notoriously have an issue with left recursion.
	It's straightforward to see why.
        If we have an open-ended expression like
        <blockquote><pre>
        a + b + c + d + e + f + [....]
        </pre></blockquote>
        where the plus signs continue off to the right,
        we cannot make the parsing decision at the first '+' unless
        we know how many more to come.
        And we cannot do this without doing a lot of looking to the right.
        There are a lot of approaches to dealing with this difficulty,
        but all of them involve trying to make left parsing into
        something it is not.
    <h3>Advantages of left parsing</h3>
    <p>It is hard to believe that an approach
    as myopic as left parsing is as good as we can do.
    But before looking at alternative,
    I want to emphasize that simplicity
    is left parsing's great strength.
    Because a left parsing does next to nothing,
    it is very easy to figure out what it is doing.
    And easy to figure out,
    means easy to customize.
    <p>
    Take another of the many constructs incomprehensible to
    a left parser:
    <blockquote><pre>
    2 * 3 * 4 + 5 * 6
    </pre></blockquote>
    How do left parser languages handle this?
    Simple: as soon as they realize they are faced
    an expression, they away from the left
    parser to a specialized parser.
    <p>These two properties -- easy to understand
    and easy to customize --
    have catapulted left parsing
    to the top of the heap.
    Behind their different presentations,
    combinator parsing, PEG, and recursive descent are
    all left parsers.
    <h3>Right parsing</h3>
    <p>Few theoreticians of the 1970's imagined that left parsing would
    be the end of the story.
    That the only way to exploit right context is ad hoc hackery
    just did not make sense --
    it certainly does not seem to be the way a human reads
    sentences like these, for example.
    <p>Don Knuth in 19xx found a way to use
    right context, that was, like left parsing as I have described it,
    deterministic.
    Deterministic was thought very important because it determinism
    avoids an explosion of choices.
    And no explosion of choices means the parse can be done in linear time.
    <p>Very simplistically, Don's suggestion was to, instead of fully deciding the
    parse at every location, to make what I will call "subdecisions" --
    decisions as to how the
    piece at that location is used, but which leave context to be decided
    separately.
    Don proposed a stack of subdecisions -- if you have a subdecision
    but cannot decide its full context, you push it on the stack ("shift").
    When you encounter the context, you pop it off the stack ("reduce")
    and make the full decision.
    <p>
    To preserve determinism,
    you have to know whether to shift or reduce,
    and if reducing, what reduction to make,
    at every location.
    But the problem with left recursion disappeared.
    In the example from above
        <blockquote><pre>
        a + b + c + d + e + f + [....]
        </pre></blockquote>
        you shift until you run out of plus signs.
        At that point you reduce all the subdecisions on the stack.
        It was as simple at that.
        Expressions like
    <blockquote><pre>
    2 * 3 * 4 + 5 * 6
    </pre></blockquote>
    solved in much the same way.
    You shift all '*' subdecisions until you hit the '+',
    the reduce them off the stack.
    <p>Finding a practical way to make all the shift-reduce and reduce-reduce
    decisions took some doing, but a way was found for what looked like a
    practicl subset of grammars.
    Theoreticians thought they'd found the Holy Grail,
    and were not able to restrain the urge to portray themselves as Grail
    Knights.
    <p>This story did not have a happy ending,
    as I've described elsewhere.
    Right parsing was driven by tables which made the algorithm fast
    for correct inputs, but unable to accurately diagnose faulty ones.
    The subset of grammars parsed still was not quite large enough even 
    for conservative language designers.
    And LR was very unfriendly to custom hacks, which meant
    its shortcomings were much harder to work arond than those of
    left parsing.
    After years of experience, left parsing re-emerged as the
    algorithm of choice.
    <h3>Non-determinism</h3>
    <p>Assumed in all of this was that,
    for an algorithm to be linear,
    in practice it would also have to be deterministic.
    But is that the case?
    <h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
