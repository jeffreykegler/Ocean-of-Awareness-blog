Top-down and bottom-up
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>Parsing has a reputation of being an esoteric
      discipline -- which it can be,
      and which is how it is usually presented.
      But its basic concepts are simple and intuitive.
    </p>
    <h3>Top-down parsing</h3>
    <p>Certainly the basic idea behind being top-down parsing is
      as about as simple as a programming strategy gets:
      you look at the next symbol and make your parsing decision
      based on it.
      This is simple but limited, so top-down parsing is almost
      combined with with lookahead.
      Lookahead of one character helps a lot, but longer lookahead
      very rapidly becomes less useful,
      at the same time as they make the size of the parser grows exponentially.
    </p><p>Top-down parsing approaches notoriously have an issue with left recursion.
      It's straightforward to see why.
      If we have an open-ended expression like
    </p><blockquote><pre>
    a + b + c + d + e + f + [....]
        </pre></blockquote><p>
      where the plus signs continue off to the right,
      we cannot make the parsing decision at the first '+' unless
      we know how many more to come.
      And we cannot do this without doing a lot of looking to the right.
      There are a lot of approaches to dealing with this difficulty,
      but all of them involve trying to make top-down parsing into
      something it is not.
    </p><h3>Advantages of top-down parsing</h3>
    <p>It is hard to believe that an approach
      as myopic as top-down parsing is as good as we can do.
      But before looking at alternative,
      I want to emphasize that simplicity
      is top-down parsing's great strength.
      Because a top-down parsing does next to nothing,
      it is very easy to figure out what it is doing.
      And easy to figure out,
      means easy to customize.
    </p><p>
      Take another of the many constructs incomprehensible to
      a top-down parser:
    </p><blockquote><pre>
    2 * 3 * 4 + 5 * 6
    </pre></blockquote><p>
      How do top-down parser languages handle this?
      Simple: as soon as they realize they are faced
      an expression, they switch away from top-down
      parser to a specialized algorithm.
    </p><p>These two properties -- easy to understand
      and easy to customize --
      have catapulted top-down parsing
      to the top of the heap.
      Behind their different presentations,
      combinator parsing, PEG, and recursive descent are
      all top-down parsers.
    </p><h3>Bottom-up parsing</h3>
    <p>Few theoreticians of the 1970's imagined that top-down parsing would
      be the end of the story.
      That the only way to exploit right context is ad hoc hackery
      just did not make sense --
      it certainly does not seem to be the way a human reads
      sentences like these, for example.
    </p><p>Don Knuth in 19xx found a way to use
      right context, that was, like top-down parsing as I have described it,
      deterministic.
      Deterministic was thought very important because it determinism
      avoids an explosion of choices.
      And no explosion of choices means the parse can be done in linear time.
    </p><p>Very simplistically, Don's suggestion was to, instead of fully deciding the
      parse at every location, to make what I will call "subdecisions" --
      decisions as to how the
      piece at that location is used, but which leave context to be decided
      separately.
      Don proposed a stack of subdecisions -- if you have a subdecision
      but cannot decide its full context, you push it on the stack ("shift").
      When you encounter the context, you pop it off the stack ("reduce")
      and make the full decision.
    </p><p>Like top-down parsing, bottom-up parsing is usually combined with lookahead.
      And for the same lookahead, a bottom-up parser parses not just everything that a
      top-down parser can handle,
      and more.
    </p><p>
      To preserve determinism,
      you have to know whether to shift or reduce,
      and if reducing, what reduction to make,
      at every location.
      But the problem with left recursion disappeared.
      In the example from above
    </p><blockquote><pre>
    a + b + c + d + e + f + [....]
        </pre></blockquote><p>
      you reduce until you run out of plus signs.
      Each reduced addition becomes the left hand side of the next addition,
      and this continues until you run out of plus signs.
    </p><p>For a bottom-up parser, right recursion is harder, but not much
      harder.
    </p><blockquote><pre>
    a = b = c = d = e = f = [....]
        </pre></blockquote><p>
      At every equal sign you "shift" the subdecision onto the stack.
      When you hit the end, you pop the subdecisions off the stack
      one by one and "reduce" them.
      Essentially, you do the same thing you did for left recursion,
      only in reverse.
      As simple at that.
      Arithmetic expressions like
    </p><blockquote><pre>
    2 * 3 * 4 + 5 * 6
    </pre></blockquote><p>
      are solved by combining the approaches.
      For this one
      you "reduce" all the multiplications,
      at which point you "shift" the result onto the stack.
      Then you "reduce" the next set of multiplications.
      At the end you pop the addition off the stack and "reduce" it.
    </p><p>In the above discussion,
      I assumed that we had some way to figure out when to reduce and when to shift.
      Finding a practical way to make the shift/reduce decision took some doing,
      but a way was found for what looked like a
      practical subset of grammars,
      and around 19-- it was released as yacc.
      (Readers today may be more familiar with yacc's successor, bison.)
    </p><p>
      With yacc, theoreticians thought they'd found the Holy Grail,
      and when the textbooks focused on bottom-up parsing came out,
      they were not always able to restrain the urge
      to portray parser writers as knights in armor.
    </p><p>But not every medieval romance has a happy ending and
      as I've described elsewhere, this story ended badly.
      Bottom-up parsing was driven by tables which made the algorithm fast
      for correct inputs, but unable to accurately diagnose faulty ones.
      The subset of grammars parsed still was not quite large enough even
      for conservative language designers.
      And bottom-up parsing was very unfriendly to custom hacks,
      so that its shortcomings loomed large.
      It was much harder to work around a problem in a right
      parser than than it was to deal with a similar problem
      in a top-down parser.
      After years of experience,
      top-down parsing has re-emerged as the
      algorithm of choice.
    </p><h3>Table parsing</h3>
    <p>For many, the return to top-down parsing
      answers the question that we asked earlier.
      The response to
      "Is there any systematic way to exploit right context when parsing?"
      turns out to be a rather surprising "No".
      But is this really the end of the story?
    </p><p>Assumed in all of this was that,
      for an algorithm to be linear,
      in practice it would also have to be deterministic.
      But is this actually the case?
    </p><p>It's not, in fact.
      To keep bottom-up parsing deterministic, we restricted ourselves to a stack.
      But what if we keep all possible subdecisions linked and in tables,
      and make the final decisions in another pass,
      once the tables are complete.
      (The second pass replaces the stack based
      see-sawing back and forth of the deterministic algorithm,
      so it's not an inefficiency.)
      Jay Earley in 19-- came up with an algorithm to do this,
      and in 1991 Joop Leo added a memoization to Earley's
      algorithm which makes it linear for all deterministic grammars.
    </p><p>The "deterministic grammars"
      are exactly the bottom-up parseable grammars
      with lookahead.
      So that means the Earley/Leo algorithm parses,
      in linear time,
      everything that a deterministic bottom-up parser can parse,
      and therefore every grammar that
      a deterministic top-down parser can parse.
      parseable by any of the algorithm
      (And in fact, the Earley/Leo algorithm is linear for a lot of
      ambiguous grammars as well.)
    </p><p>Top-down parsing had the advantage that it was easy to know where
      you are.  But the Earley/Leo algorithm has an equivalent advantage -- its
      tables know where it is, and it is easy to query them programmatically.
      In 2010, I added to the Earley/Leo algorithm the other big advantage of
      top-down parsing:
      This blogger's Marpa parser rearranges the Earley/Leo parse engine so that you can
      stop it, perform your own logic, and restart where you left off.
      The Marpa algorithm is very much implementable.
      <a href=http://savage.net.au/Marpa.html">A quite useable parser based on it</a>
      exists.
    </p><h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
