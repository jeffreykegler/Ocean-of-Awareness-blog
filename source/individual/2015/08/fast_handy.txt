Fast handy languages
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>Back around 1980, I had access to UNIX and a language I wanted to parse.
      I knew that UNIX had all the latest CS tools.
      So I expected to type in my BNF and "Presto, Language!".
    </p>
    <p>Not so easy, I was told.
      Languages were difficult things created with complex tools
      written by experts who understood the issues.
      I recall thinking that,
      while English had a syntax that is
      as hard as they come,
      toddlers manage to parse it
      just fine.
      But experts are experts,
      and more so at second-hand.
    </p>
    <p>I was steered to an LALR-based parser called yacc.
      (Readers may be more familiar with bison, a yacc successor.)
      LALR had extended the class of quickly parseable grammars a bit
      beyond recursive descent.
      But recursive descent was simple in principle,
      and its limits were easy to discover and work around.
      LALR, on the hand, was OK when it worked, but
      figuring out why it failed when it failed
      was more like decryption than debugging,
      and this was the case both with parser development
      and run-time errors.
      I soon gave up on yacc
      and found another way to solve my problem.
    </p>
    <p>Few people complained about yacc on the Internet.
      If you noise it about that you are unable
      to figure out how to use
      what everybody says is the state-of-the-art tool,
      the conclusions drawn may not be the ones you want.
      But my experience seems to have been more than common.
    </p>
    <p>LALR's claim to fame was that it was the basis of the
      industry-standard C compiler.
      Over three decades,
      its maintainers suffered amid the general silence,
      but by 2006, they'd had enough.
      GCC (the new industry standard)
      ripped its LALR engine out.
      By then
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html">the
      trend back to recursive descent</a>
      was well underway.
    </p>
    <h3>A surprise discovery</h3>
    <p>Back in the 1970's,
      there had been more powerful alternatives
      to LALR and recursive descent.
      But they were
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/04/fast_enough.html">reputed
        to be slow</a>.
    </p>
    <p>For some applications slow is OK.
      In 2007 I decided a parsing tool that parsed
      all context-free languages at state-of-the-art speeds,
      slow or fast as the case might be,
      would be a useful addition to programmer toolkits.
      And I ran into a surprise.
    </p>
    <p>Hidden in the literature was an amazing discovery --
      an 1991 article by Joop Leo described how to modify Earley's
      algorithm to be fast for every language class in practical use.
      (When I say "fast" in this article, I will mean "linear".)
      Leo's article had been almost completely ignored --
      my project (<a href="http://savage.net.au/Marpa.html">Marpa</a>)
      would become its first
      practical implementation.
    </p>
    <h3>Second-order languages</h3>
    <p>The implications of Leo's discovery go well beyond speed.
      <b>If</b>
      you can rely on the BNF that you write always producing
      a practical parser, you can auto-generate your language.
      In fact,
      you can write languages which write languages.
    </p>
    <h3>Which languages are fast?</h3>
    <p>The Leo/Earley algorithm is not fast
      for every BNF-expressible language.
      BNF is powerful, and you can write exponentially
      ambiguous languages in it.
      But programmers these days only care about unambiguous languages --
      they are accustomed to tools which parse only a subset of these.
    </p>
    <p>
      As I've said, Marpa is fast for every language in
      a class in practical use today.
      Since the modern tools for parsing impose severe limits,
      Marpa is almost certainly fast for any language,
      that a modern programmer has in mind.
      Without peeking ahead at the hints I am about to give you,
      in fact, it is actually
      <b>hard</b>
      to write an unambiguous
      grammar that goes non-linear on Marpa.
      Simply mixing up lots of left, right and middle recursions
      will
      <b>not</b>
      be enough to make an
      unambiguous grammar go non-linear.
      You will also need to violate one of the set of rule
      I am about to give you.
    </p><p>You can guarantee that Marpa is fast for your BNF language,
      if you follow three rules:
    </p>
    <ul>
      <li>Rule 1: Your BNF must be unambiguous.
      </li>
      <li>Rule 2: Your BNF must have no "unmarked" middle recursions.
      </li>
      <li>Rule 3: All of the right-recursive symbols
        in your BNF must be dedicated
        to that right recursion.
      </li>
    </ul>
    <p>Rule 3 turns out to be very easy to obey.
      I discuss it in detail in the next section,
      which will be about how to breaking these rules and
      get away with it.
    </p>
    <p>First, let's look at what an "unmarked" middle recursion is.
      Here's an example of a "marked" middle recursion:
    </p><blockquote><pre>
       M ::= 'b'
       M ::= 'a' M 'a'
    </pre></blockquote><p>
      Here the "b" symbol is the marker.
      This recursion generates sequences like
    </p><blockquote><pre>
       b
       a b a
       a a b aa
    </pre></blockquote><p>
      Now for an "unmarked" variant of this middle recursion
    </p><blockquote><pre>
       M ::= 'a' 'a'
       M ::= 'a' M 'a'
    </pre></blockquote><p>
      This recursion generates sequences like
    </p><blockquote><pre>
       a a
       a a a a
       a a a a a a
    </pre></blockquote><p>
      In this middle recursion there is no marker.
      To know where the middle is.
      you have to scan all the way to the end,
      and then count back.
    </p>
    <p>A rule of thumb is that if you can "eyeball" the middle
      of a long sequence,
      the recursion is marked.
      If you can't it is unmarked.
      Unfortunately, we can't characterize exactly what a marker
      must look like -- a marker can encode the moves of a Turing machine.
      That mean that, in the general case, determining whether a middle
      recursion is marked is an undecidable problem.
    </p>
    <h3>How to get away with breaking the rules</h3>
    <p>The rule about ambiguity is "soft".
      If you don't break it by much, your parser will stay linear.
      In particular, if you only allow limited ambiguity and
      keep your rule-breaking recursions short, your grammar will stay
      fast.</p>
    <p>Above, I promised to explain rule 3, which insisted that
      a right recursive symbol be "dedicated".
      A right recursive symbol is "dedicated" if it appears only
      as the recursive symbol in a right recursion.
      If your grammar is unambiguous, but you've used an "undedicated"
      right-recursive symbol, that is easy to fix.
      Just rewrite the grammar using two symbols.
      Dedicate one to the right recursion,
      and use the other in the non-right recursive contexts.
    </p>
    <h3>When NOT to use Marpa</h3>
    <p>The languages I have described as "fast" for Marpa
      include all those in practical use and many more.
      But do you really want to use Marpa for all of them?
      Here are three cases for which Marpa is probably not
      your best alternative.
    </p>
    <p>The first case: a language that parses easily with a regular
      expression.
      The regular expression will be much faster.
      Don't walk away from a good thing.
    </p>
    <p>The second case:
      A language
      that is easily parsed using a single
      loop and some state that fits into constant space.
      Essentially this is the SAX-ish approach.
      In its current implementation, Marpa's keeps all of its
      parse tables forever.
      This allows it to deal with the full structure of the
      input, in a way that a SAX-ish approaches cannot.
      But it does take space,
      and on long parses this is a real issue,
      an issue that SAX-ish parsers typically do not have.
      Marpa's parse engine is written is C, and if you're pitting
      Marpa's optimized C language
      against a much slower higher level language,
      there may be a win on CPU speed.
      But if you are comparing Marpa against a simple SAX
      parser,
      and you need to parse large files,
      Marpa may lose due to its the space requirements.
    </p>
    <p>The third case:
    a language that
    </p><ul>
      <li>is very small;
      </li>
      <li>changes slowly or not at all, and does not grow in complexity;
      </li>
      <li>merits careful hand-optimization, and has available the staff
        to do it;
      </li>
      <li>merits and has available the kind of on-going support that will
        keep your code optimized under changing circumstances; and
      </li>
      <li>is easily parseable via recursive descent:</li>
    </ul>
    <p>
      It is rare that all of these are the case,
      but when that happens,
      recursive descent is often preferable to Marpa.
      Lua and JSON
      are two languages which meet the above criteria.
      In Lua's case, it targets platforms with very restricted memories,
      which is an additional reason to prefer recursive descent --
      Marpa has a relatively large footprint.
    </p>
    <p>It was not good luck that made
      both Lua and JSON good targets for recursive descent --
      they were designed around its limits.
      JSON is a favorite test target of Marpa for just these reasons.
      There are carefully hand-optimized C language parsers for us to
      benchmark against.
    </p>
    <p>We get closer and closer,
      but we'll never beat small hand-optimzed JSON parsers in software.
      However, while recursive descent is a technique for hand-writing parsers,
      Marpa is a mathematical algorithm.
      Someday,
      instructions for manipulating Earley items could be implemented directly
      in silicon.
      When and if that day comes,
      Earley's algorithm will beat recursive descent even at
      parsing the grammars that were designed for it.
    </p>
    <h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href="http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
