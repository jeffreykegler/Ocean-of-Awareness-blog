Why Marpa works: table parsing
  <p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Marpa works very differently from the parsers
      in wide use today.
      Marpa is a table parser.
      And Marpa is unusual among table parsers --
      its focus is on speed.
    </p>
    <p>
      The currently favored parsers (recursive descent and,
      decreasing, LALR parsers like bison)
      use stacks and/or state machines.
      These have the advantage that it is easy
      to see how they can be made
      to run fast.
      They have the disadvantage of severely limiting
      what the parser can do and how much it can know.
    </p>
    <h3>What is table parsing</h3>
    <p>
      Table parsing means parsing by constructing a table of all the possible parses.
      This is pretty clearly something you want -- anything less means
      not completely knowing what you're doing.
      It's like walking across the yard blindfolded.
      It's fine if you can make sure there are
      no walls, open pits, or other dangerous obstacles.
      But for the general case,
      it's a bad idea.
    </p>
    <p>
    Where the analogy between walking blindfolded and parsing breaks
    down is that taking off the blindfold has no cost,
    building a table of everything that is happening while you parse
    <b>does</b> have a cost.
    If you limit your parser to a stack and a state machine,
    you may be parsing with a blindfold on,
    but it is clear that your parser can be fast.
    How to make a table parser run fast
    is not so clear.
    </p>
    <h3>The advantages of table parsing</h3>
    <p>
      What are the advantages of taking off the blindfold?
      First, your parser can be completely general --
      anything you can write in BNF it can parse.
      And second,
      you always know exactly what is going on -- what rules
      are possible, what rules have been recognized,
      how far into a rule you have recognized it,
      what symbols you expect next, etc. etc.
    <p>
    </p>
      We programmers have gotten used to parsers which run blindfolded.
      Just like when you hit something blindfolded you don't know
      what it was, whether you could get around it or how,
      when these parsers fail, they don't know why.
      They can only guess.
      If you have a full parse table
      built from left to right,
      you know what you were looking for and what you already think you
      found.
      This means that you can pinpoint and identify errors precisely.
    </p>
    <p>
       Knowing where you are in a parse also allows certain tricks,
       like the one I call the Ruby Slippers.
       In this, you parse with an over-simplified grammar and,
       when it fails, you ask the parser what it was looking for.
       Then -- poof! -- you provide it.
    </p>
    <p>
       The Ruby Slippers work beautifully when dealing with
       missing tags in HTML.
       You can define a simplified HTML grammar,
       one that lives in a non-existent world --
       an ideal world where all start
       and end tags are always where they belong.
       Then you parse away.
       If, as will happen with real-world input, a tag is missing,
       you ask the table parser what it was looking for,
       and give it a virtual tag.
    </p>
    <h3>And as for fast ...</h3>
    <p>When I decided to write Marpa in 2007 my goal was to create a table parser
    that was as fast as possible.
    I was surprised to find that the academic literature contained a
    major improvement to table parsing by Joop Leo,
    one that nobody had ever made a serious attempt to implement.
    Marpa is the first implement of Joop Leo's 1991 improvement to table parsing which,
    as far as theory goes,
    makes Marpa as fast any parser
    in practical use today.
    Any class of grammar that
    recursive descent, bison, etc. will parse,
    Marpa will parse on linear time.
    </p>
    <p>
    Table parsing did have a reputation for being slow due to a
    bad "constant factor".
    I think it's time to look again, for two reasons.
    First, Aycock and Horspool did a lot of careful work on
    the constant factor, work which Marpa builds on.
    <p>
    </p>
    Second, and more important
    the judgement about the constant factor dates from  1968,
    when computers were literally a billion
    times slower then they are now.
    Why had nobody re-examined this judgment as the years and order
    of magnitude improvements slipped by?
    <p>
    </p>
    In 1968, practitioner and theoretician alike decided
    table parsing was not practical.
    As the theoretical results changed, the theoreticians did change
    their mind because they the practitioners were the people who wrote
    compilers, and you had to assume they knew what they
    were talking about.
    Mean while the practice of programming revolutionized itself every five years,
    but the practitioners continued to write off table parsing
    as impractical.
    After all, the theoreticians said it was.
    and since it was the theoreticians who worked up
    these algorithms,
    you were pretty much forced
    to assume they knew what they were talking about.
    <p>
    I look carefully and at some length at the issue
    of the "constant factor" in a previous post.
    But summarizing,
    forty-five years and
    nine orders of magnitude in cost of CPU have passed.
    (Others say the cost of CPU has dropped by 50% a year,
    in which case it's over 14 orders of magnitude.
    But why quibble?)
    It's reasonable to believe that
    the constant factor that practitioners
    and theoreticians worried about in 1968 stopped being a
    major obstacle many years ago.
    </p>
    <h3>For more about Marpa</h3>
    <p>
      Marpa's latest version is
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2,
        which is available on CPAN</a>.
      A list of Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
      here</a>.
      There is
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        this blog</a>,
	which has
	<a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>,
	and 
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page</a>.
      For questions, support and discussion, there is a
      Google Group:
      <code>marpa-parser@googlegroups.com</code>.
      Comments on this post can be made there.
    </p>
