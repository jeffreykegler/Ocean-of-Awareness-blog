Is Earley Parsing Fast Enough?
  <blockquote>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      "First we ask, what impact will our algorithm have on the parsing
      done in production compilers for existing programming languages?
      The answer is, practically none." -- Jay Earley's Ph.D thesis, p. 122.
    </blockquote>
    <p>In the above quote, the inventor of the Earley parsing
      algorithm poses a question.
      Is his algorithm fast enough for a production compiler?  His answer is a
      stark "no".
    </p>
    <p>
    This is the verdict on Earley's that you often
    hear repeated today, 45 years later.
    Earley's, it is said, has a "high constant factor".
    When a field is as difficult as parsing theory,
    verdicts tends to be repeated more often than examined.
    This particular verdict originates with the inventor himself.
    Since 1968, parsing theory has gone out of fashion and off
    the funding radar.
    So perhaps it should not seem astonishing
    that many consider that many treat the dismissal
    of Earley's on grounds of speed to be as valid today as it
    was in 1968.
    </p>
    <p>But in the past 45 years,
    the technology has changed beyond recognition.
    And, out of fashion or not, researchers
    have made several important improvements to Earley's.
    It is time to reopen this case.
    <h3>What is a "constant factor"</h3>
    <p>The term "constant factor" here has a special meaning,
    one worth looking at carefully.
    When programmers talk about time efficiency, they do it in two ways:
    time complexity and speed.
    </p>
    <p>
      Speed is simple:
      It's how fast the algorithm is against the clock.
      To make comparison easy,
      the clock can an abstraction.
      The clock ticks could be, for example, weighted instructions
      on some convenient and mutually-agreed architecture.
    </p>
    <p>
      But by the time Earley was writing, programmers had discovered that simply comparing
      speeds,
      even on abstract clocks, was not enough.
      Computers were improving so quickly that a speed result
      that was clearly significant when a comparison was made
      might soon be unimportant .
      So that researchers could
      make statements about time efficiency that would remain true
      over the decades,
      they created the idea of time complexity.
    </p>
    <p>Time complexity is measured using several notations, but the most
      common is big-O notation.
      Here's the idea:
      Assume we are comparion two algorithms, Algorithm A and Algorithm B.
      Assume that algorithm A uses 42 weighted instructions for each input symbol.
      Assume that algorithm B uses 1792 weighted instructions for each input symbol.
      Where the count of input symbols is N,
      A's speed is 42*N, and B's is 1792*N.
      But the time complexity of both is O(N) -- the big-O notation throws
      away the "constant factor".
    </p>
    <p>It often happens that algorithms we need to compare for time efficiency
      have the same time complexity,
      but differents speeds.
      And it sometimes happens that this difference is relevant.
      When this happens, the slower algorithm is accused of having
      a "high constant factor".
    </p>
    <h3>OK, now about that constant factor</h3>
    <h3>Other considerations</h3>
    <ul>
      <li>Error diagnosis.
      </li>
      <li>Readability.
      </li>
      <li>Maintainabilty.
      </li></ul>
    <h3>For more about Marpa</h3>
    <p>Earley goes on to note that hand-written parsers can clearly be faster.
      At the time Earley wrote, this was the state of the art for production compilers and
      the world was such that languages remained static and hand-written approaches were
      reasonable.
      Today, hand-written parsers are once again the state of the art for production compilers,
      though expectations about the size and stabiity of languages,
      and perceptions about the desirability of large amounts of complex hand-crafted code,
      has changed considerably.
      But the speed remains the obstacle to anything better.  Or does it?
    </p>
    <p>Earley, writing in 1968, took it for granted that a hand-written parser
      was a highly optimized parser.  Four and a half decades of practice have taught
      us that in fact it is hard to keep complex hand-written parsers correct
      for evolving languages, and that they are in fact often slow.
      But let's assume we don't know that.
    </p>
    <p>
      Let's assume the competitors to Earley's are one hundred times as fast.
      So, slower by a factor of hundred means too slow, right?
      Wrong.
      The tendency is to repeat clear verdicts,
      rather than re-examine them,
      particularly if the field is complex.
      And parsing theory is not just complex.
      but it's out of fashion and off the funding radar.
      Which means nobody has thought about the relevance of Moore's Law.
    </p>
    <p>
      Marpa's latest version is
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2,
        which is available on CPAN</a>.
      Marpa's
      <a href="https://metacpan.org/module/JKEGL/Marpa-R2-2.052000/pod/Scanless/DSL.pod">SLIF
        is
        a new interface</a>,
      which represents a major increase
      in Marpa's "whipitupitude".
      The SLIF has tutorials
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/01/dsl_simpler2.html">here
      </a>
      and
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/01/announce_scanless.html">
        here</a>.
      Marpa has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page</a>,
      and of course it is the focus of
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        my "Ocean of Awareness" blog</a>.
    </p>
    <p>
      Comments on this post
      can be sent to the Marpa's Google Group:
      <code>marpa-parser@googlegroups.com</code>
    </p>
