<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>The Five Virtues of Parsers</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/will_want.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      In a previous post&lt;a id=&quot;footnote-1-ref&quot; href=&quot;#footnote-1&quot;&gt;[1]&lt;/a&gt;, I did a careful examination of the
      history of attempts to produce the ideal parser.
      In this one, I will look at the question in reverse --
      what makes the users of a parser
      happy or unhappy with it?
      That is, once a parser is discovered,
      what makes it successful?
      And once a parser is successful,
      what causes users to want more?
    &lt;/p&gt;
    &lt;h2&gt;The first major virtue: fast&lt;/h2&gt;
    &lt;p&gt;
      By one accounting,
      the first systematic attempt at parsing
      was via regular expressions.&lt;a id=&quot;footnote-2-ref&quot; href=&quot;#footnote-2&quot;&gt;[2]&lt;/a&gt;
      Regular expressions are still very much with us,
      so they obviously must demonstrate at least &lt;b&gt;some&lt;/b&gt;
      of the virtues that make a parser popular.
    &lt;/p&gt;
    &lt;p&gt;
      The most superficial of these is that
      a parser must be &quot;fast enough&quot;
      to please its users.
      A rigorous understanding of what &quot;fast enough&quot; means
      was slow to emerge,
      but it is now clear that, except in restricted uses,
      a parser must be linear or quasi-linear.&lt;a id=&quot;footnote-3-ref&quot; href=&quot;#footnote-3&quot;&gt;[3]&lt;/a&gt;
    &lt;/p&gt;
    &lt;h2&gt;The second major virtue: predictable&lt;/h2&gt;
    &lt;p&gt;
    Less obvious is what I will call &quot;predictability&quot;:
    It must be possible for a programmer,
    with reasonable effort,
    to know if her grammar
    will be parsed by the parser.
    In this respect, regular expressions can be called
    perfect --
    there's a handy notation for them.
    Anything in that notation is a regular expression,
    and will be parsed in linear time by your regular
    expression engine --
    you never need to desk-check.
    &lt;/p&gt;
    &lt;h2&gt;The first minor virtue: declaration-driven&lt;/h2&gt;
    &lt;p&gt;Since there is a exact notation
    for regular expression, it has the first minor virtue:
    it is declaration-driven.
    If a parser can automatically be generated from a compact
    notation, the parsing method is declaration-driven.
    &lt;/p&gt;
    &lt;p&gt;
    I call virtues minor
    if they are important in the eyes of the users,
    but of less importance than the major virtues.
    That &quot;declaration-driven&quot; is a minor virtue is clear
    from the history of parsing practice.
    &lt;/p&gt;
    &lt;h2&gt;The third major virtue: power&lt;/h2&gt;
    &lt;p&gt;Regular expressions, however,
    were not the end of the parsing story.
    If a programmer has a grammar she considers practical,
    she wants her parser to parse it.
    Here regular expressions often fail -- a lot of practical
    grammar are regular expressions, but many others are not.
    For regular expressions,
    one of the fails is ironic --
    the notation for regular expressions is recursive,
    but regular expressions cannot deal with recursion.
    &lt;/p&gt;
    &lt;p&gt;
    Grabs for power predate the parsing literature,
    which begins with Irons 1961.
    In terms of power,
    Irons parser is &quot;general&quot;,
    meaning that it can parse what are called
    the &quot;context-free grammars&quot;.
    &lt;/p&gt;
    &lt;p&gt;
    The context-free grammars are exactly those which can
    be written in BNF.
    This means that Irons parser had the same kind of predictability
    that regular expressions had -- it could parse every grammar
    written in its grammar notation.
    Again, there was no need to desk check,
    but this time the class of grammars that could be parsed was
    much, much larger.
    &lt;/p&gt;
    &lt;h2&gt;The fourth major virtue: reliability&lt;/h2&gt;
    &lt;p&gt;
    The Irons 1961 algorithm was perfectly predictable
    it was predictably powerful.
    And it could be fast.
    Iron 1961 was not predictably fast.
    In the general case,
    Irons 1961 used backtracking
    to achieve its power,
    so it would go exponential for many useful grammars.
    Irons 1961 could not be relied on to be fast.
    &lt;/p&gt;
    &lt;p&gt;
    Irons 1961 gave way to recursive descent.
    Recursive descent had many disadvantages compared to Irons 1961,
    but it was very fast,
    it was easy to customize.
    With customization.
    recursive descent was 
    reliably fast for arithmetic expressions,
    which made it an improvement over Irons.
    &lt;/p&gt;
    &lt;h2&gt;The second minor virtue: procedural&lt;/h2&gt;
    &lt;h2&gt;The fifth major virtue: error-friendly&lt;/h2&gt;
    &lt;p&gt;The other major virtues have always
    been objects of theoretical interest --
    they were in fact,
    the reason that the field of theory of algorithms
    was developed.
    &lt;/p&gt;
    &lt;p&gt;
    But theoreticians assumed that programmers always
    started with the grammar they intended to use,
    and never made a mistake.
    And theoreticians further assumed that if an input
    was wrong,
    it was enough for the parser to say so.
    It was assumed the programmer would be able to look
    at an input several meg long,
    instantly spot the error,
    mutter &quot;silly me&quot;
    correct it and rerun.
    &lt;/p&gt;
    In practice, parsers were as helpful as they could be,
    and programmers had been satisfied with that.
    Regular expressions and their inputs
    usually can be debugged by experiment,
    when the problem is not obvious.&lt;a id=&quot;footnote-4-ref&quot; href=&quot;#footnote-4&quot;&gt;[4]&lt;/a&gt;
    Recursive descent is procedural, and debugging can be approached that
    way.
    &lt;/p&gt;
    &lt;p&gt;
    It was assumed, therefore that LALR only had to be as helpful
    as it could be and users would cope.
    But LALR stretched this assumption to the limit.
    &lt;/p&gt;
    Recursive descent
    [ TO HERE ] &lt;br&gt;
    This would go without saying,
    except that solutions to the parsing problem are often advanced
    which trade power for reliability.
    An algorithm which can go quadratic or worse is often used to supplement
    an algorithm which lacks power.
    It's a kind of cross-breeding of algorithms.
    The hope is that the hybrid, in your use case,
    had the power of the slow algorithm and the
    speed of the fast one.
    This works a lot better in botany than it does in parsing.
    Once you have a successful cross in a plant,
    you can breed from that and expect good things.
    But if even once you've lucked out with a parse,
    your next parse is a just another new toss of the dice.
    &lt;/p&gt;
    &lt;p&gt;In 19XX it was thought the next step up in power,
    consistent with the Three Basic Virtues,
    might have been found:
    A parser called yacc parsed a class of grammars called LALR.
    LALR clearly had two of the Basic Virtues (fast and reliable)
    and it could be said to have the third if you were willing to stretch a point.
    Technically,
    it took a mathematician to tell if a grammar is LALR.
    But what was probably the most sophisticated LALR grammar was
    created by Larry Wall, a non-mathematician --
    so one could get sense of it after a few hard knocks.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      For more about
      Marpa, my own parsing project,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
    &lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p id=&quot;footnote-1&quot;&gt;1.
      Much of the material in this post is drawn from my
      &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
      V3 of my &quot;Parsing: A Timeline&quot;&lt;/a&gt;.
 &lt;a href=&quot;#footnote-1-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-2&quot;&gt;2.
      Strictly speaking,
      regular expressions are recognizers,
      not parsers
      (see &quot;Term: parser&quot; and
      &quot;Term: recognzier&quot; in
      &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;).
      This is because, in their pure form, regular expressions
      simply determine if the input is a match --
      they do not determine its structure.
      Nonetheless, regex engines in practical use
      often extend regular expressions,
      for example, by adding captures.
      At the low end, regexes often compete with parsers
      and why that is the case is very relevant to my
      concerns in this post.
 &lt;a href=&quot;#footnote-2-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-3&quot;&gt;3.
      For definitions of &quot;linear&quot; and &quot;quasi-linear&quot;,
      see the 'Term: &quot;linear&quot;' section of
      &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
      V3 of my &quot;Parsing: A Timeline&quot;&lt;/a&gt;.
 &lt;a href=&quot;#footnote-3-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-4&quot;&gt;4.
    It perhaps helps that regular expressions are less powerful,
    so the problems tend to be simpler.
    There are regex debuggers, but the fact they are not used
    much indicates, at least to this writer, that they aren't needed
    that much
 &lt;a href=&quot;#footnote-4-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Version 3 of &quot;Parsing: a timeline&quot;</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/04/timeline_v3.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      My most popular blog posts by far have been my two versions of &quot;Parsing: a timeline&quot;.
      I have just created
      &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;a
        3rd version&lt;/a&gt;,
      which has so many changes
      that it might be considered a new work.
      The new version is less Marpa-centric and several times as long.
      It covers new topics, including combinator and monadic
      parsing, and operator expression parsing.
      And sources are now provided for all material.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      For more about
      Marpa, my own parsing project,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Parsing: an expanded timeline</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/08/timeline2.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;The fourth century BCE&lt;/b&gt;:
      In India, Pannini creates
      a sophisticated description of the Sanskrit language,
      exact and complete, and including pronunciation.
      Sanskrit
      could be recreated using nothing but Pannini's grammar.
      Pannini's grammar is probably the first formal system of any kind, predating Euclid.
      Even today, nothing like it exists for any other natural language
      of comparable size or corpus.
      Pannini is the object of serious study today.
      But in the 1940's and 1950's Pannini is almost unknown in the West.
      His work has no direct effect on the other events in this timeline.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1943&lt;/b&gt;:
      Emil Post defines and studies a formal rewriting system using
      productions.
      With this, the process of reinventing Pannini in the West begins.
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;1948&lt;/b&gt;:
      Claude Shannon publishes the foundation paper of information theory.
      Andrey Markov's finite state processes are used heavily.
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;1952&lt;/b&gt;:
      Grace Hopper writes a linker-loader and
      &lt;a href=&quot;https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers&quot;&gt;
        describes it
        as a &quot;compiler&quot;&lt;/a&gt;.
      She seems to be the first person to use this term for a computer program.
      Hopper uses the term
      &quot;compiler&quot; in its original sense:
      &quot;something or someone that brings other things together&quot;.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1954&lt;/b&gt;:
      At IBM, a team under John Backus begins working
      on the language which will be called FORTRAN.
      The term &quot;compiler&quot; is still being used in Hopper's looser sense,
      instead of its modern one.
      In particular, there is no implication that the output of a &quot;compiler&quot;
      is ready for execution by a computer.
      &lt;!-- &quot;http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf
        pp. 133-134
      --&gt;
      The output of one 1954 &quot;compiler&quot;,
      for example, produces relative addresses,
      which need to be translated by hand before a machine can execute them.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1955&lt;/b&gt;:
      Noam Chomsky is awarded a Ph.D. in linguistics and accepts a teaching post at MIT.
      MIT does not have a linguistics department and
      Chomsky, in his linguistics course, is free to teach his own approach,
      highly original and very mathematical.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1956&lt;/b&gt;:
      &lt;!-- &quot;Three models&quot; --&gt;
      Chomsky publishes the paper which
      is usually considered the foundation of Western formal language theory.
      The paper advocates a natural language approach that involves
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;a bottom layer, using Markov's finite state processes;
      &lt;/li&gt;&lt;li&gt;a middle, syntactic layer, using context-free grammars and
        context-sensitive grammars; and
      &lt;/li&gt;&lt;li&gt;a top layer, which involves mappings or &quot;transformations&quot;
        of the output of the syntactic layer.
      &lt;/li&gt;&lt;/ul&gt;&lt;p&gt;
      These layers resemble, and will inspire,
      the lexical, syntactic and AST transformation phases
      of modern parsers.
      For finite state processes, Chomsky acknowledges Markov.
      The other layers seem to be Chomsky's own formulations --
      Chomsky does not cite Post's work.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1957&lt;/b&gt;:
      Steven Kleene discovers regular expressions,
      a very handy notation for Markov's processes.
      Regular expressions turn out to describe exactly the mathematical
      objects being studied as
      finite state automata,
      as well as some of the objects being studied as
      neural nets.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1957&lt;/b&gt;:
      Noam Chomsky publishes
      &lt;b&gt;Syntactic Structures&lt;/b&gt;,
      one of the most influential books of all time.
      The orthodoxy in 1957 is structural linguistics
      which argues, with Sherlock Holmes, that
      &quot;it is a capital mistake to theorize in advance of the facts&quot;.
      Structuralists start with the utterances in a language,
      and build upward.
    &lt;/p&gt;
    &lt;p&gt;
      But Chomsky claims that without a theory there
      are no facts: there is only noise.
      The Chomskyan approach is to start with a grammar, and use the corpus of
      the language to check its accuracy.
      Chomsky's approach will soon come to dominate linguistics.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1957&lt;/b&gt;:
      Backus's team makes the first FORTRAN compiler
      available to IBM customers.
      FORTRAN is the first high-level language
      that will find widespread implementation.
      As of this writing,
      it is the oldest language that survives in practical use.
      FORTRAN is a line-by-line language
      and its parsing is primitive.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1958&lt;/b&gt;:
      John McCarthy's LISP appears.
      LISP goes beyond the line-by-line syntax --
      it is recursively structured.
      But the LISP interpreter does not find the
      recursive structure:
      the programmer must explicitly
      indicate the structure herself,
      using parentheses.
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;1959&lt;/b&gt;:
      Backus invents a new notation to describe
      the IAL language (aka ALGOL).
      Backus's notation is influenced by his study of Post --
      he seems not to have read Chomsky until later.
      &lt;!-- http://archive.computerhistory.org/resources/text/Oral_History/Backus_John/Backus_John_1.oral_history.2006.102657970.pdf
      p. 25 --&gt;
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1960&lt;/b&gt;:
      Peter Naur
      improves the Backus notation
      and uses it to describe ALGOL 60.
      The improved notation will become known as Backus-Naur Form (BNF).
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;1960&lt;/b&gt;:
      The ALGOL 60 report
      specifies, for the first time, a block structured
      language.
      ALGOL 60 is recursively structured but the structure is
      implicit -- newlines are not semantically significant,
      and parentheses indicate syntax only in a few specific cases.
      The ALGOL compiler will have to find the structure.
      It is a case of 1960's optimism at its best.
      As the ALGOL committee is well aware, a parsing
      algorithm capable
      of handling ALGOL 60 does not yet exist.
      But the risk they are taking will soon pay off.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1960&lt;/b&gt;:
      A.E. Gleenie publishes his description of a compiler-compiler.
      &lt;!-- http://www.chilton-computing.org.uk/acl/literature/reports/p024.htm --&gt;
      Glennie's &quot;universal compiler&quot; is more of a methodology than
      an implementation -- the compilers must be written by hand.
      Glennie credits both Chomsky and Backus, and observes that the two
      notations are &quot;related&quot;.
      He also mentions Post's productions.
      Glennie may have been the first to use BNF as a description of a
      &lt;b&gt;procedure&lt;/b&gt;
      instead of as the description of a
      &lt;b&gt;Chomsky grammar&lt;/b&gt;.
      Glennie points out that the distinction is &quot;important&quot;.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;Chomskyan BNF and procedural BNF&lt;/b&gt;:
      BNF, when used as a Chomsky grammar, describes a set of strings,
      and does
      &lt;b&gt;not&lt;/b&gt;
      describe how to parse strings according to the grammar.
      BNF notation, if used to describe a procedure, is a set of instructions, to be
      tried in some order, and used to process a string.
      Procedural BNF describes a procedure first, and a language only indirectly.
    &lt;/p&gt;
    &lt;p&gt;
      Both procedural and Chomskyan BNF describe languages,
      but usually
      &lt;b&gt;not the same&lt;/b&gt;
      language.
      That is,
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;Suppose D is some BNF description.
      &lt;/li&gt;
      &lt;li&gt;Let P(D) be D interpreted as a procedure,
      &lt;/li&gt;
      &lt;li&gt;Let L(P(D)) be the language which the procedure P(D) parses.
      &lt;/li&gt;
      &lt;li&gt;Let G(D) be D interpreted as a Chomsky grammar.
      &lt;/li&gt;
      &lt;li&gt;Let L(G(D)) be the language which the grammar G(D) describes.
      &lt;/li&gt;
      &lt;li&gt;Then, usually, L(P(D)) != L(G(D)).
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      The pre-Chomskyan approach,
      using procedural BNF,
      is far more natural
      to someone trained as a computer programmer.
      The parsing problem appears to the programmer in the form of
      strings to be parsed,
      exactly the starting point of procedural BNF
      and pre-Chomsky parsing.
    &lt;/p&gt;
    &lt;p&gt;
      Even when the Chomskyan approach is pointed out,
      it does not at first seem very attractive.
      With the pre-Chomskyan approach,
      the examples of the language
      more or less naturally lead to a parser.
      In the Chomskyan approach
      the programmer has to search for
      an algorithm to parse strings according to his grammar --
      and the search for good algorithms to parse
      Chomskyan grammars has proved surprisingly
      long and difficult.
      Handling
      semantics is more natural with a Chomksyan approach.
      But, using captures, semantics
      can be added to a pre-Chomskyan parser
      and, with practice, this seems natural enough.
    &lt;/p&gt;
    &lt;p&gt;
      Despite the naturalness of the pre-Chomskyan approach
      to parsing, we will find that the first fully-described
      automated parsers are Chomskyan.
      This is a testimony to Chomsky's influence at the time.
      We will also see that Chomskyan parsers
      have been dominant ever since.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1961&lt;/b&gt;:
      In January,
      Ned Irons publishes a paper describing his ALGOL 60
      parser.
      It is the first paper to fully describe any parser.
      The Irons algorithm is Chomskyan and top-down
      with a &quot;left corner&quot; element.
      The Irons algorithm
      is general,
      meaning that it can parse anything written in BNF.
      It is syntax-driven (aka declarative),
      meaning that the parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1961&lt;/b&gt;:
      Peter Lucas publishes the first
      description of a purely top-down parser.
      This can be considered to be recursive descent,
      though in Lucas's
      paper the algorithm has a
      syntax-driven implementation, useable only for
      a restricted class of grammars.
      Today we think of recursive descent as a methodology for
      writing parsers by hand.
      Hand-coded approaches became more popular
      in the 1960's due to three factors:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        Memory and CPU were both extremely limited.
        Hand-coding paid off, even when the gains were small.
      &lt;/li&gt;
      &lt;li&gt;
        Non-hand coded top-down parsing,
        of the kind Lucas's syntax-driven
        approach allowed, is a very weak parsing technique.
        It was (and still is) often necessary
        to go beyond its limits.
      &lt;/li&gt;
      &lt;li&gt;
        Top-down parsing is intuitive -- it essentially means calling
        subroutines.
        It therefore requires little or
        no knowledge of parsing theory.
        This makes it a good fit for hand-coding.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;&lt;b&gt;1963&lt;/b&gt;:
      L. Schmidt, Howard Metcalf, and Val Schorre present papers
      on syntax-directed compilers at a Denver conference.
      &lt;!-- Schorre 1964, p. D1.3-1 --&gt;
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1964&lt;/b&gt;:
      Schorre publishes a paper on the Meta II
      &quot;compiler writing language&quot;,
      summarizing the papers of the 1963 conference.
      Schorre cites both Backus and Chomsky as sources
      for Meta II's notation.
      Schorre notes
      that his parser
      is &quot;entirely different&quot; from that of Irons 1961 --
      in fact it is pre-Chomskyan.
      Meta II is a template, rather
      than something that readers can use,
      but in principle it can be turned
      into a fully automated compiler-compiler.
      &lt;!-- Schorre 1964, p. D1.3-1
    http://ibm-1401.info/Meta-II-schorre.pdf
    --&gt;
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;1965&lt;/b&gt;:
      Don Knuth invents LR parsing.
      The LR algorithm is deterministic,
      Chomskyan and bottom-up,
      but it is not thought to be practical.
      Knuth is primarily interested
      in the mathematics.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1968&lt;/b&gt;: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is Chomskyan, syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's algorithm is both top-down and bottom-up at once --
      it uses dynamic programming and keeps track of the parse
      in tables.
      Earley's approach makes a lot of sense
      and looks very promising indeed,
      but there are three serious issues:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;First, there is a bug in the handling of zero-length rules.
      &lt;/li&gt;
      &lt;li&gt;Second, it is quadratic for right recursions.
      &lt;/li&gt;
      &lt;li&gt;Third, the bookkeeping required to set up the tables is,
        by the standards of 1968 hardware, daunting.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;&lt;b&gt;1969&lt;/b&gt;:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
      LALR looks practical.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1969&lt;/b&gt;:
      Ken Thompson writes the &quot;ed&quot; editor as one of the first components
      of UNIX.
      At this point, regular expressions are an esoteric mathematical formalism.
      Through the &quot;ed&quot; editor and its descendants,
      regular expressions will become
      an everyday
      part of the working programmer's toolkit.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;Recognizers&lt;/b&gt;:
      In comparing algorithms, it can be important to keep in mind whether
      they are recognizers or parsers.
      A
      &lt;b&gt;recognizer&lt;/b&gt;
      is a program which takes a string and produces a &quot;yes&quot;
      or &quot;no&quot; according to whether a string is in part of a language.
      Regular expressions are typically used as recognizers.
      A
      &lt;b&gt;parser&lt;/b&gt;
      is a program which takes a string and produces a tree reflecting
      its structure according to a grammar.
      The algorithm for a compiler clearly must be a parser, not a recognizer.
      Recognizers can be, to some extent,
      used as parsers by introducing captures.
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;1972&lt;/b&gt;:
      Alfred Aho and Jeffrey Ullman
      publish a two volume textbook summarizing the theory
      of parsing.
      This book is still important.
      It is also distressingly up-to-date --
      progress in parsing theory slowed dramatically
      after 1972.
      Aho and Ullman describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1972&lt;/b&gt;:
      Under the names TDPL and GTDPL,
      Aho and Ullman investigate
      the non-Chomksyan parsers in
      the Schorre lineage.
      They note that
      &quot;it can be quite difficult to determine
      what language is defined by a TDPL parser&quot;.
      That is,
      GTDPL parsers do whatever they do,
      and that whatever is something
      the programmer in general will not be able to describe.
      The best a programmer can usually do
      is to create a test suite and fiddle with the GTDPL description
      until it passes.
      Correctness cannot be established in any stronger sense.
      GTDPL is an extreme form of
      the old joke that &quot;the code is the documentation&quot; --
      with GTDPL nothing
      documents the language of the parser,
      not even the code.
    &lt;/p&gt;
    &lt;p&gt;
      GTDPL's obscurity buys nothing in the way of additional parsing
      power.
      Like all non-Chomskyan parsers,
      GTDPL is basically a extremely powerful recognizer.
      Pressed into service as a parser, it is comparatively weak.
      As a parser, GTDPL
      is essentially equivalent to Lucas's 1961 syntax-driven
      algorithm,
      which was in turn a restricted form of recursive descent.
    &lt;/p&gt;
    &lt;p&gt;
      At or around this time,
      rumor has it
      that the main line of development for GTDPL parsers
      is classified secret by the US government.
      &lt;!-- http://www.wikiwand.com/en/Talk:Metacompiler/Archive_2 --&gt;
      GTDPL parsers have the property that even small changes
      in GTDPL parsers can be very labor-intensive.
      For some government contractors,
      GTDPL parsing provides steady work for years to come.
      Public interest in GTDPL fades.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1975&lt;/b&gt;:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1977&lt;/b&gt;:
      The first &quot;Dragon book&quot; comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters &quot;LALR&quot;.
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1979&lt;/b&gt;: Bell Laboratories releases Version 7 UNIX.
      V7 includes what is, by far,
      the most comprehensive, useable and easily available
      compiler writing toolkit yet developed.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1979&lt;/b&gt;:
      Part of the V7 toolkit is Yet Another Compiler Compiler (YACC).
      YACC is LALR-powered.
      Despite its name, YACC is the first compiler-compiler
      in the modern sense.
      For some useful languages, the process of going from
      Chomskyan specification to executable is fully automated.
      Most practical languages,
      including
      the C language
      and YACC's own input language,
      still require manual hackery.
      Nonetheless,
      after two decades of research,
      it seems that the parsing problem is solved.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1987&lt;/b&gt;:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses YACC and LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1991&lt;/b&gt;:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of the Earley algorithm.
    &lt;/p&gt;
    &lt;p&gt;
      But Earley parsing is almost forgotten.
      Twenty years will pass
      before anyone writes a practical
      implementation of Leo's algorithm.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1990's&lt;/b&gt;:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is &quot;fast but stupid&quot;.
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;2000&lt;/b&gt;:
      Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;2002&lt;/b&gt;:
      John Aycock and R. Nigel Horspool
      publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;2004&lt;/b&gt;:
      Bryan Ford publishes his paper on PEG.
      Implementers by now are avoiding YACC,
      and it seems
      as if there might soon be no syntax-driven algorithms in practical
      use.
      Ford fills this gap by repackaging the nearly-forgotten GTDPL.
      Ford adds packratting, so that PEG is always linear,
      and provides PEG with an attractive new syntax.
      But nothing has been done to change
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html&quot;&gt;
        the problematic behaviors&lt;/a&gt;
      of GTDPL.
    &lt;/p&gt;&lt;p&gt;&lt;b&gt;2006&lt;/b&gt;:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;Today&lt;/b&gt;:
      After five decades of parsing theory,
      the state of the art seems to be back
      where it started.
      We can imagine someone taking
      Ned Iron's original 1961 algorithm
      from the first paper ever published describing a parser,
      and republishing it today.
      True, he would have to
      translate its code from the mix of assembler and
      ALGOL into something more fashionable, say Haskell.
      But with that change,
      it might look like a breath of fresh air.
    &lt;/p&gt;
    &lt;p&gt;
    &lt;/p&gt;&lt;h3&gt;Marpa: an afterword&lt;/h3&gt;&lt;p&gt;
      The recollections of my teachers cover most of
      this timeline.
      My own begin around 1970.
      Very early on, as a graduate student,
      I became unhappy with the way
      the field was developing.
      Earley's algorithm looked interesting,
      and it was something I returned to on and off.
    &lt;/p&gt;
    &lt;p&gt;
      The original vision of the 1960's was a parser that
      was
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;efficient,
      &lt;/li&gt;
      &lt;li&gt;practical,
      &lt;/li&gt;
      &lt;li&gt;general, and
      &lt;/li&gt;
      &lt;li&gt;syntax-driven.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      By 2010 this vision
      seemed to have gone the same way as many other 1960's dreams.
      The rhetoric stayed upbeat, but
      parsing practice had become a series of increasingly desperate
      compromises.
    &lt;/p&gt;
    &lt;p&gt;
      But,
      while nobody was looking for them,
      the solutions to the problems encountered in the 1960's
      had appeared in the literature.
      Aycock and Horspool had solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and are probably no longer relevant in any case --
      cache misses are now the bottleneck.
    &lt;/p&gt;
    &lt;p&gt;
      The programmers of the 1960's would have been prepared
      to trust a fully declarative Chomskyan parser.
      With the experience with LALR in their collective consciousness,
      modern programmers might be more guarded.
      As Lincoln said, &quot;Once a cat's been burned,
      he won't even sit on a cold stove.&quot;
      But I found it straightforward to rearrange the Earley parse engine
      to allow efficient
      event-driven handovers between procedural and syntax-driven
      logic.
      And Earley tables provide the procedural logic with
      full knowledge of the state of the
      parse so far,
      so that
      Earley's algorithm is a better platform
      for hand-written procedural logic than recursive descent.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      My implementation of Earley's algorithm is called Marpa.
      For more about Marpa, there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Introduction to Marpa Book in progress</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/03/book_intro.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;&lt;p&gt;
      What follows is a summary of the features
      of the Marpa algorithm,
      followed by a discussion of potential
      applications.
      It refers to itself as a &quot;monograph&quot;, because it
      is a draft of part of the introduction to
      a technical monograph on the Marpa algorithm.
      I hope the entire monograph will appear in a few
      weeks.
    &lt;/p&gt;
    &lt;h2&gt;The Marpa project&lt;/h2&gt;
    &lt;p&gt;
      The Marpa project was intended to create
      a practical and highly available tool
      to generate and use general context-free
      parsers.
      Tools of this kind
      had long existed
      for LALR and
      regular expressions.
      But, despite an encouraging academic literature,
      no such tool had existed for context-free parsing.
      The first stable version of Marpa was uploaded to
      a public archive on Solstice Day 2011.
      This monograph describes the algorithm used
      in the most recent version of Marpa,
      Marpa::R2.
      It is a simplification of the algorithm presented
      in
      &lt;a href=&quot;https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer&quot;&gt;my
      earlier paper&lt;/a&gt;.
      &lt;h2&gt;A proven algorithm&lt;/h2&gt;
      While the presentation in this monograph is theoretical,
      the approach is practical.
      The Marpa::R2 implementation has been widely available
      for some time,
      and has seen considerable use,
      including in production environments.
      Many of the ideas in the parsing literature
      satisfy theoretical criteria,
      but in practice turn out to face significant obstacles.
      An algorithm may be as fast as reported, but may turn
      out not to allow
      adequate error reporting.
      Or a modification may speed up the recognizer,
      but require additional processing at evaluation time,
      leaving no advantage to compensate for
      the additional complexity.
      &lt;/p&gt;
      &lt;p&gt;
      In this monograph, I describe the Marpa
      algorithm
      as it was implemented for Marpa::R2.
      In many cases,
      I believe there are better approaches than those I
      have described.
      But I treat these techniques,
      however solid their theory,
      as conjectures.
      Whenever I mention a technique
      that was not actually implemented in
      Marpa::R2,
      I will always explicitly state that
      that technique is not in Marpa as implemented.
      &lt;h2&gt;Features&lt;/h2&gt;
      &lt;h3&gt;General context-free parsing&lt;/h3&gt;
      As implemented,
      Marpa parses
      all &quot;proper&quot; context-free grammars.
      The
      proper context-free grammars are those which
      are free of cycles,
      unproductive symbols,
      and
      inaccessible symbols.
      Worst case time bounds are never worse than
      those of Earley's algorithm,
      and therefore never worse than O(n**3).
      &lt;h3&gt;Linear time for practical grammars&lt;/h3&gt;
      Currently, the grammars suitable for practical
      use are thought to be a subset
      of the deterministic context-free grammars.
      Using a technique discovered by
      Joop Leo,
      Marpa parses all of these in linear time.
      Leo's modification of Earley's algorithm is
      O(n) for LR-regular grammars.
      Leo's modification
      also parses many ambiguous grammars in linear
      time.
      &lt;h3&gt;Left-eidetic&lt;/h3&gt;
      The original Earley algorithm kept full information
      about the parse ---
      including partial and fully
      recognized rule instances ---
      in its tables.
      At every parse location,
      before any symbols
      are scanned,
      Marpa's parse engine makes available
      its
      information about the state of the parse so far.
      This information is
      in useful form,
      and can be accessed efficiently.
      &lt;h3&gt;Recoverable from read errors&lt;/h3&gt;
      When
      Marpa reads a token which it cannot accept,
      the error is fully recoverable.
      An application can try to read another
      token.
      The application can do this repeatedly
      as long as none of the tokens are accepted.
      Once the application provides
      a token that is accepted by the parser,
      parsing will continue
      as if the unsuccessful read attempts had never been made.
      &lt;h3&gt;Ambiguous tokens&lt;/h3&gt;
      Marpa allows ambiguous tokens.
      These are often useful in natural language processing
      where, for example,
      the same word might be a verb or a noun.
      Use of ambiguous tokens can be combined with
      recovery from rejected tokens so that,
      for example, an application could react to the
      rejection of a token by reading two others.
      &lt;h2&gt;Using the features&lt;/h2&gt;
      &lt;h3&gt;Error reporting&lt;/h3&gt;
      An obvious application of left-eideticism is error
      reporting.
      Marpa's abilities in this respect are
      ground-breaking.
      For example,
      users typically regard an ambiguity as an error
      in the grammar.
      Marpa, as currently implemented,
      can detect an ambiguity and report
      specifically where it occurred
      and what the alternatives were.
      &lt;h3&gt;Event driven parsing&lt;/h3&gt;
      As implemented,
      Marpa::R2
      allows the user to define &quot;events&quot;.
      Events can be defined that trigger when a specified rule is complete,
      when a specified rule is predicted,
      when a specified symbol is nulled,
      when a user-specified lexeme has been scanned,
      or when a user-specified lexeme is about to be scanned.
      A mid-rule event can be defined by adding a nulling symbol
      at the desired point in the rule,
      and defining an event which triggers when the symbol is nulled.
      &lt;h3&gt;Ruby slippers parsing&lt;/h3&gt;
      Left-eideticism, efficient error recovery,
      and the event mechanism can be combined to allow
      the application to change the input in response to
      feedback from the parser.
      In traditional parser practice,
      error detection is an act of desperation.
      In contrast,
      Marpa's error detection is so painless
      that it can be used as the foundation
      of new parsing techniques.
      &lt;/p&gt;
      &lt;p&gt;
      For example,
      if a token is rejected,
      the lexer is free to create a new token
      in the light of the parser's expectations.
      This approach can be seen
      as making the parser's
      &quot;wishes&quot; come true,
      and I have called it
      &quot;Ruby Slippers Parsing&quot;.
      &lt;/p&gt;
      &lt;p&gt;
      One use of the Ruby Slippers technique is to
      parse with a clean
      but oversimplified grammar,
      programming the lexical analyzer to make up for the grammar's
      short-comings on the fly.
      As part of Marpa::R2,
      the author has implemented an HTML parser,
      based on a grammar that assumes that all start
      and end tags are present.
      Such an HTML grammar is too simple even to describe perfectly
      standard-conformant HTML,
      but the lexical analyzer is
      programmed to supply start and end tags as requested by the parser.
      The result is a simple and cleanly designed parser
      that parses very liberal HTML
      and accepts all input files,
      in the worst case
      treating them as highly defective HTML.
      &lt;h3&gt;Ambiguity as a language design technique&lt;/h3&gt;
      In current practice, ambiguity is avoided in language design.
      This is very different from the practice in the languages humans choose
      when communicating with each other.
      Human languages exploit ambiguity in order to design highly flexible,
      powerfully expressive languages.
      For example,
      the language of this monograph, English, is notoriously
      ambiguous.
      &lt;/p&gt;
      &lt;p&gt;
      Ambiguity of course can present a problem.
      A sentence in an ambiguous
      language may have undesired meanings.
      But note that this is not a reason to ban potential ambiguity ---
      it is only a problem with actual ambiguity.
      &lt;/p&gt;
      &lt;p&gt;
      Syntax errors, for example, are undesired, but nobody tries
      to design languages to make syntax errors impossible.
      A language in which every input was well-formed and meaningful
      would be cumbersome and even dangerous:
      all typos in such a language would be meaningful,
      and parser would never warn the user about errors, because
      there would be no such thing.
      &lt;/p&gt;
      &lt;p&gt;
      With Marpa, ambiguity can be dealt with in the same way
      that syntax errors are dealt with in current practice.
      The language can be designed to be ambiguous,
      but any actual ambiguity can be detected
      and reported at parse time.
      This exploits Marpa's ability
      to report exactly where
      and what the ambiguity is.
      Marpa::R2's own parser description language, the SLIF,
      uses ambiguity in this way.
      &lt;h3&gt;Auto-generated languages&lt;/h3&gt;
      In 1973, 
      &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0022000073800509&quot;&gt;
      &amp;#x10c;ulik and Cohen&lt;/a&gt; pointed out that the ability
      to efficiently parse LR-regular languages
      opens the way to auto-generated languages.
      In particular,
      &amp;#x10c;ulik and Cohen note that a parser which
      can parse any LR-regular language will be
      able to parse a language generated using syntax macros.
      &lt;h3&gt;Second order languages&lt;/h3&gt;
      In the literature, the term &quot;second order language&quot;
      is usually used to describe languages with features
      which are useful for second-order programming.
      True second-order languages --- languages which
      are auto-generated
      from other languages ---
      have not been seen as practical,
      since there was no guarantee that the auto-generated
      language could be efficiently parsed.
      &lt;/p&gt;
      &lt;p&gt;
      With Marpa, this barrier is raised.
      As an example,
      Marpa::R2's own parser description language, the SLIF,
      allows &quot;precedenced rules&quot;.
      Precedenced rules are specified in an extended BNF.
      The BNF extensions allow precedence and associativity
      to be specified for each RHS.
      &lt;/p&gt;
      &lt;p&gt;
      Marpa::R2's precedenced rules are implemented as
      a true second order language.
      The SLIF representation of the precedenced rule
      is parsed to create a BNF grammar which is equivalent,
      and which has the desired precedence.
      Essentially,
      the SLIF does a standard textbook transformation.
      The transformation starts
      with a set of rules,
      each of which has a precedence and
      an associativity specified.
      The result of the transformation is a set of
      rules in pure BNF.
      The SLIF's advantage is that it is powered by Marpa,
      and therefore the SLIF can be certain that the grammar
      that it auto-generates will
      parse in linear time.
      &lt;/p&gt;
      &lt;p&gt;
      Notationally, Marpa's precedenced rules
      are an improvement over
      similar features
      in LALR-based parser generators like
      yacc or bison.
      In the SLIF,
      there are two important differences.
      First, in the SLIF's precedenced rules,
      precedence is generalized, so that it does
      not depend on the operators:
      there is no need to identify operators,
      much less class them as binary, unary, etc.
      This more powerful and flexible precedence notation
      allows the definition of multiple ternary operators,
      and multiple operators with arity above three.
      &lt;/p&gt;
      &lt;p&gt;
      Second, and more important, a SLIF user is guaranteed
      to get exactly the language that the precedenced rule specifies.
      The user of the yacc equivalent must hope their
      syntax falls within the limits of LALR.&lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      Marpa has a
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>What parser do birds use?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/03/parus.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;blockquote&gt;
      &quot;Here we provide, to our knowledge, the first unambiguous experimental evidence for compositional syntax in a non-human vocal system.&quot;
      --
      &lt;a href=&quot;http://www.nature.com/ncomms/2016/160308/ncomms10986/full/ncomms10986.html&quot;&gt;
        &quot;Experimental evidence for compositional syntax in bird calls&quot;,
        Toshitaka N. Suzuki,	David Wheatcroft	&amp;amp; Michael Griesser
        &lt;emph&gt;Nature Communications&lt;/emph&gt;
        7, Article number: 10986
      &lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      In this post I look at a subset of the language
      of the
      &lt;a href=&quot;https://en.wikipedia.org/wiki/Great_tit&quot;&gt;
        Japanese great tit&lt;/a&gt;,
      also known as Parus major.
      The above cited article presents evidence
      that bird brains can parse this language.
      What about standard modern computer parsing methods?
      Here is the subset -- probably a tiny one --
      of the language actually used by Parus major.
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
      S ::= ABC
      S ::= D
      S ::= ABC D
      S ::= D ABC
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
    &lt;/p&gt;
    &lt;h2&gt;Classifying the Parus major grammar&lt;/h2&gt;
    &lt;p&gt;Grammophone is a very handy
      &lt;a href=&quot;http://mdaines.github.io/grammophone/#&quot;&gt;
        new tool&lt;/a&gt;
      for classifying grammars.
      Its own parser is somewhat limited, so that it requires a period
      to mark the end of a rule.
      The above grammar is in Marpa's SLIF format, which is smart enough to
      use the &quot;&lt;tt&gt;::=&lt;/tt&gt;&quot;
      operator to spot the beginning and end of rules,
      just as the human eye does.
      Here's the same grammar converted into a form acceptable to Grammophone:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
      S -&gt; ABC .
      S -&gt; D .
      S -&gt; ABC D .
      S -&gt; D ABC .
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      Grammophone tells us that the Parus major grammar is not LL(1),
      but that it is LALR(1).
    &lt;/p&gt;
    &lt;h2&gt;What does this mean?&lt;/h2&gt;
    &lt;p&gt;LL(1) is the class of grammar parseable by top-down methods:
      it's the best class for characterizing most parsers in current use,
      including recursive descent, PEG,
      and Perl 6 grammars.
      All of these parsers fall short of dealing with the Parus major language.
    &lt;/p&gt;
    &lt;p&gt;
      LALR(1) is probably most well-known from its implementations in
      bison and yacc.
      While able to handle this subset of Parus's language,
      LALR(1) has its own, very strict, limits.
      Whether LALR(1) could handle the full
      complexity of Parus
      language is a serious question.
      But it's a question that in practice would probably not arise.
      LALR(1) has horrible error handling properties.
    &lt;/p&gt;
    &lt;p&gt;
      When the input is correct and within its limits,
      an LALR-driven parser is fast and works well.
      But if the input is not perfectly correct,
      LALR parsers produce
      no useful analysis of what went wrong.
      If Parus hears &quot;d abc d&quot;,
      a parser like Marpa, on the other hand, can produce something like
      this:
    &lt;/p&gt;
    &lt;blockquote&gt;&lt;pre&gt;
# * String before error: abc d\s
# * The error was at line 1, column 7, and at character 0x0064 'd', ...
# * here: d
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      Parus uses its language in predatory contexts,
      and one can assume that a Parus with a preference for parsers whose
      error handling is on an LALR(1) level
      will not be keeping its alleles in the gene pool for very
      long.
    &lt;/p&gt;&lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      Those readers content with sub-Parus parsing methods may stop reading here.
      Those with greater parsing ambitions, however, may
      wish to learn more about Marpa.
      A Marpa test script for parsing the Parus subset is in
      &lt;a href=https://gist.github.com/jeffreykegler/b5b8ba349b8c6e5c2e54&gt;a Github gist.&lt;/a&gt;
      Marpa has a
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  </channel>
</rss>
