<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>Parsing: an expanded timeline</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/08/timeline2.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;[ Revised 10 Aug 2016 ]
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;The fourth century BCE&lt;/b&gt;:
    In India, Pannini creates a grammar of the Sanskrit language.
    This is a sophisticated description of the Sanskrit language,
    exact and complete, and including pronunciation, so that Sanskrit
    could be recreated using nothing but Pannini's grammar.
    Pannini's grammar is probably the first formal system of any kind, predating Euclid.
    Even today, nothing like it exists for any other natural language
    of comparable size or corpus.
    Pannini is the object of serious study today.
    But in the 1940's and 1950's Pannini is almost unknown in the West and,
    if it has an effect on the other events in this timeline,
    it is very indirect.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1943&lt;/b&gt;:
    Emil Post defines and studies a formal rewriting system using
    productions.
    With this, the process of reinventing Pannini begins.
    &lt;p&gt;&lt;b&gt;1948&lt;/b&gt;:
    Claude Shannon publishes the foundation paper of information theory.
    Andrey Markov's finite state processes are used heavily.
    &lt;p&gt;&lt;b&gt;1952&lt;/b&gt;:
    Grace Hopper writes a linker-loader and
    &lt;a href=&quot;https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers&quot;&gt;
    describes it
    as a &quot;compiler&quot;&lt;/a&gt;.
    She seems to be the first person to use this term for a computer program.
    Hopper used the term
    &quot;compiler&quot; in its original sense:
    &quot;something or someone that bring other things together&quot;.
    &lt;p&gt;&lt;b&gt;1956&lt;/b&gt;:
    Noam Chomsky the paper which
    is usually considered the foundation of Western formal language theory.
    It contains one form
    of what we now know as the Chomsky hierarchy:
    &lt;ul&gt;
    &lt;li&gt;Markov's finite state processes, which probably came to
    Chomsky via Shannon;
    &lt;li&gt;Context-free grammars, and
    &lt;li&gt;Context-sensitive grammars.
    &lt;/ul&gt;
    For his finite state processes, Chomsky acknowledges Markov,
    who may have come to the attention of linguists via Shannon's
    work.
    The last two seem to be Chomsky's own formulations --
    Chomsky does not cite Post's work.
    &lt;p&gt;&lt;b&gt;1957&lt;/b&gt;:
    Steven Kleene discovers regular expressions,
    a very handy notation for Markov's processes.
    These turn out also to describe exactly the class of languages
    that is also the object of the research into neural nets
    and finite state automata.
    &lt;p&gt;&lt;b&gt;1957&lt;/b&gt;:
    Noam Chomsky publishes the first book
    on his theory of language,
    &lt;b&gt;Syntactic Structures&lt;/b&gt;.
    This is one of the most influential books of all time.
    In Chomsky's theory language is processed
    in multiple passes,
    and there is a morphophonemic pass,
    a syntactic pass, and a transformation pass.
    There are resemble, and inspired the lexical,
    syntactic and AST processing passes of later compilers.
    &lt;/p&gt;
    &lt;p&gt;Depending on your outlook,
    Chomsky either discovers a new,
    or rehabilitates a traditional,
    viewpoint of languages.
    The orthodoxy in 1967 is structural linguistics,
    which argued, with Sherlock Holmes, that
    &quot;it is a capital mistake to theorize in advance of the facts&quot;.
    Chomsky reversed this, claiming that without a theory there
    are no facts: there is only noise.
    The structuralist approach is to start with the utterances in a language,
    and build the structure up from that.
    The Chomskyite approach is to start with a grammar, and use the corpus of
    the language to check its accuracy.
    Within linguistics and philosophy, Chomsky claim is earth-shaking
    and still controversial.
    &lt;/p&gt;
    &lt;p&gt;
    In computer science, the effect is revolutionary.
    Interest in computer languages picks up,
    the ambition of the work increases sharply,
    and a Chomskian grammar-driven or syntax-driven approach dominates.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1957&lt;/b&gt;:
    John Backus' team creates FORTRAN.
    This is the first high-level language
    that will find widespread implementation.
    FORTRAN is not a block structured language, and is parsed line-by-line,
    using ad hoc methods.
    &lt;p&gt;&lt;b&gt;1958&lt;/b&gt;:
    John McCarthy's LISP appears.
    LISP is recursively structured,
    but LISP interpreter is of little help in its parsing --
    the programmer must indicate the structure herself,
    using parentheses.
    &lt;p&gt;&lt;b&gt;Compilers&lt;/b&gt;:
    Readers of the literature about the 1950's will sometimes get the impression
    of an Atlantean Age,
    a period which had access to technology more sophisticated
    that that of later times.
    This is misleading.
    The term &quot;compiler&quot; in the 1950's was used far more widely than it is now.
    In particular, there was no implication that the output of a &quot;compiler&quot;
    was something ready for execution by a computer.
    John Backus, for example,
    &lt;a href=&quot;http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf&quot;&gt;
    describes (pp. 133-134)&lt;/a&gt; a 1954 &quot;compiler&quot;
    which produces relative addresses, that needed to be translated to
    absolute addresses by hand.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1959&lt;/b&gt;:
    Backus invents a new notation to describe
    the IAL language.
    Backus's notation is influenced by Post,
    &lt;p&gt;&lt;b&gt;1960&lt;/b&gt;:
    Peter Nauer, the author of the ALGOL 60 report, 
    improved the notation Backus used to IAL,
    and uses it to describe the new language.
    It will become known as Backus-Nauer form.
    Backus's work seems to be independent of Chomsky,
    but the grammars described by BNF will soon be found to
    be the exact equivalent of Chomsky's context-free grammars.
    &lt;p&gt;&lt;b&gt;1960&lt;/b&gt;:
      The ALGOL 60 report
      specifies, for the first time, a block structured
      language.
      The ALGOL committee is well aware
      that
      nobody knows how to parse such a language.
      In fact, nobody has ever published a paper describing
      a parser.
      But they believe that,
      if they specify a block-structured
      language, a parser for it will be invented.
      Risky as this approach is, it pays off ...
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;January 1961&lt;/b&gt;:
    Ned Irons publishes a paper describing his ALGOL 60
    parser.
    It is the first paper to describe any parser.
      The Irons algorithm is a left parser --
      a form of recursive descent.
      Unlike modern
      recursive descent,
      the Irons algorithm
      is general and syntax-driven.
      &quot;General&quot; means it can parse anything written in BNF.
      &quot;Syntax-driven&quot; (aka declarative) means that parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1961&lt;/b&gt;:
    Lucas publishes the first description of recursive descent
    in its pure form.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;Left parsing&lt;/b&gt;:
    Lucas described a syntax-driven implementation, useable only for
    a restricted class of grammars.
    In the 1960's hand-coded approaches to recursive descent
    become more popular.
      Three factors were at work:
      &lt;ul&gt;
      &lt;li&gt;
      Memory and CPU were both extremely limited.
      Hand-coding paid off, even when the gains were small.
      &lt;li&gt;
      Pure left parsing, of the kind Lucas's syntax driven
      approach allowed, is a very weak parsing technique.
      It was often necessary
      to go beyond its limits.
      &lt;li&gt;
      Left parsing is intuitive, requiring little or
      no knowledge of parsing theory.
      This makes it a good fit for hand-coding.
      &lt;/ul&gt;
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1965&lt;/b&gt;:
    Don Knuth invents LR parsing.
      Knuth is primarily interested
      in the mathematics.
      Knuth describes a parsing algorithm,
      but it is not thought practical.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1965&lt;/b&gt;:
    McClure publishes a description of a non-Chomskian parser.
    It consists of a set of routines which, on success,
    absorb input and produce output,
    and on failure pass control on to the next option.
    &lt;p&gt;&lt;b&gt;1968&lt;/b&gt;: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's core idea is to
      track everything about the parse in tables.
      Earley's algorithm is enticing, but it has three major issues:
      &lt;ul&gt;
      &lt;li&gt;First, there is a bug in the handling of zero-length rules.
      &lt;li&gt;Second, it is quadratic for right recursions.
      &lt;li&gt;Third, the bookkeeping required to set up the tables is,
      by the standards of 1968 hardware, daunting.
      &lt;/ul&gt;
    &lt;p&gt;&lt;b&gt;1969&lt;/b&gt;:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1972&lt;/b&gt;:
      Aho and Ullmann describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    &lt;p&gt;&lt;b&gt;1975&lt;/b&gt;:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1977&lt;/b&gt;:
      The first &quot;Dragon book&quot; comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters &quot;LALR&quot;.
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1979&lt;/b&gt;: Bell Laboratories releases Version 7 UNIX.
	V7 includes what is, by far,
		the most comprehensive, useable and easily available
		compiler writing toolkit yet developed.
	 Central to the toolkit is
	 yacc, an LALR based parser generator.
	  With a bit of hackery,
	  yacc parses its own input language,
	  as well as the language of V7's main compiler,
	  the portable C compiler.
	  After two decades of research,
	  it seems that the parsing problem is solved.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1987&lt;/b&gt;:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1991&lt;/b&gt;:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of Earley algorithm.
      But Earley parsing is almost forgotten.
      Twenty years will pass
      before anyone writes a practical
      implementation of Leo's algorithm.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;1990's&lt;/b&gt;:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is &quot;fast but stupid&quot;.
    &lt;/p&gt;&lt;b&gt;2000&lt;/b&gt;:
    Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;2002&lt;/b&gt;:
      Aycock&amp;Horspool publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;2004&lt;/b&gt;:
    Ford publishes his paper on PEG.
    Implementers by now are avoiding YACC,
    and the Irons, Earley and  Lucas algorithms are forgotten, so it seemed
    as if there would soon be no syntax-driven algorithms in practical
    use.
    Ford's PEG fills this looming gap.
    PEG has an attractive new syntax,
    but the underlying algorithm is
    that of the old compiler-compilers,
    and nothing has been done to change
    &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html&quot;&gt;
    their tricky behaviors&lt;/a&gt;.
    &lt;p&gt;&lt;b&gt;2006&lt;/b&gt;:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    &lt;/p&gt;
    &lt;p&gt;&lt;b&gt;2000 to today&lt;/b&gt;:
    With the retreat from LALR comes a collapse in the
      prestige of parsing theory.
      After a half century,
      we seem to be back
      where we started.
      If you took Ned Iron's original 1961 algorithm,
      changed the names and dates,
      and translated the code from the mix of assembler and
      ALGOL into Haskell,
      you would easily republish it today,
      and bill it as 
      as revolutionary and new.
    &lt;/p&gt;
    &lt;p&gt;
    &lt;h3&gt;Marpa&lt;/h3&gt;
      Over the years,
      I had come back to Earley's algorithm again and again.
      Around 2010, I realized
      that the original, long-abandoned vision --
      an efficient, practical, general and syntax-driven parser --
      was now, in fact, quite possible.
      The necessary pieces had fallen into place.
    &lt;/p&gt;
    &lt;p&gt;
      Aycock&amp;Horspool have solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and probably no longer relevant in any case --
      caches misses are now the bottleneck.
    &lt;/p&gt;
    &lt;p&gt;But while the original issues with Earley's disappeared,
      a new issue emerged.
      With a parsing algorithm as powerful as Earley's behind it,
      a syntax-driven approach can do much more than it can with
      a left parser.
      But with the experience with LALR in their collective consciousness,
      few modern programmers are prepared
      to trust a purely declarative parser.
      As Lincoln said, &quot;Once a cat's been burned,
      he won't even sit on a cold stove.&quot;
    &lt;/p&gt;
    &lt;p&gt;
      To be accepted, Marpa needed to allow
      procedure parsing,
      not just declarative parsing.
      So Marpa allows the user to specify events --
      occurrences of symbols and rules --
      at which declarative parsing pauses.
      While paused,
      the application can call procedural logic
      and single-step forward token by token.
      The procedural logic can hand control back
      over to syntax-driven parsing at any point it likes.
      The Earley tables can provide the procedural logic with
      full knowledge of the state of the
      parse so far:
      all rules recognized
      in all possible parses so far,
      and all symbols expected.
      Earley's algorithm is now a even better companion
      for hand-written procedural logic than recursive descent.
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      For more about Marpa, there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Introduction to Marpa Book in progress</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/03/book_intro.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;&lt;p&gt;
      What follows is a summary of the features
      of the Marpa algorithm,
      followed by a discussion of potential
      applications.
      It refers to itself as a &quot;monograph&quot;, because it
      is a draft of part of the introduction to
      a technical monograph on the Marpa algorithm.
      I hope the entire monograph will appear in a few
      weeks.
    &lt;/p&gt;
    &lt;h2&gt;The Marpa project&lt;/h2&gt;
    &lt;p&gt;
      The Marpa project was intended to create
      a practical and highly available tool
      to generate and use general context-free
      parsers.
      Tools of this kind
      had long existed
      for LALR and
      regular expressions.
      But, despite an encouraging academic literature,
      no such tool had existed for context-free parsing.
      The first stable version of Marpa was uploaded to
      a public archive on Solstice Day 2011.
      This monograph describes the algorithm used
      in the most recent version of Marpa,
      Marpa::R2.
      It is a simplification of the algorithm presented
      in
      &lt;a href=&quot;https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer&quot;&gt;my
      earlier paper&lt;/a&gt;.
      &lt;h2&gt;A proven algorithm&lt;/h2&gt;
      While the presentation in this monograph is theoretical,
      the approach is practical.
      The Marpa::R2 implementation has been widely available
      for some time,
      and has seen considerable use,
      including in production environments.
      Many of the ideas in the parsing literature
      satisfy theoretical criteria,
      but in practice turn out to face significant obstacles.
      An algorithm may be as fast as reported, but may turn
      out not to allow
      adequate error reporting.
      Or a modification may speed up the recognizer,
      but require additional processing at evaluation time,
      leaving no advantage to compensate for
      the additional complexity.
      &lt;/p&gt;
      &lt;p&gt;
      In this monograph, I describe the Marpa
      algorithm
      as it was implemented for Marpa::R2.
      In many cases,
      I believe there are better approaches than those I
      have described.
      But I treat these techniques,
      however solid their theory,
      as conjectures.
      Whenever I mention a technique
      that was not actually implemented in
      Marpa::R2,
      I will always explicitly state that
      that technique is not in Marpa as implemented.
      &lt;h2&gt;Features&lt;/h2&gt;
      &lt;h3&gt;General context-free parsing&lt;/h3&gt;
      As implemented,
      Marpa parses
      all &quot;proper&quot; context-free grammars.
      The
      proper context-free grammars are those which
      are free of cycles,
      unproductive symbols,
      and
      inaccessible symbols.
      Worst case time bounds are never worse than
      those of Earley's algorithm,
      and therefore never worse than O(n**3).
      &lt;h3&gt;Linear time for practical grammars&lt;/h3&gt;
      Currently, the grammars suitable for practical
      use are thought to be a subset
      of the deterministic context-free grammars.
      Using a technique discovered by
      Joop Leo,
      Marpa parses all of these in linear time.
      Leo's modification of Earley's algorithm is
      O(n) for LR-regular grammars.
      Leo's modification
      also parses many ambiguous grammars in linear
      time.
      &lt;h3&gt;Left-eidetic&lt;/h3&gt;
      The original Earley algorithm kept full information
      about the parse ---
      including partial and fully
      recognized rule instances ---
      in its tables.
      At every parse location,
      before any symbols
      are scanned,
      Marpa's parse engine makes available
      its
      information about the state of the parse so far.
      This information is
      in useful form,
      and can be accessed efficiently.
      &lt;h3&gt;Recoverable from read errors&lt;/h3&gt;
      When
      Marpa reads a token which it cannot accept,
      the error is fully recoverable.
      An application can try to read another
      token.
      The application can do this repeatedly
      as long as none of the tokens are accepted.
      Once the application provides
      a token that is accepted by the parser,
      parsing will continue
      as if the unsuccessful read attempts had never been made.
      &lt;h3&gt;Ambiguous tokens&lt;/h3&gt;
      Marpa allows ambiguous tokens.
      These are often useful in natural language processing
      where, for example,
      the same word might be a verb or a noun.
      Use of ambiguous tokens can be combined with
      recovery from rejected tokens so that,
      for example, an application could react to the
      rejection of a token by reading two others.
      &lt;h2&gt;Using the features&lt;/h2&gt;
      &lt;h3&gt;Error reporting&lt;/h3&gt;
      An obvious application of left-eideticism is error
      reporting.
      Marpa's abilities in this respect are
      ground-breaking.
      For example,
      users typically regard an ambiguity as an error
      in the grammar.
      Marpa, as currently implemented,
      can detect an ambiguity and report
      specifically where it occurred
      and what the alternatives were.
      &lt;h3&gt;Event driven parsing&lt;/h3&gt;
      As implemented,
      Marpa::R2
      allows the user to define &quot;events&quot;.
      Events can be defined that trigger when a specified rule is complete,
      when a specified rule is predicted,
      when a specified symbol is nulled,
      when a user-specified lexeme has been scanned,
      or when a user-specified lexeme is about to be scanned.
      A mid-rule event can be defined by adding a nulling symbol
      at the desired point in the rule,
      and defining an event which triggers when the symbol is nulled.
      &lt;h3&gt;Ruby slippers parsing&lt;/h3&gt;
      Left-eideticism, efficient error recovery,
      and the event mechanism can be combined to allow
      the application to change the input in response to
      feedback from the parser.
      In traditional parser practice,
      error detection is an act of desperation.
      In contrast,
      Marpa's error detection is so painless
      that it can be used as the foundation
      of new parsing techniques.
      &lt;/p&gt;
      &lt;p&gt;
      For example,
      if a token is rejected,
      the lexer is free to create a new token
      in the light of the parser's expectations.
      This approach can be seen
      as making the parser's
      &quot;wishes&quot; come true,
      and I have called it
      &quot;Ruby Slippers Parsing&quot;.
      &lt;/p&gt;
      &lt;p&gt;
      One use of the Ruby Slippers technique is to
      parse with a clean
      but oversimplified grammar,
      programming the lexical analyzer to make up for the grammar's
      short-comings on the fly.
      As part of Marpa::R2,
      the author has implemented an HTML parser,
      based on a grammar that assumes that all start
      and end tags are present.
      Such an HTML grammar is too simple even to describe perfectly
      standard-conformant HTML,
      but the lexical analyzer is
      programmed to supply start and end tags as requested by the parser.
      The result is a simple and cleanly designed parser
      that parses very liberal HTML
      and accepts all input files,
      in the worst case
      treating them as highly defective HTML.
      &lt;h3&gt;Ambiguity as a language design technique&lt;/h3&gt;
      In current practice, ambiguity is avoided in language design.
      This is very different from the practice in the languages humans choose
      when communicating with each other.
      Human languages exploit ambiguity in order to design highly flexible,
      powerfully expressive languages.
      For example,
      the language of this monograph, English, is notoriously
      ambiguous.
      &lt;/p&gt;
      &lt;p&gt;
      Ambiguity of course can present a problem.
      A sentence in an ambiguous
      language may have undesired meanings.
      But note that this is not a reason to ban potential ambiguity ---
      it is only a problem with actual ambiguity.
      &lt;/p&gt;
      &lt;p&gt;
      Syntax errors, for example, are undesired, but nobody tries
      to design languages to make syntax errors impossible.
      A language in which every input was well-formed and meaningful
      would be cumbersome and even dangerous:
      all typos in such a language would be meaningful,
      and parser would never warn the user about errors, because
      there would be no such thing.
      &lt;/p&gt;
      &lt;p&gt;
      With Marpa, ambiguity can be dealt with in the same way
      that syntax errors are dealt with in current practice.
      The language can be designed to be ambiguous,
      but any actual ambiguity can be detected
      and reported at parse time.
      This exploits Marpa's ability
      to report exactly where
      and what the ambiguity is.
      Marpa::R2's own parser description language, the SLIF,
      uses ambiguity in this way.
      &lt;h3&gt;Auto-generated languages&lt;/h3&gt;
      In 1973, 
      &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0022000073800509&quot;&gt;
      &amp;#x10c;ulik and Cohen&lt;/a&gt; pointed out that the ability
      to efficiently parse LR-regular languages
      opens the way to auto-generated languages.
      In particular,
      &amp;#x10c;ulik and Cohen note that a parser which
      can parse any LR-regular language will be
      able to parse a language generated using syntax macros.
      &lt;h3&gt;Second order languages&lt;/h3&gt;
      In the literature, the term &quot;second order language&quot;
      is usually used to describe languages with features
      which are useful for second-order programming.
      True second-order languages --- languages which
      are auto-generated
      from other languages ---
      have not been seen as practical,
      since there was no guarantee that the auto-generated
      language could be efficiently parsed.
      &lt;/p&gt;
      &lt;p&gt;
      With Marpa, this barrier is raised.
      As an example,
      Marpa::R2's own parser description language, the SLIF,
      allows &quot;precedenced rules&quot;.
      Precedenced rules are specified in an extended BNF.
      The BNF extensions allow precedence and associativity
      to be specified for each RHS.
      &lt;/p&gt;
      &lt;p&gt;
      Marpa::R2's precedenced rules are implemented as
      a true second order language.
      The SLIF representation of the precedenced rule
      is parsed to create a BNF grammar which is equivalent,
      and which has the desired precedence.
      Essentially,
      the SLIF does a standard textbook transformation.
      The transformation starts
      with a set of rules,
      each of which has a precedence and
      an associativity specified.
      The result of the transformation is a set of
      rules in pure BNF.
      The SLIF's advantage is that it is powered by Marpa,
      and therefore the SLIF can be certain that the grammar
      that it auto-generates will
      parse in linear time.
      &lt;/p&gt;
      &lt;p&gt;
      Notationally, Marpa's precedenced rules
      are an improvement over
      similar features
      in LALR-based parser generators like
      yacc or bison.
      In the SLIF,
      there are two important differences.
      First, in the SLIF's precedenced rules,
      precedence is generalized, so that it does
      not depend on the operators:
      there is no need to identify operators,
      much less class them as binary, unary, etc.
      This more powerful and flexible precedence notation
      allows the definition of multiple ternary operators,
      and multiple operators with arity above three.
      &lt;/p&gt;
      &lt;p&gt;
      Second, and more important, a SLIF user is guaranteed
      to get exactly the language that the precedenced rule specifies.
      The user of the yacc equivalent must hope their
      syntax falls within the limits of LALR.&lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      Marpa has a
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>What parser do birds use?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/03/parus.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;blockquote&gt;
      &quot;Here we provide, to our knowledge, the first unambiguous experimental evidence for compositional syntax in a non-human vocal system.&quot;
      --
      &lt;a href=&quot;http://www.nature.com/ncomms/2016/160308/ncomms10986/full/ncomms10986.html&quot;&gt;
        &quot;Experimental evidence for compositional syntax in bird calls&quot;,
        Toshitaka N. Suzuki,	David Wheatcroft	&amp;amp; Michael Griesser
        &lt;emph&gt;Nature Communications&lt;/emph&gt;
        7, Article number: 10986
      &lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      In this post I look at a subset of the language
      of the
      &lt;a href=&quot;https://en.wikipedia.org/wiki/Great_tit&quot;&gt;
        Japanese great tit&lt;/a&gt;,
      also known as Parus major.
      The above cited article presents evidence
      that bird brains can parse this language.
      What about standard modern computer parsing methods?
      Here is the subset -- probably a tiny one --
      of the language actually used by Parus major.
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
      S ::= ABC
      S ::= D
      S ::= ABC D
      S ::= D ABC
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
    &lt;/p&gt;
    &lt;h2&gt;Classifying the Parus major grammar&lt;/h2&gt;
    &lt;p&gt;Grammophone is a very handy
      &lt;a href=&quot;http://mdaines.github.io/grammophone/#&quot;&gt;
        new tool&lt;/a&gt;
      for classifying grammars.
      Its own parser is somewhat limited, so that it requires a period
      to mark the end of a rule.
      The above grammar is in Marpa's SLIF format, which is smart enough to
      use the &quot;&lt;tt&gt;::=&lt;/tt&gt;&quot;
      operator to spot the beginning and end of rules,
      just as the human eye does.
      Here's the same grammar converted into a form acceptable to Grammophone:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
      S -&gt; ABC .
      S -&gt; D .
      S -&gt; ABC D .
      S -&gt; D ABC .
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      Grammophone tells us that the Parus major grammar is not LL(1),
      but that it is LALR(1).
    &lt;/p&gt;
    &lt;h2&gt;What does this mean?&lt;/h2&gt;
    &lt;p&gt;LL(1) is the class of grammar parseable by top-down methods:
      it's the best class for characterizing most parsers in current use,
      including recursive descent, PEG,
      and Perl 6 grammars.
      All of these parsers fall short of dealing with the Parus major language.
    &lt;/p&gt;
    &lt;p&gt;
      LALR(1) is probably most well-known from its implementations in
      bison and yacc.
      While able to handle this subset of Parus's language,
      LALR(1) has its own, very strict, limits.
      Whether LALR(1) could handle the full
      complexity of Parus
      language is a serious question.
      But it's a question that in practice would probably not arise.
      LALR(1) has horrible error handling properties.
    &lt;/p&gt;
    &lt;p&gt;
      When the input is correct and within its limits,
      an LALR-driven parser is fast and works well.
      But if the input is not perfectly correct,
      LALR parsers produce
      no useful analysis of what went wrong.
      If Parus hears &quot;d abc d&quot;,
      a parser like Marpa, on the other hand, can produce something like
      this:
    &lt;/p&gt;
    &lt;blockquote&gt;&lt;pre&gt;
# * String before error: abc d\s
# * The error was at line 1, column 7, and at character 0x0064 'd', ...
# * here: d
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      Parus uses its language in predatory contexts,
      and one can assume that a Parus with a preference for parsers whose
      error handling is on an LALR(1) level
      will not be keeping its alleles in the gene pool for very
      long.
    &lt;/p&gt;&lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      Those readers content with sub-Parus parsing methods may stop reading here.
      Those with greater parsing ambitions, however, may
      wish to learn more about Marpa.
      A Marpa test script for parsing the Parus subset is in
      &lt;a href=https://gist.github.com/jeffreykegler/b5b8ba349b8c6e5c2e54&gt;a Github gist.&lt;/a&gt;
      Marpa has a
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>What are the reasonable computer languages?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/01/lrr.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;blockquote&gt;
      &quot;You see things; and you say 'Why?' But I dream things that never were; and I say 'Why not?'&quot;
      --
      &lt;a href=&quot;http://www.bartleby.com/73/465.html&quot;&gt;George Bernard Shaw&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      In the 1960's and 1970's computer languages were evolving rapidly.
      It was not clear which way they were headed.
      Would most programming be done with
      general-purpose languages?
      Or would programmers create a language for every task domain?
      Or even for every project?
      And, if lots of languages were going to be created,
      what kinds of languages would be needed?
    &lt;/p&gt;
    &lt;p&gt;
      It was in that context that &amp;#268;ulik and Cohen,
      in
      &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0022000073800509&quot;&gt;a
        1973 paper&lt;/a&gt;,
      outlined what they thought programmers would want and should have.
      In keeping with the spirit of the time,
      it was quite a lot:
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;Programmers would want to extend their grammars with new syntax,
        including new
        kinds of expressions.&lt;/li&gt;
      &lt;li&gt;Programmers would also want to use tools that automatically generated new syntax.&lt;/li&gt;
      &lt;li&gt;Programmers would not want to, and especially
        in the case of auto-generated syntax
        would usually not be able to,
        massage the syntax into very restricted forms.
        Instead, programmers would create grammars and languages
        which required unlimited lookahead to disambiguate,
        and they would require parsers which could handle these grammars.&lt;/li&gt;
      &lt;li&gt;Finally, programmers would need to be able to rely on
        all of this parsing being done in linear time.&lt;/li&gt;
    &lt;/ul&gt;&lt;p&gt;
      Today, we think we know that
      &amp;#268;ulik and Cohen's vision was naive,
      because we think we know that parsing technology cannot support it.
      We think we know that parsing is much harder than they thought.
    &lt;/p&gt;
    &lt;h2&gt;The eyeball grammars&lt;/h2&gt;
    &lt;p&gt;As a thought problem, consider the &quot;eyeball&quot; class of grammars.
      The &quot;eyeball&quot; class of grammars contains all the grammars that a human
      can parse at a glance.
      If a grammar is in the eyeball class,
      but a computer cannot parse it,
      it presents an interesting choice.  Either,
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;your computer is not using the strongest practical algorithm; or
      &lt;/li&gt;
      &lt;li&gt;your mind is using some power which cannot be reduced to a machine computation.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      There are some people out there (I am one of them)
      who don't believe that everything the mind can do reduces
      to a machine computation.
      But even those people
      will tend to go for the choice in this case:
      There must be some practical computer parsing algorithm which
      can do at least as well at parsing as a human
      can do by &quot;eyeball&quot;.
      In other words, the class of &quot;reasonable grammars&quot; should
      contain the eyeball class.
    &lt;/p&gt;
    &lt;p&gt;
      &amp;#268;ulik and Cohen's candidate for the class of &quot;reasonable grammars&quot;
      were the grammars that
      a deterministic parse engine
      could parse if it had a lookahead that was infinite,
      but restricted to distinguishing between regular expressions.
      They called these the LR-regular, or LRR, grammars.
      And the LRR grammars
      &lt;b&gt;do&lt;/b&gt;
      in fact seem to be a good first approximation
      to the eyeball class.
      They do not allow lookahead that contains things
      that you have to count, like palindromes.
      And, while I'd be hard put to eyeball every possible string for every possible regular expression,
      intuitively the concept of scanning for a regular expression
      does seem close to capturing the idea of glancing through a text looking for a telltale pattern.
    &lt;/p&gt;
    &lt;h2&gt;So what happened?&lt;/h2&gt;
    &lt;p&gt;
      Alas, the algorithm in the &amp;#268;ulik and Cohen paper turned out to be impractical.
      But in 1991, Joop Leo discovered a way to adopt Earley's algorithm to parse the LRR grammars
      in linear time, without doing the lookahead.
      And Leo's algorithm does have a practical implementation:
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;Marpa&lt;/a&gt;.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Top-down parsing is guessing</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/12/topdown.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;
      Top-down parsing is guessing.  Literally.
      Bottom-up parsing is looking.
    &lt;/p&gt;
    &lt;p&gt;The way you'll often hear that phrased is that top-down parsing is
      looking, starting at the top,
      and bottom-up parsing is looking, starting at the bottom.
      But that is misleading, because the input is at the bottom --
      at the top there is nothing to look at.
      A usable top-down parser
      &lt;b&gt;must&lt;/b&gt;
      have a bottom-up component,
      even if that component is just lookahead.
    &lt;/p&gt;
    &lt;p&gt;A more generous, but still accurate, way to describe the top-down
      component of parsers is &quot;prediction&quot;.
      And prediction is, indeed, a very useful component of a parser,
      when used in combination with other techniques.
    &lt;/p&gt;
    &lt;p&gt;Of course, if a parser does nothing but predict,
      it can predict only one input.
      Top-down parsing must always be combined with a bottom-up
      component.
      This bottom-up component may be as modest as lookahead, but it
      &lt;b&gt;must&lt;/b&gt;
      be there or else top-down parsing is really not parsing at all.
    &lt;/p&gt;
    &lt;h2&gt;So why is top-down parsing used so much?&lt;/h2&gt;
    &lt;p&gt;Top-down parsing may be unusable in its pure form,
      but from one point of view that is irrelevant.
      Top-down parsing's biggest advantage is that it is highly flexible --
      there's no reason to stick to its &quot;pure&quot; form.
    &lt;/p&gt;
    &lt;p&gt;A top-down parser can be written as a series of subroutine calls --
      a technique called recursive descent.
      Recursive descent
      allows you to hook in custom-written bottom-up logic at every
      top-down choice point,
      and it is a technique which is
      completely understandable to programmers with little or no training
      in parsing theory.
      When dealing with recursive descent parsers,
      it is more useful to be a seasoned, far-thinking programmer
      than it is to be a mathematician.
      This makes recursive descent very appealing to
      seasoned, far-thinking programmers,
      and they are the audience that counts.
    &lt;/p&gt;
    &lt;h2&gt;Switching techniques&lt;/h2&gt;
    &lt;p&gt;You can even use the flexibility of top-down to switch
      away from top-down parsing.
      For example, you could claim that a top-down parser could do anything my
      own parser
      (&lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;Marpa&lt;/a&gt;)
      could do, because a recursive descent parser can call
      a Marpa parser.
    &lt;/p&gt;
    &lt;p&gt;
      A less dramatic switchoff, and one that still leaves the parser with a good claim to be
      basically top-down, is very common.
      Arithmetic expressions are essential for a computer language.
      But they are also
      among the many things
      top-down parsing cannot handle, even with ordinary lookahead.
      Even so, most computer languages these days are parsed top-down --
      by recursive descent.
      These recursive descent parsers deal with expressions
      by temporarily handing control over to an bottom-up operator
      precedence parser.
      Neither of these parsers is
      extremely smart about the hand-over and hand-back
      -- it is up to the programmer to make sure the two play together nicely.
      But used with caution, this approach works.
    &lt;/p&gt;
    &lt;h2&gt;Top-down parsing and language-oriented programming&lt;/h2&gt;
    &lt;p&gt;But what about taking top-down methods into the future of language-oriented programming,
      extensible languages, and grammars which write grammars?
      Here we are forced to confront the reality -- that the effectiveness of
      top-down parsing comes entirely from the foreign elements that are added to it.
      Starting from a basis of top-down parsing is literally starting
      with nothing.
      As I have shown in more detail
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/12/composable.html&quot;&gt;elsewhere&lt;/a&gt;,
      top-down techniques simply do not have enough horsepower to deal with grammar-driven programming.
    &lt;/p&gt;
    &lt;p&gt;Perl 6 grammars are top-down -- PEG with lots of extensions.
      These extensions
      include backtracking, backtracking control,
      a new style of tie-breaking and lots of opportunity
      for the programmer to intervene and customize everything.
      But behind it all is a top-down parse engine.
    &lt;/p&gt;
    &lt;p&gt;One aspect of Perl 6 grammars might be seen as breaking
      out of the top-down trap.
      That trick of switching over to a
      bottom-up operator precedence parser for expressions,
      which I mentioned above,
      is built into Perl 6 and semi-automated.
      (I say semi-automated because making sure the two parsers &quot;play nice&quot;
      with each other is not automated -- that's still up to the programmer.)
    &lt;/p&gt;
    &lt;p&gt;As far as I know, this semi-automation of expression handling
      is new with Perl 6 grammars, and it
      may prove handy for duplicating what is done
      in recursive descent parsers.
      But it adds no new technique to those already in use.
      And features
      like
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;mulitple types of expression, which can be told apart
        based on their context,&lt;/li&gt;
      &lt;li&gt;&lt;i&gt;n&lt;/i&gt;-ary expressions for arbitrary
        &lt;i&gt;n&lt;/i&gt;, and&lt;/li&gt;
      &lt;li&gt;the autogeneration of multiple rules, each allowing
        a different precedence scheme,
        for expressions of arbitrary arity and associativity,
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      all of which are available and in current use in
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;Marpa&lt;/a&gt;,
      are impossible for the technology behind Perl 6 grammars.
    &lt;/p&gt;
    &lt;p&gt;I am a fan of the Perl 6 effort.
      Obviously, I have doubts about one specific set of hopes for Perl 6 grammars.
      But these hopes have not been central to the Perl 6 effort,
      and I will be an eager student of the Perl 6 team's work over the coming months.
    &lt;/p&gt;
    &lt;h2&gt;Comments&lt;/h2&gt;
    &lt;p&gt;
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  </channel>
</rss>
