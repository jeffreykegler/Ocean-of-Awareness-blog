<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>Mixing procedural and declarative parsing gracefully</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/06/mixing-procedural.html</link>
    <description>&lt;h3&gt;Declarative and procedural parsing&lt;/h3&gt;
&lt;p&gt;&lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
--&gt;A declarative parser
takes a description of your language and parses it
for you.
On the face of it, this sounds like the way you'd want
to go,
and Marpa offers that possibily -- it generates
a parser from anything you can write in BNF and,
if the parser in is one of the classes currently in
practical use,
that parser will run in linear time.
&lt;/p&gt;
&lt;p&gt;
But practical grammars often have context-sensitive parts --
features which cannot be described in BNF.
Nice as declarative parsing may sound,
the need to do at least &lt;b&gt;some&lt;/b&gt; procedural parsing
can seem to an obstacle
in real-life.
&lt;/p&gt;
In this post, I take a problem for which procedural
parsing is essential,
and create a fast, short solution that
mixes procedural and declarative.
The &quot;weld point&quot; between
the procedural and the declaration is
not tricky or awkward,
and the solution comes out looking like
the problem.
&lt;/p&gt;
&lt;h3&gt;The application&lt;/h3&gt;
&lt;p&gt;This is a sample of the language:
&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;
A2(A2(S3(Hey)S13(Hello, World!))S5(Ciao!))
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;
It describes strings in nested arrays.
The strings are introducted by the letter 'S', followed by a length count and then,
in parentheses, the string itself.
Arrays are introducted by the letter 'A' followed by an element count and, inside parentheses, the
arrays contents.
The array contents are a concatenated series of strings and other arrays.
&lt;/p&gt;
&lt;p&gt;I call this a Dyck-Hollerith language because it
combines
&lt;a href=&quot;http://en.wikipedia.org/wiki/Hollerith_constant&quot;&gt;
Hollerith constants&lt;/a&gt;
(strings preceded by a count),
with balanced parentheses
(what is called
&lt;a href=&quot;http://en.wikipedia.org/wiki/Dyck_language&quot;&gt;
a Dyck language&lt;/a&gt;
by mathematicians).
&lt;/p&gt;
&lt;p&gt;
The language is one I've dealt with before.
It is apparently from &quot;real life&quot;, and is described more fully
in 
In
&lt;a href=&quot;http://blogs.perl.org/users/polettix/2012/04/parserecdescent-and-number-of-elements-read-on-the-fly.html&quot;&gt;
a blog post by
Flavio Poletti&lt;/a&gt;.
Several people, Gabor Szabo among them, challenged me to show how
&lt;a href=&quot;http://jeffreykegler.github.com/Marpa-web-site/&quot;&gt;
Marpa&lt;/a&gt;
would do on this language.
I did this a year ago, using Marpa's previous version, Marpa::XS.
In this post, I redo it, using Marpa's new SLIF interface.
The result, already quite satisfactory, is a considerable improvement
in readability, flexibility and speed.
&lt;/p&gt;
&lt;h3&gt;The code&lt;/h3&gt;
&lt;p&gt;The full code for this example is in
&lt;a href=&quot;https://gist.github.com/jeffreykegler/5745272&quot;&gt;
a Github gist&lt;/a&gt;.
In what follows, I will assume the reader is skimming,
looking for the general idea,
and I will focus on major features.
Actual tutorials can be found
&lt;a href=&quot;https://metacpan.org/module/Marpa::R2&quot;.
in Marpa's documentation&lt;/a&gt;,
on
&lt;a href=http://jeffreykegler.github.io/Ocean-of-Awareness-blog/&quot;&gt;
the Ocean of Awareness blog&lt;/a&gt;,
and on
&lt;a href=&quot;http://marpa-guide.github.io/index.html&quot;
the Marpa Guide,
a new website&lt;/a&gt;
being
built due to the generosity of Peter Stuifzand
and Ron Savage.
&lt;/p&gt;
&lt;h3&gt;The DSL&lt;/h3&gt;
&lt;p&gt;First off, let's look at the declarative part.
The core of the parser is the following lines,
containing the BNF for the language's top-level structure.
&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;
my $dsl = &amp;lt;&amp;lt;'END_OF_DSL';
# The BNF
:start ::= sentence
sentence ::= element
array ::= 'A' &amp;lt;array count&amp;gt; '(' elements ')'
    action =&amp;gt; check_array
string ::= ( 'S' &amp;lt;string length&amp;gt; '(' ) text ( ')' )
elements ::= element+
  action =&amp;gt; ::array
element ::= string | array
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;Details of this syntax are in Marpa's documentation,
but it's a dialect of EBNF.
Adverbs like &lt;tt&gt;action =&amp;gt; semantics&lt;/tt&gt; tell Marpa what the semantics will be.
The default (which will be set below) is for a rule to retun its first child.
The parentheses in the &lt;tt&gt;string&lt;/tt&gt; declaration &quot;hide&quot; the symbols from the
semantics, so that &lt;tt&gt;text&lt;/tt&gt; is treated by the semantics as if it
were the &quot;first&quot; symbol.
&lt;tt&gt;::array&lt;/tt&gt; semantics tell Marpa to return all every &lt;tt&gt;element&lt;/tt&gt;
of &lt;tt&gt;elements&lt;/tt&gt; in an array.
And &lt;tt&gt;check_array&lt;/tt&gt; is the name of a function providing
the semantics, one which will be described below.
&lt;/p&gt;
&lt;p&gt;Marpa's SLIF provides a lexer for the user, and this one will handle most
of the symbols.  The single-quoted strings we saw in the BNF are actually instructions
to this lexer, and here are some on how to handle
&lt;tt&gt;&amp;lt;array_count&amp;gt;&lt;/tt&gt; and
&lt;tt&gt;&amp;lt;string length&amp;gt;&lt;/tt&gt;.
&lt;blockquote&gt;
&lt;pre&gt;
&amp;lt;array count&amp;gt; ~ [\d]+
&amp;lt;string length&amp;gt; ~ [\d]+
text ~ [\d\D]
END_OF_DSL
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;
&lt;tt&gt;&amp;lt;array_count&amp;gt;&lt;/tt&gt; are
&lt;tt&gt;&amp;lt;string length&amp;gt;&lt;/tt&gt; are both declared to be a series of digits.
&lt;tt&gt;text&lt;/tt&gt; is a stub.
The length of &lt;tt&gt;text&lt;/tt&gt; depends on the numeric value of
&lt;tt&gt;&amp;lt;string length&amp;gt;&lt;/tt&gt;, and that's beyond the power of BNF to deal with,
so when the time comes,
When it comes time to deal with &lt;tt&gt;text&lt;/tt&gt;,
we will hand control over to an external lexer.
For the purposes of Marpa's lexer, it's enough to tell it
that text is a single character of anything.
&lt;/p&gt;
&lt;p&gt;Now comes the weld between declarative and procedural ...
&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;
:lexeme ~ &amp;lt;string length&amp;gt; pause =&amp;gt; after
:lexeme ~ text pause =&amp;gt; before
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;These two statements tell Marpa that 
&lt;tt&gt;&amp;lt;string length&amp;gt;&lt;/tt&gt;
and
&lt;tt&gt;&amp;lt;text&amp;gt;&lt;/tt&gt;
are two lexicals at which Marpa's own parsing should &quot;pause&quot;,
handing over control to external procedural logic.
In the case of &lt;tt&gt;&amp;lt;string length&amp;gt;&lt;/tt&gt;,
the pause should be after it is read.
In the case of &lt;tt&gt;&amp;lt;text&amp;gt;&lt;/tt&gt;
the pause should be before.
What happens during the &quot;pause&quot;, we will soon see.
&lt;/p&gt;
&lt;h3&gt;Starting the parse&lt;/h3&gt;
&lt;p&gt;Next follows the code to read the DSL,
and start the parser.
&lt;blockquote&gt;
&lt;pre&gt;
my $grammar = Marpa::R2::Scanless::G-&amp;gt;new(
    {   action_object  =&amp;gt; 'My_Actions',
        default_action =&amp;gt; '::first',
        source         =&amp;gt; \$dsl
    }
);

my $recce = Marpa::R2::Scanless::R-&amp;gt;new( { grammar =&amp;gt; $grammar } );
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;The previous lines tell Marpa that, when its semantics
is provided by a Perl semantics, to look for its semantics in a package called
&lt;tt&gt;My_Actions&lt;/tt&gt;.
The default semantics are &lt;tt&gt;::first&lt;/tt&gt;, which means simply pass the value of
the first RHS symbol of a rule upwards.
&lt;/p&gt;
&lt;blockquote&gt;
&lt;pre&gt;
    $input = 'A2(A2(S3(Hey)S13(Hello, World!))S5(Ciao!))';
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;pre&gt;

my $last_string_length;
my $input_length = length $input;
INPUT:
for (
    my $pos = $recce-&amp;gt;read( \$input );
    $pos &amp;lt; $input_length;
    $pos = $recce-&amp;gt;resume($pos)
    )
{
    my $lexeme = $recce-&amp;gt;pause_lexeme();
    die q{Parse exhausted in front of this string: &quot;},
        substr( $input, $pos ), q{&quot;}
        if not defined $lexeme;
    my ( $start, $lexeme_length ) = $recce-&amp;gt;pause_span();
    if ( $lexeme eq 'string length' ) {
        $last_string_length = $recce-&amp;gt;literal( $start, $lexeme_length ) + 0;
        $pos = $start + $lexeme_length;
        next INPUT;
    }
    if ( $lexeme eq 'text' ) {
        my $text_length = $last_string_length;
        $recce-&amp;gt;lexeme_read( 'text', $start, $text_length );
        $pos = $start + $text_length;
        next INPUT;
    } ## end if ( $lexeme eq 'text' )
    die &quot;Unexpected lexeme: $lexeme&quot;;
} ## end INPUT: for ( my $pos = $recce-&amp;gt;read( \$input ); $pos &amp;lt; $input_length...)
&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;pre&gt;

package My_Actions;

&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;pre&gt;

sub check_array {
    my ( undef, undef, $declared_size, undef, $array ) = @_;
    my $actual_size = @{$array};
    warn
        &quot;Array size ($actual_size) does not match that specified ($declared_size)&quot;
        if $declared_size != $actual_size;
    return $array;
} ## end sub check_array

&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;pre&gt;

my $result = $recce-&amp;gt;value();
die 'No parse' if not defined $result;
my $received = Data::Dumper::Dumper( ${$result} );

&lt;/pre&gt;
&lt;/blockquote&gt;
    &lt;h3&gt;For more about Marpa&lt;/h3&gt;
    &lt;p&gt;The techniques described apply to problems considerably
    larger this than the example of this post.
    Jean-Damien Durand is using them to create
    &lt;a href=&quot;https://github.com/jddurand/MarpaX-Languages-C-AST&quot;&gt;
    a C-to-AST tool&lt;/a&gt;.
    This
    takes C language and converts it to an AST,
    following the C11
    specification carefully.
    The AST can then be manipulated
    as you wish.
    &lt;/p
    &lt;p&gt;
      Marpa's latest version is
      &lt;a href=&quot;https://metacpan.org/module/Marpa::R2&quot;&gt;Marpa::R2,
        which is available on CPAN&lt;/a&gt;.
      A list of my Marpa tutorials can be found
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL&quot;&gt;
        here&lt;/a&gt;.
      There is
      &lt;a href=&quot;http://marpa-guide.github.io/chapter1.html&quot;&gt;
        a new tutorial by Peter Stuifzand&lt;/a&gt;.
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/&quot;&gt;
        This blog&lt;/a&gt;
      focuses on Marpa,
      and it has
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html&quot;&gt;an annotated guide&lt;/a&gt;.
      Marpa also has
      &lt;a href=&quot;http://jeffreykegler.github.com/Marpa-web-site/&quot;&gt;a web page&lt;/a&gt;.
      For questions, support and discussion, there is a
      Google Group:
      &lt;code&gt;marpa-parser@googlegroups.com&lt;/code&gt;.
      Comments on this post can be made there.
    &lt;/p&gt;</description>
  </item>
  <item>
    <title>The Design of Four</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/05/design_of_4.html</link>
    <description>  &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      In the Perl world at this moment,
      a lot is being said about the consequences of bad design.
      And it is useful to study design failures.
      But the exercise will come to nothing
      without a road to good design.
      This post will point out four
      Perl-centric
      projects that
      are worth study as models
      of good design.
    &lt;/p&gt;
    &lt;p&gt;
      The projects are
      &lt;a href=&quot;https://metacpan.org/release/ack&quot;&gt;
      ack&lt;/a&gt;,
      &lt;a href=&quot;https://metacpan.org/release/App-cpanminus&quot;&gt;
      cpanm&lt;/a&gt;,
      &lt;a href=&quot;https://metacpan.org/release/local-lib&quot;&gt;local::lib&lt;/a&gt; and
      &lt;a href=&quot;https://metacpan.org/release/App-perlbrew&quot;&gt;perlbrew&lt;/a&gt;.
      Each of these is perfect
      in the older sense of &quot;having all that is requisite to its nature and kind&quot;
      (&lt;a href=&quot;http://1828.mshaffer.com/d/word/perfect&quot;&gt;Webster's 1828&lt;/a&gt;).
	If you are into Perl,
	they are all widely useful,
	and looking at them as a potential or an actual user
	is the best way to gain an appreciation of the art behind them.
    &lt;/p&gt;
    &lt;p&gt;&lt;tt&gt;ack&lt;/tt&gt; is a file search tool -- UNIX's grep with improvements.
      The improvements are influenced by a Perl sensibility,
      and &lt;tt&gt;ack&lt;/tt&gt; is written in Perl.
      But while the other tools I list are of little interest unless
      you are into Perl,
      &lt;tt&gt;ack&lt;/tt&gt; can help you out even if you otherwise shun Perl tools.
    &lt;/p&gt;
    &lt;p&gt;
      &lt;tt&gt;cpanm&lt;/tt&gt; is for installing CPAN packages from CPAN.
      If you don't know what that means, you aren't interested.
      If you do, you want to be using it.
      It does everything you need and importantly, nothing more.
      The interface is without clutter.
      Like I said, perfect.
    &lt;/p&gt;
    &lt;p&gt;
      &lt;tt&gt;local::lib&lt;/tt&gt; is for installing Perl modules in the directory of your choice.
      Even if you have root permission on a system,
      it is good practice to leave the delivered Perl on your system
      untouched except by vendor-sponsored patches and updates.
      &lt;tt&gt;local::lib&lt;/tt&gt; allows you to do this easily and conveniently.
      It has every feature and convenience I want.
      And reading its documentation is again an encounter with
      perfection.
      Every feature described is
      &lt;ul&gt;
      &lt;li&gt;something that you need today,
      &lt;li&gt;something that you are worried you might need tomorrow, or
      &lt;li&gt;something that you are not worried you might need,
      but on reading the documentation will discover that you should be.
      &lt;/ul&gt;
      Aside from that, there is nothing else.
      Clutter-free.  Perfect.
    &lt;/p&gt;
    &lt;p&gt;
      Repeated perfection can be boring,
      a fact which
      I suspect plays no small role
      in making perfection an unusual thing in this world.
      So of &lt;tt&gt;perlbrew&lt;/tt&gt;,
      I will simply say that it does for Perl versions and executables
      what &lt;tt&gt;local::lib&lt;/tt&gt; does for Perl modules.
      &lt;tt&gt;perlbrew&lt;/tt&gt; is the way to manage alternative Perl executables.
      And using &lt;tt&gt;perlbrew&lt;/tt&gt;
      is a good way to study yet another perfect interface.
    &lt;/p&gt;
    &lt;p&gt;
    How much relevance does the work of
      Andy Lester (&lt;tt&gt;ack&lt;/tt&gt;),
      Tatsuhiko Miyagawa (&lt;tt&gt;cpanm&lt;/tt&gt;),
      Matt S Trout (&lt;tt&gt;local::lib&lt;/tt&gt;)
      and Kang-min Liu (&lt;tt&gt;perlbrew&lt;/tt&gt;) have to other projects,
      including projects that now seem larger and more complex?
      Certainly
      these four applications all seem simple, well-defined,
      and self-contained.
      But I would argue that,
      if these problems seem simple and well-defined today,
      much of that impression is the result of the design skills
      of Andy, Tatsuhiko, Matt and Kang-min.
      And if, to an extent, they did benefit from
      having the good fortune to pick the the right problem at
      the right time,
      it is useful to recall
      Ben Hogan's comment on his profession:
    &lt;/p&gt;
    &lt;blockquote&gt;Golf is a game of luck.
      The more I practice, the luckier I get.
    &lt;/blockquote&gt;</description>
  </item>
  <item>
    <title>Why Marpa works: table parsing</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/05/table.html</link>
    <description>  &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      Marpa works very differently from the parsers
      in wide use today.
      Marpa is a table parser.
      And Marpa is unusual among table parsers --
      its focus is on speed.
    &lt;/p&gt;
    &lt;p&gt;
      The currently favored parsers use stacks,
      in some cases together with a state machine.
      These have the advantage that it is easy
      to see how they can be made
      to run fast.
      They have the disadvantage of severely limiting
      what the parser can do and how much it can know.
    &lt;/p&gt;
    &lt;h3&gt;What is table parsing?&lt;/h3&gt;
    &lt;p&gt;
      Table parsing means parsing by constructing a table of all the possible parses.
      This is pretty clearly something you want -- anything less means
      not completely knowing what you're doing.
      It's like walking across the yard blindfolded.
      It's fine if you can make sure there are
      no walls, open pits, or other dangerous obstacles.
      But for the general case,
      it's a bad idea.
    &lt;/p&gt;
    &lt;p&gt;
      Where the analogy between walking blindfolded and parsing breaks
      down is that while taking off the blindfold has no cost,
      building a table of everything that is happening while you parse
      &lt;b&gt;does&lt;/b&gt;
      have a cost.
      If you limit your parser to a stack and a state machine,
      you may be parsing with a blindfold on,
      but it is clear that your parser can be fast.
      How to make a table parser run fast
      is not so clear.
    &lt;/p&gt;
    &lt;h3&gt;The advantages of table parsing&lt;/h3&gt;
    &lt;p&gt;
      What are the advantages of taking off the blindfold?
      First, your parser can be completely general --
      anything you can write in BNF it can parse.
      And second,
      you always know exactly what is going on -- what rules
      are possible, what rules have been recognized,
      how far into a rule you have recognized it,
      what symbols you expect next, etc. etc.
    &lt;/p&gt;&lt;p&gt;
    &lt;/p&gt;&lt;p&gt;
      We programmers have gotten used to parsers which run blindfolded.
      When you bump into something while
      blindfolded you don't know
      what it was.
      When non-table parsers fail, they usually don't know why --
      they can only guess.
      If you have a full parse table,
      built from left to right,
      you know what you were looking for and what you already think you
      found.
      This means that you can pinpoint and identify errors precisely.
    &lt;/p&gt;
    &lt;p&gt;
      Knowing where you are in a parse also allows certain tricks,
      like the one I call the Ruby Slippers.
      In this, you parse with an over-simplified grammar and,
      when it fails, you ask the parser what it was looking for.
      Then -- poof! -- you provide it.
    &lt;/p&gt;
    &lt;p&gt;
      The Ruby Slippers work beautifully when dealing with
      missing tags in HTML.
      You can define a simplified HTML grammar,
      one that lives in a non-existent world --
      an ideal world where all start
      and end tags are always where they belong.
      Then you parse away.
      If, as will happen with real-world input, a tag is missing,
      you ask the table parser what it was looking for,
      and give it a virtual tag.
    &lt;/p&gt;
    &lt;h3&gt;And as for fast ...&lt;/h3&gt;
    &lt;p&gt;When I decided to write Marpa in 2007 my goal was to create a table parser
      that was as fast as possible.
      I was surprised to find that the academic literature contained a
      major improvement to table parsing by Joop Leo,
      an improvement which nobody had ever made a serious attempt to implement.
      Marpa is the first implementation of Joop Leo's 1991 improvement to table parsing which,
      as far as theory goes,
      makes Marpa as fast any parser
      in practical use today.
      Any class of grammar that
      recursive descent, bison, etc. will parse,
      Marpa will parse in linear time.
    &lt;/p&gt;
    &lt;p&gt;
      Table parsing has had a reputation for being slow due to a
      bad &quot;constant factor&quot;.
      Theoreticians, when looking at speed as time complexity,
      throw away constant factors.
      What's left once the constant factor is ignored is always more
      important.
      Surprisingly often,
      time complexity results which ignore
      constant factors are also the most
      meaningful results in practical terms.
    &lt;/p&gt;&lt;p&gt;
      But not 100% of the time.
      Sometimes the constant factor makes all the difference.
      And deciding when a constant factor does make a difference,
      and when it does not,
      is a tricky matter,
      one that lies in that murky zone between practice and theory
      where neither practitioner or theorist feels fully at home.
    &lt;/p&gt;&lt;p&gt;
    &lt;/p&gt;
    &lt;p&gt;
      It's time to look again, for two reasons.
      First, Aycock and Horspool did a lot of careful work on
      reducing
      the constant factor for table parsing,
      work which Marpa incorporates
      and builds on.
      Second,
      the judgment about the constant factor dates from 1968,
      when computers were literally a billion
      times slower then they are now.
    &lt;/p&gt;&lt;p&gt;
    &lt;/p&gt;&lt;p&gt;
      Why has nobody re-examined this judgment as the years and the order
      of magnitude speed-ups marched by?
      When a judgment crosses sub-disciplines, it can be &quot;sticky&quot;
      beyond all reason,
      and this one is a good example.
      The decision that its &quot;constant factor&quot; made table parsing
      too slow for many practical applications
      is in part a practical take on a theoretical issue,
      and in part a theoretical call on a practical matter.
    &lt;/p&gt;&lt;p&gt;
    &lt;/p&gt;&lt;p&gt;
      Since 1968,
      the theoretical results have improved.
      But the theoreticians did not change
      their mind about the speed of table parsing
      because it was a judgment about the practical application
      of the theory.
      The practitioners were actually out there writing compilers.
      When it comes down to practice,
      you have to assume that practitioners know what they
      are talking about, right?
    &lt;/p&gt;&lt;p&gt;
    &lt;/p&gt;&lt;p&gt;
      Meanwhile the practice of writing software underwent
      revolution after revolution.
      But the practitioners continued to write off table parsing
      as impractical.
      Talking about the speed of table parsers quickly got you
      into some very heavy math.
      And some of the algorithms
      did not even exist except as mathematical
      notation on the pages
      of the journals and textbooks.
      When it comes down to theory about things
      that don't exist outside of theory,
      who do you listen to if not
      the theoreticians?
    &lt;/p&gt;
    &lt;p&gt;
      I look carefully at the issue
      of the &quot;constant factor&quot; in
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/04/fast_enough.html&quot;&gt;
        a previous post&lt;/a&gt;.
      Forty-five years have passed and
      cost of CPU has fallen
      nine orders of magnitude.
      (Others say the cost of CPU has dropped by 50% a year,
      in which case it's over 14 orders of magnitude.
      But why quibble?)
      It's reasonable to suspect that
      the constant factor that practitioners
      and theoreticians were worried about in 1968
      stopped being a
      major obstacle many years ago.
    &lt;/p&gt;
    &lt;h3&gt;For more about Marpa&lt;/h3&gt;
    &lt;p&gt;
      Marpa's latest version is
      &lt;a href=&quot;https://metacpan.org/module/Marpa::R2&quot;&gt;Marpa::R2,
        which is available on CPAN&lt;/a&gt;.
      A list of my Marpa tutorials can be found
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL&quot;&gt;
        here&lt;/a&gt;.
      There is
      &lt;a href=&quot;http://marpa-guide.github.io/chapter1.html&quot;&gt;
        a new tutorial by Peter Stuifzand&lt;/a&gt;.
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/&quot;&gt;
        This blog&lt;/a&gt;
      focuses on Marpa,
      and it has
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html&quot;&gt;an annotated guide&lt;/a&gt;.
      Marpa also has
      &lt;a href=&quot;http://jeffreykegler.github.com/Marpa-web-site/&quot;&gt;a web page&lt;/a&gt;.
      For questions, support and discussion, there is a
      Google Group:
      &lt;code&gt;marpa-parser@googlegroups.com&lt;/code&gt;.
      Comments on this post can be made there.
    &lt;/p&gt;</description>
  </item>
  <item>
    <title>Is Earley parsing fast enough?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/04/fast_enough.html</link>
    <description>  &lt;blockquote&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      &quot;First we ask, what impact will our algorithm have on the parsing
      done in production compilers for existing programming languages?
      The answer is, practically none.&quot; -- Jay Earley's Ph.D thesis, p. 122.
    &lt;/blockquote&gt;
    &lt;p&gt;In the above quote, the inventor of the Earley parsing
      algorithm poses a question.
      Is his algorithm fast enough for a production compiler?  His answer is a
      stark &quot;no&quot;.
    &lt;/p&gt;
    &lt;p&gt;
      This is the verdict on Earley's that you often
      hear repeated today, 45 years later.
      Earley's, it is said, has a too high a &quot;constant factor&quot;.
      Verdicts tends to be repeated more often than examined.
      This particular verdict originates with the inventor himself.
      So perhaps it is not astonishing
      that many treat the dismissal
      of Earley's on grounds of speed to be as valid today as it
      was in 1968.
    &lt;/p&gt;
    &lt;p&gt;But in the past 45 years,
      computer technology has changed beyond recognition
      and researchers
      have made several significant improvements to Earley's.
      It is time to reopen this case.
    &lt;/p&gt;&lt;h3&gt;What is a &quot;constant factor&quot;&lt;/h3&gt;
    &lt;p&gt;The term &quot;constant factor&quot; here has a special meaning,
      one worth looking at carefully.
      Programmers talk about time efficiency in two ways:
      time complexity and speed.
    &lt;/p&gt;
    &lt;p&gt;
      Speed is simple:
      It's how fast the algorithm is against the clock.
      To make comparison easy,
      the clock can be an abstraction.
      The clock ticks could be, for example, weighted instructions
      on some convenient and mutually-agreed architecture.
    &lt;/p&gt;
    &lt;p&gt;
      By the time Earley was writing, programmers had discovered that simply comparing
      speeds,
      even on well-chosen abstract clocks, was not enough.
      Computers were improving very quickly.
      A speed result
      that was clearly significant when the comparison was made
      could quickly become unimportant.
      Researchers needed to
      talk about time efficiency in a way that made what they said as true
      decades later as on the day they said it.
      To do this, researchers created the idea of time complexity.
    &lt;/p&gt;
    &lt;p&gt;Time complexity is measured using several notations, but the most
      common is
      &lt;a href=&quot;http://en.wikipedia.org/wiki/Big_O_notation&quot;&gt;big-O
        notation&lt;/a&gt;.
      Here's the idea:
      Assume we are comparing two algorithms, Algorithm A and Algorithm B.
      Assume that algorithm A uses 42 weighted instructions for each input symbol.
      Assume that algorithm B uses 1792 weighted instructions for each input symbol.
      Where the count of input symbols is N,
      A's speed is 42*N, and B's is 1792*N.
      But the time complexity of both is the same: O(N).
      The big-O notation throws away the two &quot;constant factors&quot;, 42 and 1792.
      Both are said to be &quot;linear in N&quot;.
      (Or more often, just &quot;linear&quot;.)
    &lt;/p&gt;
    &lt;p&gt;It often happens that algorithms we need to compare for time efficiency
      have different speeds,
      but the same time complexity.
      In practice,
      this usually this means we can treat them as having essentially
      the same time efficiency.
      But not always.
      It sometimes happens that this difference is relevant.
      When this happens, the rap against the slower algorithm is that it
      has a &quot;high constant factor&quot;.
    &lt;/p&gt;
    &lt;h3&gt;OK, about that high constant factor&lt;/h3&gt;
    &lt;p&gt;What is the &quot;constant factor&quot; between Earley and the current favorite
      parsing algorithm, as a number?
      (My interest is practical, not historic,
      so I will be talking about Earley's
      as modernized by Aycock, Horspool, Leo and myself.
      But much of what I say applies to Earley's algorithm in general.)
    &lt;/p&gt;
    &lt;p&gt;What the current favorite parsing algorithm is
      can be an interesting question.
      When Earley wrote, it was hand-written recursive descent.
      The next year (1969) LALR parsing was invented,
      and the year after (1970) a tool that used it was introduced -- yacc.
      At points over the next decades,
      yacc chased both Earley's
      and recursive descent almost completely out of the textbooks.
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2010/09/perl-and-parsing-6-rewind.html&quot;&gt;
        But as I have detailed elsewhere&lt;/a&gt;,
          yacc had serious problems.
          In 2006 things went full circle -- the industry's standard C
          compiler, GCC, replaced LALR with recursive descent.
        &lt;/p&gt;
    &lt;p&gt;So back to 1970.
    That year, Jay Earley wrote up his algorithm for
    &quot;Communications of the ACM&quot;,
      and put a rough number on his &quot;constant factor&quot;.
      He said that his algorithm was an &quot;order of magnitude&quot; slower
      than the current favorites -- a factor of 10.
      Earley suggested ways to lower this 10-handicap,
      and modern implementations have followed up on them
      and found others.
      But for this post,
      let's concede the factor of ten and throw
      in another.
      Let's say Earley's is 100 times slower than the current favorite,
      whatever that happens to be.
    &lt;/p&gt;
    &lt;h3&gt;Moore's Law and beyond&lt;/h3&gt;
    &lt;p&gt;Let's look at the handicap of 100
      in the light of Moore's Law.
      Since 1968, computers have gotten a billion times faster -- nine orders
      of magnitude. Nine factors of ten.
      This means that today Earley's runs
      seven factors of ten faster than
      the current favorite algorithm did in
      1968.
      Earley's is 10 million times as fast as the algorithm that was
      then considered practical.
    &lt;/p&gt;
    &lt;p&gt;
      Of course, our standard of &quot;fast enough to be practical&quot; also evolves.
      But it evolves a lot more slowly.
      Let's exaggerate
      and say that &quot;practical&quot; meant &quot;takes an hour&quot; in 1968,
      but that today we would demand that the same program take only a second.
      Do the arithmetic and you find that Earley's is now
      more than 2,000 times faster than it needs to be to be practical.
    &lt;/p&gt;
    &lt;p&gt;Bringing in Moore's Law is just the beginning.
      The handicap Jay Earley gave his algorithm
      is based on a straight comparison of CPU speeds.
      But parsing, in practical cases, involves I/O.
      And the &quot;current favorite&quot; needs to do as much I/O as Earley's.
      I/O overheads, and the accompanying context switches,
      swamp considerations of CPU speed,
      and that is more true today
      that it was in 1968.
      When an application is I/O bound, CPU is in effect free.
      Parsing may not be I/O bound in this sense, but neither
      is it one of those applications where the comparison can be made
      in raw CPU terms.
    &lt;/p&gt;
    &lt;p&gt;Finally, pipelining has changed
      the nature of the CPU overhead itself radically.
      In 1968, the time to run a series of CPU
      instructions varied linearly with the number of instructions.
      Today, that is no longer true,
      and the change favors strategies like Earley's,
      which require a higher instruction count,
      but achieve efficiency in other ways.
    &lt;/p&gt;
    &lt;h3&gt;Achievable speed&lt;/h3&gt;
    &lt;p&gt;
      So far, I've spoken in terms of theoretical speeds, not achievable ones.
      That is, I've assumed that both Earley's
      and the current favorite are producing their best speed, unimpeded by
      implementation considerations.
    &lt;/p&gt;
    &lt;p&gt;
      Earley, writing in 1968 and thinking of hand-written recursive descent,
      assumed that production compilers
      could be, and in practice usually would be,
      written by
      programmers with plenty of time to do
      careful and well-thought-out hand-optimization.
      After forty-five years of real-life experience,
      we know better.
    &lt;/p&gt;
    &lt;p&gt;
      In those widely used practical compilers and interpreters
      that rely on lots of procedural logic --
      and these days that is almost all of them --
      it is usually all the maintainers can do to keep the procedural logic correct.
      In all but a few cases, optimization is opportunistic,
      not systematic.
      Programmers have been exposed to
      the realities of parsing with
      large amounts of complex procedural logic,
      and hand-written recursive descent has acquired a
      reputation for being slow.
    &lt;/p&gt;
    &lt;p&gt;
      In theory,
      LALR based compilers are less dependent on procedural
      parsing and therefore easier to keep optimal.
      In practice they are as bad or worse.
      LALR parsers usually still need a considerable amount of procedural logic,
      but procedural logic is harder to write for LALR than it
      is for recursive descent.
    &lt;/p&gt;
    &lt;p&gt;Modern Earley parsing
      has a much easier time actually delivering
      its theoretical best speed in practice.
      Earley's is powerful enough,
      and in its modern version well-enough aware of the state of the parse,
      that procedural logic can be kept to minimum or eliminated.
      Most of the parsing is done by the mathematics at its core.
    &lt;/p&gt;
    &lt;p&gt;
      The math at Earley's core can be heavily optimized,
      and any optimization benefits all applications.
      Optimization of special-purpose procedural logic benefits
      only the application that uses that logic.
    &lt;/p&gt;
    &lt;h3&gt;Other considerations&lt;/h3&gt;
    &lt;p&gt;But you might say,
    &lt;/p&gt;&lt;blockquote&gt;
      &quot;A lot of interesting points, Jeffrey, but all things being
      equal, a factor of 10,
      or even what's left from a factor of ten once I/O,
      pipelining and implementation inefficiencies have all nibbled away at it,
      is still worth having.
      It may in a lot of instances not even be measurable, but why not grab
      it for the sake of the cases where it is?&quot;
    &lt;/blockquote&gt;&lt;p&gt;
      Which is a good point.
      The &quot;implementation inefficiences&quot; can be nasty enough that Earley's is in
      fact faster in raw terms,
      but let's assume
      that some cost in speed is still being paid for the use of Earley's.
      Why incur that cost?
    &lt;/p&gt;&lt;h4&gt;Error diagnosis&lt;/h4&gt;&lt;p&gt;
      The parsing algorithms currently favored,
      in their quest for efficiency,
      do not maintain full
      information about the state of the parse.
      This is fine when the source is 100% correct,
      but in practice an important function of a parser is to find and
      diagnose errors.
      When the parse fails, the current favorites
      often have little idea of why.
      An Earley parser knows the full state of the parse.
      This added knowledge can save a lot of
      programmer time.
    &lt;/p&gt;&lt;h4&gt;Readability&lt;/h4&gt;
    &lt;p&gt;
      The more that a parser does from the grammar,
      and the less procedural logic it uses,
      the more readable the code will be.
      This has a determining effect on maintainance costs
      and the software's ability to evolve over time.
    &lt;/p&gt;&lt;h4&gt;Accuracy&lt;/h4&gt;
    &lt;p&gt;Procedural logic can produce inaccuracy -- inability
      to describe or control the actual language begin parsed.
      Some parsers, particularly LALR and PEG,
      have a second major source of inaccuracy -- they use
      a precedence scheme for conflict resolution.
      In specific cases, this can work, but
      precedence-driven conflict resolution
      produces a language without
      a &quot;clean&quot; theoretical description.
    &lt;/p&gt;
    &lt;p&gt;
      The obvious problem with not knowing what language you
      are parsing is failure to parse correct source code.
      But another, more subtle, problem can be worse over the
      life cycle of a language ...
    &lt;/p&gt;
    &lt;h4&gt;False positives&lt;/h4&gt;
    &lt;p&gt;False positives are cases
      where the input is in error,
      and should be reported as such, but instead
      the result is what you wanted.
      This may sound like unexpected good news,
      but when a false positive does surface,
      it is quite possible that it cannot be fixed
      without breaking code that, while incorrect, does work.
      Over the life of a language, false positives are deadly.
      False positives produce buggy and poorly understood code
      which must be preserved and maintained forever.
    &lt;/p&gt;
    &lt;h4&gt;Power&lt;/h4&gt;
    &lt;p&gt;
      The modern Earley implementation can parse vast classes
      of grammar in linear time.
      These classes include all those currently in practical use.
    &lt;/p&gt;&lt;h4&gt;Flexibility&lt;/h4&gt;
    &lt;p&gt;Modern Earley implementations
      parse all context-free grammars in times that are, in practice,
      considered optimal.
      With other parsers,
      the class of grammars parsed is highly restricted,
      and there is usually a real danger that a new change
      will violate those restrictions.
      As mentioned,
      the favorite alternatives to Earley's
      make it hard to know exactly what language you are,
      in fact, parsing.
      A change can break one of these parsers
      without there being any indication.
      By comparison,
      syntax changes and extensions to Earley's grammars
      are carefree.
    &lt;/p&gt;
    &lt;h3&gt;For more about Marpa&lt;/h3&gt;
    &lt;p&gt;
      Above I've spoken of &quot;modern Earley parsing&quot;,
      by which I've meant Earley parsing as amended and improved
      by the efforts of Aho, Horspool, Leo and myself.
      At the moment, the only implementation that contains
      all of these modernizations is Marpa.
    &lt;/p&gt;
    &lt;p&gt;
      Marpa's latest version is
      &lt;a href=&quot;https://metacpan.org/module/Marpa::R2&quot;&gt;Marpa::R2,
        which is available on CPAN&lt;/a&gt;.
      Marpa's
      &lt;a href=&quot;https://metacpan.org/module/JKEGL/Marpa-R2-2.052000/pod/Scanless/DSL.pod&quot;&gt;SLIF
        is
        a new interface&lt;/a&gt;,
      which represents a major increase
      in Marpa's &quot;whipitupitude&quot;.
      The SLIF has tutorials
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/01/dsl_simpler2.html&quot;&gt;here
      &lt;/a&gt;
      and
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/01/announce_scanless.html&quot;&gt;
        here&lt;/a&gt;.
      Marpa has
      &lt;a href=&quot;http://jeffreykegler.github.com/Marpa-web-site/&quot;&gt;a web page&lt;/a&gt;,
      and of course it is the focus of
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/&quot;&gt;
        my &quot;Ocean of Awareness&quot; blog&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;
      Comments on this post
      can be sent to the Marpa's Google Group:
      &lt;code&gt;marpa-parser@googlegroups.com&lt;/code&gt;
    &lt;/p&gt;</description>
  </item>
  <item>
    <title>Marpa's SLIF now allows procedural parsing</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/04/procedural.html</link>
    <description>  &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;
      Marpa's SLIF (scanless interface)
      allows an application to parse directly from any BNF grammar.
      Marpa parses vast classes of grammars in linear time,
      including all those classes currently in practical use.
      With
      &lt;a href=&quot;https://metacpan.org/release/Marpa-R2&quot;&gt;
        its latest release&lt;/a&gt;,
      Marpa::R2's SLIF
      also allows an application to intermix
      its own custom lexing and parsing logic
      with Marpa's,
      and to switch back and forth between them.
      This means,
      among other things,
      that Marpa's SLIF can now
      do procedural parsing.
    &lt;/p&gt;
    &lt;p&gt;
      What is procedural parsing?
      Procedural parsing is parsing using
      ad hoc code in a procedural language.
      The opposite of procedural parsing is declarative parsing
      -- parsing driven by some kind of formal description
      of the grammar.
      Procedural parsing
      may be described as what you do when you've given up
      on your parsing algorithm.
      Dissatisfaction with parsing theory
      has left modern programmers accustomed to procedural parsing.
      And in fact some problems are best tackled with procedural parsing.
    &lt;/p&gt;
    &lt;h3&gt;An example&lt;/h3&gt;
    &lt;p&gt;
      One such problem is parsing Perl-style here-documents.
      Peter Stuifzand has tackled this using
      &lt;a href=&quot;https://metacpan.org/release/JKEGL/Marpa-R2-2.052000&quot;&gt;
        the
        just-released version of Marpa::R2&lt;/a&gt;.
      For those unfamiliar, Perl allows documents to be incorporated
      into its source files in line-oriented fashion as &quot;here-documents&quot;.
      Here-documents can be used in expressions.
      The syntax to do this is very handy, if a little strange.
      For example,
    &lt;/p&gt;
    &lt;blockquote&gt;&lt;pre&gt;
say &amp;lt;&amp;lt;ENDA, &amp;lt;&amp;lt;ENDB, &amp;lt;&amp;lt;ENDC; say &amp;lt;&amp;lt;ENDD;
a
ENDA
b
ENDB
c
ENDC
d
ENDD&lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      starts with a single line declaring four here-documents spread out
      over two
      &lt;tt&gt;say&lt;/tt&gt;
      statements.
      The expressions of the form
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;&amp;lt;&amp;lt;ENDX&lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      are here-document expressions.
      &lt;tt&gt;&amp;lt;&amp;lt;&lt;/tt&gt;
      is the heredoc operator.
      The string which follows it (in this example,
      &lt;tt&gt;ENDA&lt;/tt&gt;,
      &lt;tt&gt;ENDB&lt;/tt&gt;, etc.) is the heredoc terminator string --
      the string that will signal end
      of body of the here-document.
      The body of the here-documents follow, in order, over the next eight lines.
      More details of here-document syntax, with examples, can be found
      in
      &lt;a href=&quot;http://perldoc.perl.org/perlop.html#Quote-Like-Operators&quot;&gt;the
        Perl documentation&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;All of this poses quite a challenge to a parser-lexer combination,
      which is one reason I chose it as an example --
      to illustrate that the Marpa's SLIF support for procedural parsing can
      handle genuinely difficult cases.
      There are a few ways Marpa could approach this.
      The one
      Peter Stuifzand chose was to
      to read the
      here-document's body as the value of the terminator in
      each
      &lt;tt&gt;&amp;lt;&amp;lt;ENDX&lt;/tt&gt;
      expression.
    &lt;/p&gt;
    &lt;p&gt;
      The strategy works this way:
      Marpa allows the application to mark certain lexemes as &quot;pause&quot; lexemes.
      Whenever a &quot;pause&quot; lexeme is encountered, Marpa's internal scanning stops,
      and control is handed over to the application.
      In this case, the application is set up to pause after every newline,
      and before the terminator in every here-document expression.
    &lt;/p&gt;
    &lt;p&gt;
      While reading the line containing the four here-document expressions,
      Marpa's SLIF pauses and resumes five times -- once for each here-document expression,
      then once for the final newline.
      Details can be found in compact form in the heavily commented code
      in
      &lt;a href=&quot;https://gist.github.com/jeffreykegler/5431739&quot;&gt;this
        Github gist&lt;/a&gt;.
    &lt;/p&gt;
    &lt;h3&gt;Marpa as a better procedural parser&lt;/h3&gt;
    &lt;p&gt;So far I've talked in terms of Marpa &quot;allowing&quot; procedural parsing.
      In fact, there can be much more to it.
      Marpa can make procedural parsing easier and more accurate.
    &lt;/p&gt;
    &lt;p&gt;Marpa knows, at every point, which rules it is recognizing, and how far it
      is into them.
      Marpa also knows which new rules the grammar expects, and which terminals.
      The procedural parsing logic can consult this information to guide its decisions.
      Marpa can provide your procedural parsing logic with radar,
      as well as the option to use a very smart autopilot.
    &lt;/p&gt;
    &lt;h3&gt;For more about Marpa&lt;/h3&gt;
    &lt;p&gt;
      Marpa's latest version is
      &lt;a href=&quot;https://metacpan.org/module/Marpa::R2&quot;&gt;Marpa::R2,
        which is available on CPAN&lt;/a&gt;.
      Marpa's
      &lt;a href=&quot;https://metacpan.org/module/JKEGL/Marpa-R2-2.052000/pod/Scanless/DSL.pod&quot;&gt;SLIF
        is
        a new interface&lt;/a&gt;,
      which represents a major increase
      in Marpa's &quot;whipitupitude&quot;.
      The SLIF has tutorials
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/01/dsl_simpler2.html&quot;&gt;here
      &lt;/a&gt;
      and
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/01/announce_scanless.html&quot;&gt;
        here&lt;/a&gt;.
      Marpa has
      &lt;a href=&quot;http://jeffreykegler.github.com/Marpa-web-site/&quot;&gt;a web page&lt;/a&gt;,
      and of course it is the focus of
      &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/&quot;&gt;
        my &quot;Ocean of Awareness&quot; blog&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;
      Comments on this post
      can be sent to the Marpa's Google Group:
      &lt;code&gt;marpa-parser@googlegroups.com&lt;/code&gt;
    &lt;/p&gt;</description>
  </item>
  </channel>
</rss>
