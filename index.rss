<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>Grammar reuse</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/12/composable.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;Every year the Perl 6 community creates an &quot;Advent&quot; series of posts.
      I usually follow these, but
      &lt;a href=&quot;https://perl6advent.wordpress.com/2015/12/08/day-8-grammars-generating-grammars/&quot;&gt;one
        in particular&lt;/a&gt;
      caught my attention this year.
      It presents a vision of a future where programming is language-driven.
      A vision that I share.
      The post went on to encourage that its readers to follow up on this vision,
      and suggested an approach.
      But I do not think the particular approach suggested would be fruitful.
      In this post I'll start to explain why.
    &lt;/p&gt;
    &lt;h2&gt;Reuse&lt;/h2&gt;
    &lt;p&gt;The focus of the Advent post was language-driven programming,
      and that is the aspect that excites me most.
      But the points that wish to make are probably more easily understand if
      I root them in more familiar ground
      -- grammar reuse.
    &lt;/p&gt;&lt;p&gt;
      Most programmers will be very familiar with grammar reuse from regular expressions.
      In the regular expression (&quot;RE&quot;) world programming by cutting and pasting
      is very practical and often practiced --
      so much so that there
      is even a backlash against what some writers call &quot;cargo cult programming&quot;.
    &lt;/p&gt;
    &lt;p&gt;
      For this post I will consider grammar reusability to be the ability
      to join two grammars and create a third.
      This is also sometimes called grammar composition.
      I will use the term &quot;grammar&quot; somewhat loosely,
      and include RE's and PEG descriptions
      among the things that I call &quot;grammars&quot;.
      Ideally, when you compose two grammars, the result will be
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;a language you can reasonably predict,
      &lt;/li&gt;
      &lt;li&gt;and, if each of the two original grammars can be parsed in reasonable time,
        a language that can be parsed in reasonable time.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      Not all language representations are reusable.
      RE's are, and BNF is.
      PEG, which looks like a combination of BNF and RE's.
      But a PEG description is a parser specification,
      not BNF and is not an RE,
      and PEG parser specifications are
      one of the least reusable language representations ever invented.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and regular expressions&lt;/h2&gt;
    &lt;p&gt;We'll start with RE's, which are as well-behaved under
      reuse as a language representation can get.
      The combination of two RE's is always another RE,
      and you can reasonably determine what language the combined RE recognizes by
      examining it.
      Further, every RE is parseable in linear time.
    &lt;/p&gt;
    &lt;p&gt;The one downside, often mentioned by critics, is that RE's
      do not scale in terms of readability.
      Here, however, the problem is not really one of reusability.
      The problem is that RE's are quite limited in their capabilities,
      and programmers often exploit the excellent behavior of RE's under reuse
      to push them into applications for which RE's just do not have the power.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and PEG&lt;/h2&gt;
    &lt;p&gt;When programmers first look at PEG syntax, they often think they've encountered
      paradise. They see both BNF and RE's, and imagine they'll have the
      best of each.
      But the convenient behavior of
      RE's depends on their ambiguity.
      You simply cannot write
      an ambiguous RE -- it's impossible.
    &lt;/p&gt;
    &lt;p&gt;
      More powerful and more flexible, BNF allows you to describe many more grammars --
      including ambiguous ones.
      How does PEG resolve this?  With a Gordian knot approach.
      Whenever it encounters an ambiguity, it throws all but one of the choices away.
      The author of the PEG description gets some control over which choice is thrown away --
      he specifies an order of preference for the choices.
      But degree of control is less than it seems,
      and in practice PEG grammars have all
      the predictability of nitroglycerin.
    &lt;/p&gt;&lt;p&gt;
      Consider these two PEG grammars:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
	 (&quot;a&quot;|&quot;aa&quot;)&quot;a&quot;
	 (&quot;aa&quot;|&quot;a&quot;)&quot;a&quot;
       &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      One accepts &quot;aaa&quot; but not &quot;aa&quot;.  The other does the reverse.
      Which one?
      For the answer to this, a discussion of which classes of grammar &lt;b&gt;are&lt;/b&gt; predictable for PEG, and more,
      look at
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html&quot;&gt;
        my previous post on PEG&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;For practical grammars, using even one PEG grammar
      means giving up on knowing more much about the language you are actually parsing
      than that you got it to pass your test suite.
      Combining two PEG grammars is rolling the dice.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and the native Perl 6 parser&lt;/h2&gt;
    &lt;p&gt;
      The native Perl 6 parser, is an extended PEG parser.
      These extensions are very interesting from the PEG point of view.
      The PEG &quot;tie breaking&quot; has been changed,
      and backtracking can be used.
      These features mean the Perl 6 parser can be extended to languages
      well beyond what
      ordinary PEG parsers cannot handle.
      But, if you use the extra features, reuse will be even trickier than
      if you stuck with vanilla PEG.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and general BNF parsing&lt;/h2&gt;
    &lt;p&gt;
      As mentioned, general BNF is reusable, and so general BNF parsers like Marpa
      are as reusable as regular expressions, with two caveats.
      First, if the two grammars are not doing their own lexing, their lexers will have
      to be reusable.
    &lt;/p&gt;&lt;p&gt;Second,
      with regular expressions you had the advantage that
      &lt;b&gt;every&lt;/b&gt;
      regular expression parses in linear time, so that speed will be acceptable.
      Marpa users reuse grammars and pieces of grammars all the time.
      The result is always the language specified by the merged BNF,
      and in fact, I've never heard anyone complain that performance deterioriated.
      &lt;p&gt;
      But, while it usually does not happen,
      it is possible to combine two Marpa grammars that run in linear time
      and end up with one that does not.
      You can guarantee your merged Marpa grammar will stay linear if you follow 2 rules:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;keep the grammar unambiguous;&lt;/li&gt;
      &lt;li&gt;don't use an unmarked middle recursion.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      Unmarked middle recursions are not things you're likely to need a lot: they
      are those palindromes where you have to count to find the middle:
      grammars like &quot;&lt;tt&gt;A ::= a | a A a&lt;/tt&gt;&quot;.
      If you do use a middle recursions at all, it is almost certainly going to
      be marked, like &quot;&lt;tt&gt;A ::= b | a A a&lt;/tt&gt;&quot;,
      which generates strings like &quot;&lt;tt&gt;aabaa&lt;/tt&gt;&quot;.
      With Marpa, as with RE's, resuse is easy and practical.
      And, as I hope to show in a future post, unlike RE's,
      Marpa opens the road to language-driven programming.
    &lt;/p&gt;
    &lt;h2&gt;Perl 6&lt;/h2&gt;
    &lt;p&gt;I'd like to emphasize that I'm a fan of the Perl 6 effort.
      I certainly
      &lt;b&gt;should&lt;/b&gt;
      be a supporter, after the many favors they've done for me
      and the Marpa community over the years.
      If the points of this post are taken,
      they will disappoint some of the
      hopes for applications of the native Perl 6 parser.
      But these applications have not been central to the Perl 6 effort,
      of which I will be an eager student over the coming months.
    &lt;/p&gt;
    &lt;h2&gt;Comments&lt;/h2&gt;
    &lt;p&gt;Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Fast handy languages</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/08/fast_handy.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;Back around 1980, I had access to UNIX and a language I wanted to parse.
      I knew that UNIX had all the latest CS tools.
      So I expected to type in my BNF and &quot;Presto, Language!&quot;.
    &lt;/p&gt;
    &lt;p&gt;Not so easy, I was told.
      Languages were difficult things created with complex tools
      written by experts who understood the issues.
      I recall thinking that,
      while English had a syntax that is
      as hard as they come,
      toddlers manage to parse it
      just fine.
      But experts are experts,
      and more so at second-hand.
    &lt;/p&gt;
    &lt;p&gt;I was steered to an LALR-based parser called yacc.
      (Readers may be more familiar with bison, a yacc successor.)
      LALR had extended the class of quickly parseable grammars a bit
      beyond recursive descent.
      But recursive descent was simple in principle,
      and its limits were easy to discover and work around.
      LALR, on the hand, was OK when it worked, but
      figuring out why it failed when it failed
      was more like decryption than debugging,
      and this was the case both with parser development
      and run-time errors.
      I soon gave up on yacc
      and found another way to solve my problem.
    &lt;/p&gt;
    &lt;p&gt;Few people complained about yacc on the Internet.
      If you noise it about that you are unable
      to figure out how to use
      what everybody says is the state-of-the-art tool,
      the conclusions drawn may not be the ones you want.
      But my experience seems to have been more than common.
    &lt;/p&gt;
    &lt;p&gt;LALR's claim to fame was that it was the basis of the
      industry-standard C compiler.
      Over three decades,
      its maintainers suffered amid the general silence.
      But by 2006, they'd had enough.
      GCC (the new industry standard)
      ripped its LALR engine out.
      By then
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html&quot;&gt;the
        trend back to recursive descent&lt;/a&gt;
      was well underway.
    &lt;/p&gt;
    &lt;h3&gt;A surprise discovery&lt;/h3&gt;
    &lt;p&gt;Back in the 1970's,
      there had been more powerful alternatives
      to LALR and recursive descent.
      But they were
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/04/fast_enough.html&quot;&gt;reputed
        to be slow&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;For some applications slow is OK.
      In 2007 I decided that a parsing tool that parsed
      all context-free languages at state-of-the-art speeds,
      slow or fast as the case might be,
      would be a useful addition to programmer toolkits.
      And I ran into a surprise.
    &lt;/p&gt;
    &lt;p&gt;Hidden in the literature was an amazing discovery --
      an 1991 article by Joop Leo that
      described how to modify Earley's
      algorithm to be fast for every language class in practical use.
      (When I say &quot;fast&quot; in this article, I will mean &quot;linear&quot;.)
      Leo's article had been almost completely ignored --
      my project (&lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;Marpa&lt;/a&gt;)
      would become its first
      practical implementation.
    &lt;/p&gt;
    &lt;h3&gt;Second-order languages&lt;/h3&gt;
    &lt;p&gt;The implications of Leo's discovery go well beyond speed.
      If you can rely on the BNF that you write always producing
      a practical parser, you can auto-generate your language.
      In fact,
      you can write languages which write languages.
    &lt;/p&gt;
    &lt;h3&gt;Which languages are fast?&lt;/h3&gt;
    &lt;p&gt;The Leo/Earley algorithm is not fast
      for every BNF-expressible language.
      BNF is powerful, and you can write exponentially
      ambiguous languages in it.
      But programmers these days
      mostly care about unambiguous languages --
      we are accustomed to tools and techniques
      that parse only a subset of these.
    &lt;/p&gt;
    &lt;p&gt;
      As I've said, Marpa is fast for every language
      class in practical use today.
      Marpa is almost certainly fast for any language
      that a modern programmer has in mind.
      Unless you peek ahead at the hints I am about to give you,
      in fact, it is actually
      &lt;b&gt;hard&lt;/b&gt;
      to write an unambiguous
      grammar that goes non-linear on Marpa.
      Simply mixing up lots of left, right and middle recursions
      will
      &lt;b&gt;not&lt;/b&gt;
      be enough to make an
      unambiguous grammar go non-linear.
      You will also need to violate a rule
      in the set that
      I am about to give you.
    &lt;/p&gt;
    &lt;p&gt;To guarantee that Marpa is fast for your BNF language,
      follow three rules:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Rule 1: Your BNF must be unambiguous.
      &lt;/li&gt;
      &lt;li&gt;Rule 2: Your BNF must have no &quot;unmarked&quot; middle recursions.
      &lt;/li&gt;
      &lt;li&gt;Rule 3: All of the right-recursive symbols
        in your BNF must be dedicated
        to right recursion.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;Rule 3 turns out to be very easy to obey.
      I discuss it in detail in the next section,
      which will be about how to break these rules and
      get away with it.
    &lt;/p&gt;
    &lt;p&gt;Before we do that,
      let's look at what an &quot;unmarked&quot; middle recursion is.
      Here's an example of a &quot;marked&quot; middle recursion:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       M ::= 'b'
       M ::= 'a' M 'a'
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      Here the &quot;b&quot; symbol is the marker.
      This marked middle recursion generates sequences like
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       b
       a b a
       a a b a a
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;Now for an &quot;unmarked&quot; middle recursion:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       M ::= 'a' 'a'
       M ::= 'a' M 'a'
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      This unmarked middle recursion generates sequences like
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       a a
       a a a a
       a a a a a a
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      In this middle recursion there is no marker.
      To know where the middle is,
      you have to scan all the way to the end,
      and then count back.
    &lt;/p&gt;
    &lt;p&gt;A rule of thumb is that if you can &quot;eyeball&quot; the middle
      of a long sequence,
      the recursion is marked.
      If you can't, it is unmarked.
      Unfortunately, we can't characterize exactly what a marker
      must look like -- a marker can encode the moves of a Turing machine,
      so marker detection is undecidable.
    &lt;/p&gt;
    &lt;h3&gt;How to get away with breaking the rules&lt;/h3&gt;
    &lt;p&gt;The rules about ambiguity and recursions are &quot;soft&quot;.
      If you only use limited ambiguity and
      keep your rule-breaking recursions short,
      your parser will stay fast.
    &lt;/p&gt;
    &lt;p&gt;Above, I promised to explain rule 3, which insisted that
      a right recursive symbol be &quot;dedicated&quot;.
      A right recursive symbol is &quot;dedicated&quot; if it appears only
      as the recursive symbol in a right recursion.
      If your grammar is unambiguous, but you've used an &quot;undedicated&quot;
      right-recursive symbol, that is easy to fix.
      Just rewrite the grammar, replacing the &quot;undedicated&quot; symbol
      with two different symbols.
      Dedicate one of the two to the right recursion,
      and use the other symbol everywhere else.
    &lt;/p&gt;
    &lt;h3&gt;When NOT to use Marpa&lt;/h3&gt;
    &lt;p&gt;The languages I have described as &quot;fast&quot; for Marpa
      include all those in practical use and many more.
      But do you really want to use Marpa for all of them?
      Here are four cases for which Marpa is probably not
      your best alternative.
    &lt;/p&gt;
    &lt;p&gt;The first case: a language that parses easily with a regular
      expression.
      The regular expression will be much faster.
      Don't walk away from a good thing.
    &lt;/p&gt;
    &lt;p&gt;The second case:
      a language
      that is easily parsed using a single
      loop and some state that fits into constant space.
      This parser might be very easy to write and maintain.
      If you are using a much slower higher level language,
      Marpa's optimized C language
      may be a win on CPU speed.
      But, as before, if you have a good thing,
      don't walk away from it.
    &lt;/p&gt;&lt;p&gt;The third case:
      a variation on the second.
      Here your single loop might be getting out of hand,
      making you yearn for the syntax-driven convenience
      of Marpa,
      but your state still fits into constant space.
      In its current implementation, Marpa keeps all of its
      parse tables forever, so Marpa does
      &lt;b&gt;not&lt;/b&gt;
      parse in constant space.
      Keeping the tables
      allows Marpa to deal with the full structure of its
      input, in a way that a SAX-ish approaches cannot.
      But if space requirements are an issue,
      and your application allows a simplified constant-space
      approach,
      Marpa's power and convenience may not be enough to
      make up for that.
    &lt;/p&gt;
    &lt;p&gt;The fourth case:
      a language that
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;is very small;
      &lt;/li&gt;
      &lt;li&gt;changes slowly or not at all, and does not grow in complexity;
      &lt;/li&gt;
      &lt;li&gt;merits careful hand-optimization, and has available the staff
        to do it;
      &lt;/li&gt;
      &lt;li&gt;merits and has available the kind of on-going support that will
        keep your code optimized under changing circumstances; and
      &lt;/li&gt;
      &lt;li&gt;is easily parseable via recursive descent:&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      It is rare that all of these are the case,
      but when that happens,
      recursive descent is often preferable to Marpa.
      Lua and JSON
      are two languages which meet the above criteria.
      In Lua's case, it targets platforms with very restricted memories,
      which is an additional reason to prefer recursive descent --
      Marpa has a relatively large footprint.
    &lt;/p&gt;
    &lt;p&gt;It was not good luck that made
      both Lua and JSON good targets for recursive descent --
      they were designed around its limits.
      JSON is a favorite test target of Marpa for just these reasons.
      There are carefully hand-optimized C language parsers for us to
      benchmark against.
    &lt;/p&gt;
    &lt;p&gt;We get closer and closer,
      but Marpa will
      never beat small hand-optimized JSON parsers in software.
      However, while recursive descent is a technique for hand-writing parsers,
      Marpa is a mathematical algorithm.
      Someday,
      instructions for manipulating Earley items could be implemented directly
      in silicon.
      When and if that day comes,
      Earley's algorithm will beat recursive descent even at
      parsing the grammars that were designed for it.
    &lt;/p&gt;
    &lt;h3&gt;Comments&lt;/h3&gt;
    &lt;p&gt;Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Linear?  Yeah right.</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/03/linear.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;h3&gt;Linear?&lt;/h3&gt;
    &lt;p&gt;I have claimed that my new parser,
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;Marpa&lt;/a&gt;,
      is linear for vast classes of grammars,
      going well beyond what the traditional parsers can do.
      But skepticism is justified.
      When it comes to parsing algorithms,
      there have been a lot of time complexity claims
      that are hand-wavy, misleading or just
      plain false.
      This post describes how someone,
      who
      is exercising the appropriate degree of skepticism,
      might conclude that believing Marpa's claims is a reasonable
      and prudent thing to do.
    &lt;/p&gt;
    &lt;p&gt;
      Marpa's linearity claims seem to be,
      in comparison with the other parsers in practical use
      today,
      bold.
      Marpa claims linearity,
      not just for every class of grammar for which
      yacc/bison, PEG and recursive descent currently claim linearity,
      but for considerably more.
      (The mathematical details of these claims
      are in a
      &lt;a href=&quot;#DETAILS&quot;&gt;section at the end&lt;/a&gt;.)
      It seems too good to be true.
    &lt;/p&gt;
    &lt;h3&gt;Why should I believe you?&lt;/h3&gt;
    &lt;p&gt;The most important thing to realize,
      in assessing the believability of Marpa's time complexity claims,
      is that they are not new.
      They were already proved in a long-accepted paper in the refereed literature.
      They are the time complexity claims proved by Joop Leo for his algorithm
      in 1991, over two decades ago.
      Marpa is derived from Leo's algorithm, and its time complexity claims
      are those proved for Leo's algorithm.
    &lt;/p&gt;
    &lt;p&gt;Above I said that Marpa's time complexity claims &quot;seem&quot; bold.
      On any objective assessment, they are in fact a bit of a yawn.
      The claims
      &lt;b&gt;seem&lt;/b&gt;
      surprising only because a lot of people
      are unaware of Leo's results.
      That is, they are surprising in the same sense that someone who
      had avoided hearing about radio waves would be surprised
      to learn that he can
      communicate instantly
      with someone on the other side of the world.
    &lt;/p&gt;
    &lt;p&gt;So, if there's so little to prove, why does the
      &lt;a href=&quot;https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer&quot;&gt;
        Marpa paper&lt;/a&gt;
      have proofs?
      In Marpa, I made many implementation decisions about,
      and some changes to, the Leo/Earley algorithm.
      None of my changes produced better time complexity results --
      my only claim is that I did not change the Leo/Earley
      algorithm in a way that slowed it down.
      To convince myself of this claim, I reworked
      the original proofs of
      Leo and Earley,
      changing them to reflect my changes,
      and demonstrated that the results
      that Leo had obtained still held.
    &lt;/p&gt;
    &lt;p&gt;Proofs of this kind,
      which introduce no new mathematical techniques,
      but simply take a previous result and march from
      here to there by well-know means,
      are called &quot;tedious&quot;.
      In journals, where there's a need to conserve space,
      they are usually omitted,
      especially if,
      as is the case with Marpa's time complexity proofs,
      the results are intuitively quite plausible.
    &lt;/p&gt;
    &lt;h3&gt;Getting from plausible to near-certain&lt;/h3&gt;
    &lt;p&gt;So let's say you are not going to work through every line
      of Marpa's admittedly tedious proofs.
      We've seen that the results are intuitively plausible,
      as long as you don't reject the previous literature.
      But can we do better than merely &quot;plausible&quot;?
    &lt;/p&gt;&lt;p&gt;As an aside, many people misunderstand the phrase
      &quot;mathematically proven&quot;, especially as it applies to branches
      of math like parsing theory.
      The fact is that proofs in papers often contain errors.
      Usually these are minor,
      and don't affect the result.
      On the other hand, Jay Earley's paper,
      while one of the best Computer Science papers ever published,
      also contained a very
      serious error.
      And this error slipped past his Ph.D. committee and
      his referees.
      Mathematical arguments and proofs do not allow us to achieve
      absolute certainty.
      They can only improve the degree of certainty.
    &lt;/p&gt;
    &lt;p&gt;There's a second way to dramatically increase
      your degree of conviction
      in Marpa's linearity claims, and it is quite simple.
      Create examples of problematic grammars,
      run them and time them.
      This is not as satisfying as a mathematical proof,
      because no set of test grammars can be exhaustive.
      But if you can't find a counter-example
      to Marpa's linearity claims among the grammars of
      most interest to you,
      that should help lift
      your level of certainty to &quot;certain for
      all practical purposes&quot;.
    &lt;/p&gt;
    &lt;p&gt;Much of this increase in certainty can be
      achieved without bothering to run your own tests.
      Marpa is in wide use at this point.
      If Marpa was going quadratic on grammars
      for which it claimed to be linear,
      and these were grammars of practical interest,
      that would very likely have been noticed by now.
    &lt;/p&gt;
    &lt;h3&gt;I'm still not convinced&lt;/h3&gt;
    &lt;p&gt;Let's suppose all this has not brought you to
      the level of certainty you need to use Marpa.
      That means the reasonable thing is to continue to
      struggle to work with the restrictions of the
      traditional algorithms, right?
      No, absolutely not.
    &lt;/p&gt;
    &lt;p&gt;OK, so you don't believe that Marpa preserves
      the advances in power and speed made by Leo.
      Does that mean that parsers have to stay underpowered?
      No, it simply means that there should be a
      more direct implementation of Leo's 1991,
      bypassing Marpa.
    &lt;/p&gt;&lt;p&gt;But if you are looking for an implementation of Leo's
      1991 algorithm,
      I think you may end up coming back to Marpa as the most
      reasonable choice.
      Marpa's additional features
      include the ability to use custom,
      procedural logic,
      as you can with recursive descent.
      And Marpa has worked out a lot of the
      implementation details for you.
    &lt;/p&gt;&lt;h3&gt;Comments&lt;/h3&gt;
    &lt;p&gt;Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
    &lt;/p&gt;
    &lt;h3&gt;Appendix: Some technical details&lt;/h3&gt;
    &lt;p&gt;
      &lt;a name=&quot;DETAILS&quot;&gt;Above&lt;/a&gt;
      I talked about algorithms, classes of grammars and their
      linearity claims.
      I didn't give details because most folks aren't interested.
      For those who are, they are in this section.
    &lt;/p&gt;
    &lt;p&gt;
      yacc is linear for a grammar class called LALR,
      which is a subset of another grammar class
      called LR(1).
      If you are willing to hassle with GLR,
      bison claims linearity for all of LR(1).
      Recursive descent is a technique, not an algorithm,
      but it is top-down with look-ahead,
      and therefore can be seen as some form of LL(k),
      where k depends on how it is implemented.
      In practice, I suspect k is never much bigger than 3,
      and usually pretty close to 1.
      With packratting,
      PEG can be made linear for everything it
      parses but there is a catch --
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html&quot;&gt;
        only in limited cases do you know
        what language your PEG grammar actually parses&lt;/a&gt;.
      In current practice, that means your PEG grammar
      must be LL(1).
      Some of the PEG literature looks at techniques for
      extending this as far as LL-regular, but there are no
      implementations, and it remains to be seen if the
      algorithms described are practical.
    &lt;/p&gt;
    &lt;p&gt;
      The
      &lt;a href=&quot;https://www.academia.edu/10341474/Marpa_A_practical_general_parser_the_recognizer&quot;&gt;
        Marpa paper&lt;/a&gt;
      contains a proof,
      based on a proof of the same claim by
      Joop Leo,
      that Marpa is linear for LR-regular grammars.
      The LR-regular grammars
      include the LR(k) grammars for every k.
      So Marpa is linear for LR(1), LR(2), LR(8675309), etc.
      LR-regular also includes LL-regular.
      So every class of grammar under discussion
      in the PEG literature is
      already parsed in linear time by Marpa.
      From this,
      it is also safe to conclude that,
      if a grammar can be parsed by
      anything reasonably described as recursive descent,
      it can be parsed in linear time by Marpa.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>PEG: Ambiguity, precision and confusion</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/03/peg.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;h3&gt;Precise?&lt;/h3&gt;
    &lt;p&gt;&lt;a href=&quot;http://bford.info/packrat/&quot;&gt;PEG parsing&lt;/a&gt;
      is a new notation
      for a notorously tricky algorithm that goes back
      to the earliest computers.
      In its PEG form,
      this algorithm acquired an seductive new interface,
      one that looks like the best of
      extended BNF combined with the best of regular expressions.
      Looking at a sample of it, you are tempted to imagine
      that writing a parser has suddenly become a very straightforward
      matter. Not so.
    &lt;/p&gt;
    &lt;p&gt;For those not yet in the know on this,
      I'll illustrate with a pair of examples from
      &lt;a href=&quot;http://www.romanredz.se/papers/FI2008.pdf&quot;&gt;
        an excellent 2008 paper by Redziejowski&lt;/a&gt;.
      Let's start with these two PEG specifications.
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    (&quot;a&quot;|&quot;aa&quot;)&quot;a&quot;
    (&quot;aa&quot;|&quot;a&quot;)&quot;a&quot;
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      One of these two PEG grammars accepts
      the string &quot;&lt;tt&gt;aaa&lt;/tt&gt;&quot; but not the string &quot;&lt;tt&gt;aa&lt;/tt&gt;&quot;.
      The other does the opposite -- it accepts the string
      the string &quot;&lt;tt&gt;aa&lt;/tt&gt;&quot; but not the string &quot;&lt;tt&gt;aaa&lt;/tt&gt;&quot;.
      Can you tell which one?
      (For the answer,
      see page 4 of
      &lt;a href=&quot;http://www.romanredz.se/papers/FI2008.pdf&quot;&gt;
        Redziejowski 2008&lt;/a&gt;.)
    &lt;/p&gt;&lt;p&gt;
      Here is another example:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    A = &quot;a&quot;A&quot;a&quot;/&quot;aa&quot;
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      What language does this describe?
      All the strings in the
      language are obviously
      the letter &quot;&lt;tt&gt;a&lt;/tt&gt;&quot;,
      repeated some number of times.
      But which string lengths are in the language,
      and which are not?
      Again the answer is on
      page 4 of
      &lt;a href=&quot;http://www.romanredz.se/papers/FI2008.pdf&quot;&gt;
        Redziejowski 2008&lt;/a&gt;
      -- it's exactly those strings
      whose length is a power of 2.
    &lt;/p&gt;
    &lt;p&gt;With PEG, what you see in the extended BNF
      is not what you get.
      PEG parsing has been called &quot;precise&quot;,
      apparently based on
      the idea that PEG parsing is in a certain sense unambiguous.
      In this case &quot;precise&quot; is taken as synonymous with
      &quot;unique&quot;.
      That is, PEG parsing is precise in exactly the same
      sense that Jimmy Hoffa's body
      is at a precise location.
      There is (presumably) exactly one such place,
      but we are hard put to be any more specific about the matter.
    &lt;/p&gt;
    &lt;h3&gt;Syntax-driven?&lt;/h3&gt;
    &lt;p&gt;The advantage of using
      a syntax-driven parser generator is that
      the syntax you specify describes
      the language that will be parsed.
      For most practical grammars, PEG is not syntax-driven
      in this sense.
      Several important
      PEG researchers understand this issue,
      and have tried to deal with it.
      I will talk about their work below.
      This is much more at stake than bragging rights
      over which algorithm is really syntax-driven and
      which is not.
    &lt;/p&gt;
    &lt;p&gt;When you do not know the language your parser is
      parsing, you of course have the problem that
      your parser
      might not parse all the strings in your language.
      That can be dealt with by
      fixing the parser to accept the correct input,
      as you encounter problems.
    &lt;/p&gt;
    &lt;p&gt;A second, more serious, problem is often forgotten.
      Your PEG parser might accept strings that are
      &lt;b&gt;not&lt;/b&gt;
      in your language.
      At worst, this creates a security loophole.
      At best, it leaves with a choice:
      break compatiblity,
      or leave the problem unfixed.
    &lt;/p&gt;
    &lt;p&gt;It's important to be able to convince
      yourself
      that your code is correct by examining it and thinking
      about it.
      Beginning programmers often simply hack things,
      and call code complete once it passes the test suite.
      Test suites don't catch everything,
      but there is a worse problem with the beginner's approach.
    &lt;/p&gt;
    &lt;p&gt;
      Since the beginner has no clear idea of why his code
      works, even when it does,
      it is unlikely to be well-organized or readable.
      Programming techniques like PEG,
      where the code can be made to work,
      but where it is much harder,
      and in practice usually not possible,
      to be sure why the code works,
      become maintenance nightmares.
    &lt;/p&gt;
    &lt;p&gt;
      The maintenance implications are especially worrisome if
      the PEG parser is for a language with a life cycle that may
      involve bug fixes or other changes.
      The impact of even
      small changes to a PEG specification
      is hard to predict and
      hard to discover after the fact.
    &lt;/p&gt;
    &lt;h3&gt;Is PEG unambiguous?&lt;/h3&gt;
    &lt;p&gt;PEG is not unambiguous in any
      helpful sense of that word.
      BNF allows you to specify ambiguous grammars,
      and that feature is tied to its power and flexibility
      and often useful in itself.
      PEG will only deliver one of those parses.
      But without
      an easy way of knowing which parse,
      the underlying ambiguity is not addressed --
      it is just ignored.
    &lt;/p&gt;
    &lt;p&gt;My Marpa parser
      is a general BNF parser based on Earley's.
      It also can simply throw all but one of the parses in an ambiguous
      parse away.
      But I would not feel justified in saying to a user who has an
      issue with ambiguity,
      that Marpa has solved her problem
      by throwing all but one arbitrarily chosen result.
    &lt;/p&gt;
    &lt;p&gt;
      Sticking with Marpa for a moment,
      we can see one
      example of a more helpful approach
      to ambiguity.
      Marpa allows a user to rank rules,
      so that all but the highest ranking rules are not used
      in a parse.
      Marpa's rule rankings are specified in its BNF,
      and they work together with the BNF
      in an intuitive way.
      In every case,
      Marpa delivers precisely the parses its BNF and its rule
      rankings specify.
      And it
      is &quot;precision&quot; in this sense that a parser writer is looking for.
    &lt;/p&gt;
    &lt;h3&gt;Is there a sensible way to use PEG?&lt;/h3&gt;
    &lt;p&gt;
      I'll return to Marpa at the end of this post.
      For now,
      let's assume
      that you are not interested in using Marpa --
      you are committed to PEG,
      and you want to make the best of PEG.
      Several excellent programmers have focused
      on PEG,
      without blinding themselves to its limitations.
      I've already mentioned one important paper
      by Redziejowski.
      Many of
      &lt;a href=&quot;http://www.romanredz.se/pubs.htm&quot;&gt;
        Redziejowski's collected papers&lt;/a&gt;
      are about PEG,
      and Redziejowski, in his attempts to use PEG,
      does not sugarcoat its problems.
    &lt;/p&gt;
    &lt;p&gt;Roberto Ierusalimschy, author of Lua and one of the best
      programmers of our time,
      has written a PEG-based parser of his own.
      Roberto is fully aware of PEG's limits,
      but he makes a very good case for choosing PEG
      as the basis of LPEG, his parser generator.
      LPEG is intended for use with Lua,
      a ruthlessly minimal language.
      Roberto's minimalist implementation limits the power of his parser,
      but his aim is to extend regular expressions in a disciplined way,
      and a compact parser of limited power is quite acceptable for his
      purposes.
    &lt;/p&gt;
    &lt;h3&gt;Matching the BNF to the PEG spec&lt;/h3&gt;
    &lt;p&gt;
      As Redziejowski and Ierusalimschy and the other authors
      of
      &lt;a href=&quot;http://arxiv.org/abs/1304.3177&quot;&gt;Mascarenhas et al, 2013&lt;/a&gt;
      recognize,
      not knowing what language you are
      parsing is more than an annoyance.
      We can call a language
      &quot;well-behaved for PEG&quot;
      if the PEG spec delivers exactly
      the language the BNF describes.
    &lt;/p&gt;
    &lt;p&gt;Which languages are are well-behaved for PEG?
      According to
      &lt;a href=&quot;http://arxiv.org/abs/1304.3177&quot;&gt;Mascarenhas et al, 2013&lt;/a&gt;,
      the LL(1) languages are well-behaved.
      (The LL(1) languages are the languages
      a top-down parser can parse based on at
      most one character of input.)
      Syntax-driven parsers for LL(1) have been around
      for much longer than PEG --
      one such parser is described
      in the first paper to describe recursive descent
      (&lt;a href=&quot;http://archive.computerhistory.org/resources/text/algol/algol_bulletin/AS16/AS16.HTM&quot;&gt;Peter
        Lucas, 1961&lt;/a&gt;).
      But most practical languages are not LL(1).
      &lt;a href=&quot;http://www.romanredz.se/papers/FI2013.pdf&quot;&gt;Redziejowski 2013&lt;/a&gt;
      and
      &lt;a href=&quot;http://www.romanredz.se/papers/FI2014.pdf&quot;&gt;Redziejowski 2014&lt;/a&gt;
      seek to extend this result by defining the language class LL(1p) --
      those top-down languages with one
      &quot;parsing procedure&quot;
      of lookahead.
      The LL(1p) languages are also well-behaved for PEG.
    &lt;/p&gt;
    &lt;p&gt;
      &lt;a href=&quot;http://arxiv.org/abs/1304.3177&quot;&gt;Mascarenhas et al, 2013&lt;/a&gt;
      also look at a different approach -- instead of writing a PEG specification
      and trying to keep it well-behaved,
      they look at taking languages from larger top-down classes
      and translating them to PEG.
      I don't know of any followup,
      but it's possible this approach could produce
      well-behaved top-down parsers which are
      an improvement over direct-from-PEG parsing.
      But for those who are open to leaving top-down parsing behind,
      a parser which handles languages in all these classes
      and more is already available.
    &lt;/p&gt;
    &lt;h3&gt;Marpa&lt;/h3&gt;
    &lt;p&gt;
      In this post,
      I have adopted the point of view of programmers using PEG,
      or thinking of doing so.
      My own belief in this matter is that
      very few programmers
      should want to bother with the issues I've just described.
      My reason for this is the Marpa parser --
      a general BNF Earley-driven parser that
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;has an implementation you can use today;&lt;/li&gt;
      &lt;li&gt;allows the application to combine syntax-driven parsing
        with custom procedural logic;&lt;/li&gt;
      &lt;li&gt;makes available full, left-eidetic knowledge of the parse to
        the procedural logic;&lt;/li&gt;
      &lt;li&gt;and parses a vast class of grammars in linear time,
        including all the LR-regular grammars.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      The LR-regular grammars include the LR(k) and LL(k)
      grammars for all
      &lt;i&gt;k&lt;/i&gt;.
      LR-regular includes all the languages
      which are well-behaved under PEG,
      and all of those that
      &lt;a href=&quot;http://arxiv.org/abs/1304.3177&quot;&gt;Mascarenhas et al, 2013&lt;/a&gt;
      consider translating into PEG.
    &lt;/p&gt;
    &lt;h3&gt;Comments&lt;/h3&gt;
    &lt;p&gt;Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Parsing: Top-down versus bottom-up</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/ll.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;&lt;p&gt;Comparisons between top-down and bottom-up parsing
      are often either too high-level or too low-level.
      Overly high-level treatments reduce the two approaches to buzzwords,
      and the comparison to a recitation of received wisdom.
      Overly low-level treatments get immersed in the minutiae of implementation,
      and the resulting comparison is as revealing as placing
      two abstractly related code listings side by side.
      In this post I hope to find the middle level;
      to shed light on why advocates of bottom-up
      and top-down parsing approaches take the positions
      they do;
      and to speculate about the way forward.
    &lt;/p&gt;
    &lt;h3&gt;Top-down parsing&lt;/h3&gt;
    &lt;p&gt;The basic idea of top-down parsing is
      as brutally simple as anything in programming:
      Starting at the top, we add pieces.
      We do this by looking at the next token and deciding then and there
      where it fits into the parse tree.
      Once we've looked at every token,
      we have our parse tree.
    &lt;/p&gt;&lt;p&gt;
      In its purest form,
      this idea is too simple for practical parsing,
      so top-down parsing is almost
      always combined with lookahead.
      Lookahead of one token helps a lot.
      Longer lookaheads
      are very sparsely used.
      They just aren't that helpful,
      and since
      the number of possible lookaheads grows exponentially,
      they get very expensive very fast.
    &lt;/p&gt;&lt;p&gt;Top-down parsing has an issue with left recursion.
      It's straightforward to see why.
      Take
      an open-ended expression like
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    a + b + c + d + e + f + [....]&lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      Here the plus signs continue off to the right,
      and adding any of them to the parse tree
      requires a dedicated node which
      must be above the node for the first plus sign.
      We cannot put that first plus sign into a top-down parse
      tree without having first dealt with all those plus signs that follow it.
      For a top-down strategy, this is a big, big problem.
    &lt;/p&gt;&lt;p&gt;
      Even in the simplest expression,
      there is no way of counting the plus signs
      without looking to the right,
      quite possibly a very long way to the right.
      When we are not dealing with simple expressions,
      this rightward-looking needs to get
      sophisticated.
      There are ways of dealing with this difficulty,
      but all of them share one thing in common --
      they are trying to make top-down parsing into
      something that it is not.
    &lt;/p&gt;&lt;h3&gt;Advantages of top-down parsing&lt;/h3&gt;
    &lt;p&gt;Top-down parsing does not look at the right context in any systematic way,
      and in the 1970's it was hard to believe that
      top-down was as good as we can do.
      (It's not all that easy to believe today.)
      But its extreme simplicity
      is also top-down parsing's great strength.
      Because a top-down parser is extremely simple,
      it is very easy to figure out what it is doing.
      And easy to figure out means easy to customize.
    &lt;/p&gt;&lt;p&gt;
      Take another of the many constructs incomprehensible to
      a top-down parser:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    2 * 3 * 4 + 5 * 6
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      How do top-down parsers typically handle this?
      Simple: as soon as they realize they are faced
      with an expression, they give up on top-down
      parsing and switch to a special-purpose algorithm.
    &lt;/p&gt;&lt;p&gt;These two properties -- easy to understand
      and easy to customize --
      have catapulted top-down parsing
      to the top of the heap.
      Behind their different presentations,
      combinator parsing, PEG, and recursive descent are
      all top-down parsers.
    &lt;/p&gt;&lt;h3&gt;Bottom-up parsing&lt;/h3&gt;
    &lt;p&gt;Few theoreticians of the 1970's imagined that top-down parsing might
      be the end of the parsing story.
      Looking to the right in ad hoc ways clearly does help.
      It would be almost paradoxical if
      there was no systematic way to exploit the right context.
    &lt;/p&gt;&lt;p&gt;In 1965, Don Knuth found an algorithm to exploit
      right context.
      Knuth's LR algorithm was,
      like top-down parsing as I have described it,
      deterministic.
      Determinism was thought to be essential --
      allowing more than one choice easily leads to
      a combinatorial explosion in the
      number of possibilities that have to be considered at once.
      When parsers are restricted to dealing with a single choice,
      it is much easier to guarantee that
      they will run in linear time.
    &lt;/p&gt;
    &lt;p&gt;Knuth's algorithm did &lt;b&gt;not&lt;/b&gt;
      try to hang
      each token from a branch of a top-down parse tree
      as soon as it was encountered.
      Instead, Knuth suggested delaying that decision.
      Knuth's algorithm collected
      &quot;subparses&quot;.
      &lt;p&gt;
      When I say &quot;subparses&quot; in this discussion,
      I mean pieces of the parse that 
      contain all the decisions necessary to construct
      the part of the parse tree that is below them.
      But subparses do not contain any decisions about what is above them
      in the parse tree.
      Put another way, subparses know who they are,
      but not where they belong.
    &lt;p&gt;&lt;/p&gt;
      Subparses may not know where they belong,
      but knowing who they are is enough for them
      to be assembled into larger subparses.
      And, if we keep assembling the subparses,
      eventually we will have a &quot;subparse&quot; that
      is the full parse tree.
      And at that point we will
      know both who everyone is 
      and where everyone belongs.
    &lt;/p&gt;&lt;p&gt;
      Knuth's algorithm stored subparses by shifting them onto a stack.
      The operation to do this was called a &quot;shift&quot;.
      (Single tokens of the input are treated as subparses with a single node.)
      When there was enough context to build a larger subparse,
      the algorithm popped one or more subparses off the stack,
      assembled a larger subparse,
      and put the resulting subparse back on the stack.
      This operation was called a &quot;reduce&quot;,
      based on the idea that its repeated application
      eventually &quot;reduces&quot; the parse tree to its root node.
    &lt;/p&gt;&lt;p&gt;
      In handling the stack, we will often be faced with
      choices.
      One kind of choice is between using what we already have
      on top of the stack to assemble a larger subparse;
      or pushing more subparses on top of the stack instead (&quot;shift/reduce&quot;).
      When we decide to reduce,
      we may encounter the other kind of choice --
      we have to decide which rule to use (&quot;reduce/reduce&quot;).
    &lt;/p&gt;
    &lt;p&gt;Like top-down parsing, bottom-up parsing is usually combined with lookahead.
      For the same lookahead, a bottom-up parser parses everything that a
      top-down parser can handle,
      and more.
    &lt;/p&gt;&lt;p&gt;Formally, Knuth's approach is now called shift/reduce parsing.
      I want to demonstrate why theoreticians,
      and for a long time almost everybody else as well,
      was so taken with this method.
      I'll describe how it works on some examples,
      including two very important ones that
      stump top-down parsers: arithmetic expressions and left-recursion.
      My purpose here is bring to light the basic concepts,
      and not to guide an implementor.
      There are excellent implementation-oriented presentations in many other places.
      &lt;a href=http://en.wikipedia.org/wiki/Shift-reduce_parser&gt;The Wikipedia article&lt;/a&gt;,
      for example, is excellent.
    &lt;/p&gt;
    &lt;p&gt;
      Bottom-up parsing solved
      the problem of left recursion.
      In the example from above,
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    a + b + c + d + e + f + [....]&lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      we simply build one subparse after another,
      as rapidly as we can.
      In the terminology of shift/reduce,
      whenever we can reduce, we do.
      Eventually we will have run out of tokens,
      and will have reduced until there is only one element on the stack.
      That one remaining element is the subparse that is also,
      in fact, our full parse tree.
    &lt;/p&gt;&lt;p&gt;
      The top-down parser had a problem with left recursion
      precisely because it needed to build top-down.
      To build top-down, it needed to know about all the plus signs to come,
      because these needed to be fitted into the parse tree above the current plus
      sign.
      But when building bottom-up,
      we don't need to know anything about
      the plus signs that will be above the current one in the parse tree.
      We can afford to wait until we encounter them.
    &lt;/p&gt;
    &lt;p&gt;But if working bottom-up solves the left recursion problem,
      doesn't it create a right recursion problem?
      In fact,
      for a bottom-up parser, right recursion is harder, but not much.
      That's because of the stack.
      For a right recursion like this:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    a = b = c = d = e = f = [....]&lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      we use a strategy opposite to the one we used for the
      left recursion.
      For left recursion, we reduced whenever we could.
      For right recursion, when we have a choice, we always shift.
      This means we will immediately shift the entire input onto the stack.
      Once the entire input is on the stack,
      we have no choice but to start reducing.
      Eventually we will reduce the stack to a single element.
      At that point, we are done.
      Essentially, what we are doing is exactly what we did for left recursion,
      except that we use the stack to reverse the order.
    &lt;/p&gt;&lt;p&gt;
      Arithmetic expressions like
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
    2 * 3 * 4 + 5 * 6&lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      require a mixed strategy.
      Whenever we have a shift/reduce choice,
      and one of the operators is on the stack,
      we check to see if the topmost operator is a multiply or an addition operator.
      If it is a multiply operator, we reduce.
      In all other cases, if there is a shift/reduce choice, we shift.
    &lt;/p&gt;
    &lt;p&gt;
      In the discussion above,
      I have pulled the strategy for making stack decisions
      (shift/reduce and reduce/reduce)
      out of thin air.
      Clearly, if bottom-up parsing was going to be
      a practical parsing algorithm,
      the stack decisions
      would have to be
      made algorithmically.
      In fact, discovering a practical way to do this
      was a far from trivial task.
      The solution in Knuth's paper was considered (and apparently intended)
      to be mathematically provocative, rather than practical.
      But by 1979, it was thought a practical way to make stack decisions
      had been found
      and yacc, a parser generator based on bottom-up parsing, was released.
      (Readers today may be more familiar with yacc's successor, bison.)
    &lt;/p&gt;
    &lt;h3&gt;The fate of bottom-up parsing&lt;/h3&gt;
    &lt;p&gt;
      With yacc, it looked as if the limitations of top-down parsing were past us.
      We now had a parsing algorithm that could readily and directly
      parse left and right recursions, as well as arithmetic expressions.
      Theoreticians thought they'd found the Holy Grail.
    &lt;/p&gt;&lt;p&gt;But not all of the medieval romances had happy endings.
      And as I've
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html&quot;&gt;described
        elsewhere&lt;/a&gt;,
      this story ended badly.
      Bottom-up parsing was driven by tables which made the algorithm fast
      for correct inputs, but unable to accurately diagnose faulty ones.
      The subset of grammars parsed was still not quite large enough,
      even for conservative language designers.
      And bottom-up parsing was very unfriendly to custom hacks,
      which made every shortcoming loom large.
      It is much harder to work around a problem in a bottom-up
      parser than than it is to deal with a similar shortcoming
      in a top-down parser.
      After decades of experience with bottom-up parsing,
      top-down parsing has re-emerged as the
      algorithm of choice.
    &lt;/p&gt;
    &lt;h3&gt;Non-determinism&lt;/h3&gt;
    &lt;p&gt;For many, the return to top-down parsing
      answers the question that we posed earlier:
      &quot;Is there any systematic way to exploit right context when parsing?&quot;
      So far, the answer seems to be a rather startling &quot;No&quot;.
      Can this really be the end of the story?
    &lt;/p&gt;&lt;p&gt;
      It would be very strange if the best basic parsing algorithm we know is top-down.
      Above, I described at some length some very important grammars that
      can be parsed bottom-up
      but not top-down, at least not directly.
      Progress like this seems like a lot to walk away from,
      and especially to walk back all the way to what is
      essentially a brute force algorithm.
      This perhaps explains why lectures
      and textbooks persist in teaching bottom-up parsing to
      students who are very unlikely to use it.
      Because the verdict from practitioners seems to be in,
      and likely to hold up on appeal.
    &lt;/p&gt;
    &lt;p&gt;Fans of deterministic top-down parsing,
      and proponents of deterministic bottom-up parsing share
      an assumption:
      For a practical algorithm to be linear,
      it has to be deterministic.
      But is this actually the case?
    &lt;/p&gt;
    &lt;p&gt;It's not, in fact.
      To keep bottom-up parsing deterministic, we restricted ourselves to a stack.
      But what if we track all possible subpieces of parses?
      For efficiency, we can link them and put them into tables,
      making the final decisions in a second pass,
      once the tables are complete.
      (The second pass replaces the stack-driven
      see-sawing back and forth of the deterministic bottom-up algorithm,
      so it's not an inefficiency.)
      Jay Earley in 1968 came up with an algorithm to do this,
      and in 1991 Joop Leo added a memoization to Earley's
      algorithm which made it linear for all deterministic grammars.
    &lt;/p&gt;&lt;p&gt;The &quot;deterministic grammars&quot;
      are exactly the bottom-up parseable grammars
      with lookahead -- the set of grammars parsed by Knuth's algorithm.
      So that means the Earley/Leo algorithm parses,
      in linear time,
      everything that a deterministic bottom-up parser can parse,
      and therefore every grammar that
      a deterministic top-down parser can parse.
      (In fact, the Earley/Leo algorithm is linear for a lot of
      ambiguous grammars as well.)
    &lt;/p&gt;&lt;p&gt;Top-down parsing had the advantage that it was easy to know where
      you are.  The Earley/Leo algorithm has an equivalent advantage -- its
      tables know where it is, and it is easy to query them programmatically.
      In 2010, this blogger modified the Earley/Leo algorithm
      to have the other big advantage of top-down parsing:
      The Marpa algorithm rearranges the Earley/Leo parse engine so that we can
      stop it, perform our own logic, and restart where we left off.
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;A quite useable parser based on the Marpa algorithm&lt;/a&gt;
      is available as open source.
    &lt;/p&gt;&lt;h3&gt;Comments&lt;/h3&gt;
    &lt;p&gt;Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  </channel>
</rss>
