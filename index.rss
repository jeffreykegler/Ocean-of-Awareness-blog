<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>A new way to look at parsing</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/pingali.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body style=&quot;max-width:850px&quot;&gt;
    &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;h2&gt;Derivatives == Earley?&lt;/h2&gt;
    &lt;p&gt;In a very cordial Twitter exchange with Matt Might and
    and David Darais, Prof. Might asked why I was not looking at his
    derivatives-based approach.
    I answered that I already was --
    I see
    Marpa as an optimized version of the Might/Darais approach.
    &lt;/p&gt;
    &lt;p&gt;This may sound strange.
    At first glance, the two algorithms seem about as different as
    two algorithms that address the same problem space can
    get.
    My Marpa parser is an Earley parser, table-driven
    and its parse engine is written in C language.
    The MDS (Might/Darais/Spiewak) parser is
    an extension of regular expressions
    which constructs states on the fly.
    MDS is uses combinators and is implemented via
    several functional programming languages.
    &lt;/p&gt;
    &lt;h2&gt;Grammar Flow Graphs&lt;/h2&gt;
    &lt;p&gt;How then could imagine that Marpa is another version of the MDS
    approach?
    The key was a paper sent
    me by Prof. Keshav Pingali at Austin: &quot;Parsing with Pictures&quot;.
    The title is a little misleading:
    their approach is not &lt;b&gt;that&lt;/b&gt; easy,
    and the paper does require some math.
    But it is a lot easier than the traditional way
    of learning the various approaches to parsing.
    Pingali and Bilardi suggest that their approach,
    taken out of the terse form it has in their
    papers,
    can make it a easier to teach
    and understand Parsing Theory.
    I think that may be right.
    &lt;/p&gt;
    &lt;p&gt;The basis of the Pingali-Bilardi approach
    is Grammar Flow Graphs.
    These (the &quot;pictures&quot; of their title)
    are basically state diagrams
    (regular expressions in diagram form)
    with recursion added.
    As has long been known,
    adding recursion to a state diagram
    allows it to represent any context-free languages.
    &lt;/p&gt;
    &lt;p&gt;What's new with Pingali and Bilardi is
    where they take their GFG's.
    A GFG can be traversed using the same algorithm
    as an NFA, which they do.
    At first glance the result is not impressive --
    the algorithm is a recognizer, not a parser,
    and it's a bad recognizer:
    It is over-liberal and recognizes string which
    are not in the context-free language.
    &lt;/p&gt;
    &lt;p&gt;Next comes their real insight:
    They enhance their NFA-based algorithm
    to have it track where each recursion begins.
    (This can be done by adding an integer to each item&lt;a id=&quot;footnote-1-ref&quot; href=&quot;#footnote-1&quot;&gt;[1]&lt;/a&gt;.)
    This fix not only corrects the recognizer
    -- it turns it into a parser.
    &lt;/p&gt;
    &lt;p&gt;
    It turns out that Pingali and Bilardi's fixed NFA algorithm
    is not new.
    In fact, it is &lt;b&gt;exactly&lt;/b&gt; Earley's algorithm.
    &lt;/p&gt;
    &lt;p&gt;Pingali and Bilardi do not stop there.
    Using their new framework,
    they go on to show that all LL-based and LR-based algorithms
    are simplifications of their Earley parser.
    From this point of view,
    Earley parsing is the foundation of all context-free parsing,
    and LL- and LR-based algorithms are Earley optimizations.
    &lt;h2&gt;Showing the equivalence: step 1&lt;/h2&gt;
    &lt;h2&gt;The MDS algorithm&lt;/h2&gt;
    &lt;p&gt;
    To show that Marpa is an optimization of the MDS approach,
    I will start with the MDS algorithm, and attempt to optimize it.
    For its functional programming language,
    the MDS paper uses Racket.
    The MDS parser is described directly,
    in the usual functional language manner,
    as a matching
    operation.
    In the MDS paper,
    the MDS parser is optimized with laziness and memoization.
    Nulls are dealt with by computing their fixed points on the fly.
    Even with these 3 optimizations,
    the result is still highly inefficient.
    As a last step, MDS also
    implements &quot;deep recursive simplication&quot; -- in effect,
    strategically replacing laziness with eagerness.
    With this the MDS paper conjectures that the algorithm's time
    is linear for a large class of practical grammars.
    &lt;/p&gt;
    &lt;h2&gt;Step 2: Extended regular expressions&lt;/h2&gt;
    &lt;p&gt;
    Step 1 in going from the MDS algorithm to Leo/Earley/Marpa via
    the Pingali and Bilardi &quot;pictures&quot; is
    to notice both MDS and GFG's start at the same place:
    context-free grammars in the form of
    regular expressions extended to allow recursion.
    MDS starts with a match expression in Racket,
    and its Racket match expressions have a natural equivalent in a GFG --
    so natural you could imagine the MDS paper using GFG's
    as illustrations.
    &lt;/p&gt;
    &lt;h2&gt;Step 2&gt;: Following an NFA&lt;/h2&gt;
    &lt;p&gt;
    The MDS and GFG approaches are also similar in their next step:
    Both follow an NFA by consuming a single character to produce a
    partial parse.
    For both, a partial parse is a duple consisting of a set of parses
    and a string.
    The string is the remaining input,
    and the set of parses is the parses so far.
    &lt;/p&gt;
    &lt;h2&gt;Comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
    &lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p id=&quot;footnote-1&quot;&gt;1.
    Term?
 &lt;a href=&quot;#footnote-1-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Why is parsing considered solved?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body style=&quot;max-width:850px&quot;&gt;
    &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;p&gt;It is often said that parsing is a &quot;solved problem&quot;.
      Given the level of frustration with the state of the art,
      the underuse of the very powerful technique of
      Language-Oriented Programming due to problematic tools&lt;a id=&quot;footnote-1-ref&quot; href=&quot;#footnote-1&quot;&gt;[1]&lt;/a&gt;,
      and the vast superiority of human parsing ability
      over computers,
      this requires explanation.
    &lt;/p&gt;
    &lt;p&gt;
      On what grounds would someone say that parsing is &quot;solved&quot;?
      To understand this,
      we need to look at the history of Parsing Theory.&lt;a id=&quot;footnote-2-ref&quot; href=&quot;#footnote-2&quot;&gt;[2]&lt;/a&gt;
      In fact, we'll have to start decades before computer Parsing Theory
      exists,
      with a now nearly-extinct school of linguistics,
      and its desire to put the field on strictly
      scientific basis.
    &lt;/p&gt;
    &lt;h2&gt;1929: Bloomfield redefines &quot;language&quot;&lt;/h2&gt;
    &lt;p&gt;In 1929 Leonard Bloomfield,
      as part of his effort to create a linguistics that
      would be taken seriously as a science,
      published his &quot;Postulates&quot;.&lt;a id=&quot;footnote-3-ref&quot; href=&quot;#footnote-3&quot;&gt;[3]&lt;/a&gt;
      The &quot;Postulates&quot; include his definition of language:
    &lt;/p&gt;&lt;blockquote&gt;
      The totality of utterances that can be made in a speech
      community is the
      &lt;b&gt;language&lt;/b&gt;
      of that speech-community.&lt;a id=&quot;footnote-4-ref&quot; href=&quot;#footnote-4&quot;&gt;[4]&lt;/a&gt;
    &lt;/blockquote&gt;&lt;p&gt;
      There is no reference in this definition to the usual view,
      that the utterances of a language &quot;mean&quot; something.
      This omission is not accidental:
    &lt;/p&gt;&lt;blockquote&gt;
      The statement of meanings is therefore the weak point in
      language-study, and will remain so until human knowledge
      advances very far beyond its present state. In practice, we define the
      meaning of a linguistic form, wherever we can, in terms of some
      other science.&lt;a id=&quot;footnote-5-ref&quot; href=&quot;#footnote-5&quot;&gt;[5]&lt;/a&gt;
    &lt;/blockquote&gt;&lt;p&gt;
      Bloomfield is passing the buck,
      because the behaviorist science of his time rejects
      any claims about mental states as
      unverifiable statements -- essentially,
      as claims to be able to read minds.
      &quot;Hard&quot; sciences like physics, chemistry and even
      biology avoid dealing with unverifiable mental states.
      Bloomfield and the behaviorists want to make the methods of linguistics
      as close to hard science as possible.
    &lt;/p&gt;
    &lt;p&gt;
      Draconian as Bloomfield's exclusion of meaning is,
      it is a big success.
      Known as structural linguistics,
      Bloomfield's approach dominates lingustics for
      the next couple of decades.
    &lt;/p&gt;
    &lt;h2&gt;1955: Noam Chomsky graduates&lt;/h2&gt;
    &lt;p&gt;
      Noam Chomsky earns his PhD at the University of Pennsylvania.
      His teacher, Zelig Harris, is a prominent Bloomfieldian,
      and Chomsky's early work is thought to be in the Bloomfield school.&lt;a id=&quot;footnote-6-ref&quot; href=&quot;#footnote-6&quot;&gt;[6]&lt;/a&gt;
      Chomsky becomes a professor at MIT.
      MIT does not have a linguistics department,
      and Chomsky is free to teach his own approach to the subject.
    &lt;/p&gt;
    &lt;h2&gt;The term &quot;language&quot; as of 1956&lt;/h2&gt;
    &lt;p&gt;Chomsky publishes his &quot;Three models&quot; paper,
      one of the most important papers of all time.
      His definition of language uses the terminology
      of set theory:
    &lt;/p&gt;&lt;blockquote&gt;
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of symbols.&lt;a id=&quot;footnote-7-ref&quot; href=&quot;#footnote-7&quot;&gt;[7]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      This definition is pure Bloomfield in substance,
      but signs of departure from the behaviorist orthodoxy are
      apparent in &quot;Three Models&quot; --
      Chomsky is quite willing to talk about what sentences mean,
      when it serves his purposes.
      For a utterance with multiple meanings,
      Chomsky's new model produces multiple syntactic derivations.
      Each of these syntactic derivations
      &quot;looks&quot; like the natural representation
      of one of the meanings.
      Chomsky points out that the insight into semantics
      that his new model provides is a very
      desirable property to have.&lt;a id=&quot;footnote-8-ref&quot; href=&quot;#footnote-8&quot;&gt;[8]&lt;/a&gt;
    &lt;/p&gt;
    &lt;h2&gt;1959: Chomsky reviews Skinner&lt;/h2&gt;
    &lt;p&gt;In 1959, Chomsky reviews a book by B.F. Skinner's on linguistics.&lt;a id=&quot;footnote-9-ref&quot; href=&quot;#footnote-9&quot;&gt;[9]&lt;/a&gt;
      Skinner is the most prominent behaviorist of the time.
    &lt;/p&gt;
    &lt;p&gt;
      Chomsky's review removes all doubt about where he stands
      on behaviorism
      or on the relevance of linguistics to the study of meaning.&lt;a id=&quot;footnote-10-ref&quot; href=&quot;#footnote-10&quot;&gt;[10]&lt;/a&gt;
      His review galvanizes the opposition to behaviorism, and
      Chomsky establishes himself as behavorism's most
      prominent and effective critic.
    &lt;/p&gt;
    &lt;p&gt;
      In later years,
      Chomsky will make it clear that he had had no intention
      of avoiding semantics:
    &lt;/p&gt;&lt;blockquote&gt;
      [...] it would be absurd to develop
      a general syntactic theory
      without assigning an absolutely
      crucial role to semantic considerations,
      since obviously the necessity to support
      semantic interpretation is one of the primary
      requirements
      that the structures
      generated by the syntactic component of a grammar
      must meet.&lt;a id=&quot;footnote-11-ref&quot; href=&quot;#footnote-11&quot;&gt;[11]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;h2&gt;1961: Oettinger discovers pushdown automata&lt;/h2&gt;
    &lt;p&gt;
      While the stack itself goes back to Turing&lt;a id=&quot;footnote-12-ref&quot; href=&quot;#footnote-12&quot;&gt;[12]&lt;/a&gt;,
      its significance for parsing becomes an object
      of interest in itself with
      Samuelson and Bauer's 1959 paper&lt;a id=&quot;footnote-13-ref&quot; href=&quot;#footnote-13&quot;&gt;[13]&lt;/a&gt;.
      Mathematical study of stacks as models of computing begins with Anthony Oettinger's 1961 paper.&lt;a id=&quot;footnote-14-ref&quot; href=&quot;#footnote-14&quot;&gt;[14]&lt;/a&gt;&lt;/p&gt;
    &lt;p&gt;Oettinger 1961 is full of evidence that stacks
      (which he calls &quot;pushdown stores&quot;) are still very new.
      For example,
      Oettinger does not use the terms &quot;push&quot; or &quot;pop&quot;,
      but instead describes operations on his pushdown stores using
      a set of vector operations which will later form the basis
      of the APL language.
    &lt;/p&gt;
    &lt;p&gt;Oettinger defines 4 languages.
      Oettinger's definitions all follow the behavorist model --
      they are sets of strings.&lt;a id=&quot;footnote-15-ref&quot; href=&quot;#footnote-15&quot;&gt;[15]&lt;/a&gt;
      Oettinger's pushdown stores
      will eventually be called
      deterministic pushdown automata (DPDA's) and
      become the basis of a model of language
      and the subject of a substantial literature,
      all of which will use the behaviorist definition
      of &quot;language&quot;.
    &lt;/p&gt;
    &lt;p&gt;
      Oettinger hopes that DPDA's
      will be an adequate basis for
      the study of both computer and
      natural language translation.
      (Oettinger's own field is Russian translation.)
      DPDA's soon prove totally inadequate
      for natural languages.
    &lt;/p&gt;
    &lt;p&gt;
      But for dealing with computing languages,
      DPDA's will have a much longer life.
      As of 1961, all algorithms with acceptable speed are using
      stacks with various modifications.
    &lt;/p&gt;
    &lt;blockquote&gt;
      The development of a theory of pushdown algorithms should
      hopefully lead to systematic techniques for generating
      algorithms satisfying given requirements to replace
      the ad hoc invention of each new algorithm.&lt;a id=&quot;footnote-16-ref&quot; href=&quot;#footnote-16&quot;&gt;[16]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      The search for a comprehensive theory of
      stack-based parsing
      quickly becomes identified
      with the search for a theoretical basis for practical parsing.
    &lt;/p&gt;
    &lt;h2&gt;1965: Knuth discovers LR(k)&lt;/h2&gt;
    &lt;p&gt;Donald Knuth
      reports his new results on stack-based parsing.
      In a pivotal paper&lt;a id=&quot;footnote-17-ref&quot; href=&quot;#footnote-17&quot;&gt;[17]&lt;/a&gt;,
      Knuth sets out a theory that
      encompasses all the &quot;tricks&quot;&lt;a id=&quot;footnote-18-ref&quot; href=&quot;#footnote-18&quot;&gt;[18]&lt;/a&gt;
      used for efficient parsing up to that time.
      With this Oettinger's hope for a theory
      to replace &quot;ad hoc invention&quot; is fulfilled.
      In an exhilarating (and exhausting) 39-page
      demonstration of mathematical virtuousity,
      Knuth shows that stack-based parsing is
      equivalent to a new and unexpected class of grammars.
      Knuth calls these LR(k), and provides a parsing algorithm for them.
    &lt;/p&gt;
    &lt;p&gt;
      Knuth's new algorithm might be expected to be &quot;the one to rule
      them all&quot;.
      Unfortunately, while deterministic and linear,
      it is not practical -- it requires huge tables well beyond
      the memory capabilities of the time.
    &lt;/p&gt;
    &lt;p&gt;
      The impracticality of his LR(k) algorithm
      does not suggest to Knuth that the stack-based model
      is inappropriate as a model of practical parsing.
      Instead it suggests to him, and to the field,
      that the boundary of practical parsing lies in a subclass of the
      LR(k) grammars.
    &lt;/p&gt;
    &lt;p&gt;
      To be sure,
      Knuth, in his program for further research&lt;a id=&quot;footnote-19-ref&quot; href=&quot;#footnote-19&quot;&gt;[19]&lt;/a&gt;,
      does suggests investigation of parsers for superclasses
      of LR(k).
      He even describes a new superclass of his own:
      LR(k,t), which is LR(k) with more aggressive lookahead.
      But he is clearly unenthusiastic about LR(k,t).&lt;a id=&quot;footnote-20-ref&quot; href=&quot;#footnote-20&quot;&gt;[20]&lt;/a&gt;
      It is reasonable to suppose
      that Knuth is even more negative about
      the more general approaches that
      he does not bother to mention.&lt;a id=&quot;footnote-21-ref&quot; href=&quot;#footnote-21&quot;&gt;[21]&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      In any case, those reading Knuth's LR(k) paper focused almost
      exclusively on his suggestions for research within the stack-based
      model.
      These included grammar rewrites;
      streamlining of the LR(k) tables;
      and research into LR(k) subclasses.
      It is LR(k) subclassing that will receive the most attention.&lt;a id=&quot;footnote-22-ref&quot; href=&quot;#footnote-22&quot;&gt;[22]&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      The idea that the solution to the parsing problem must be
      stack-based is not without foundation.
      In 1965, the limits of computer technology are severe.
      For practitioners,
      any parsing technique that required much more
      than a reasonably-sized state
      machine and a stack,
      is not likely to happen.
      After all,
      only four years earlier,
      stacks were bleeding edge.
    &lt;/p&gt;
    &lt;p&gt;The practitioners of 1965
      are inclined to believe that,
      like it or not,
      they are stuck with stack-based parsing.
      But why do the theoreticians feel compelled to follow them?
      The answer is that theoreticians talk themselves into
      it, using a misleading equivalence based
      on the behaviorist definition of language.
    &lt;/p&gt;
    &lt;h2&gt;&quot;Language&quot; as of 1965&lt;/h2&gt;
    &lt;p&gt;
      Knuth defines language as follows:
    &lt;/p&gt;
    &lt;blockquote&gt;
      The language defined by G is&lt;br&gt;
      &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
      { &amp;alpha; | S =&gt; &amp;alpha; and &amp;alpha; is a string over T }&lt;br&gt;
      namely, the set of all terminal strings derivable from S by using
      the productions of G as substitution rules.&lt;a id=&quot;footnote-23-ref&quot; href=&quot;#footnote-23&quot;&gt;[23]&lt;/a&gt;
    &lt;/blockquote&gt;&lt;p&gt;
      Here G is a grammar whose start symbol is S and whose set
      of terminals is T.
      This is the behavorist definition of language
      translated into set-theoretic terms.
    &lt;/p&gt;
    &lt;p&gt;Knuth proves, to the satisfaction of the profession,
      the &quot;equivalence&quot; of LR(k) and DPDA's.
      LR(k) is a class of grammars and the DPDA model is of
      languages -- sets of strings.
      At first glance, this is an &quot;apples and oranges&quot; comparison --
      how do you prove the equivalence of a class of languages
      and a class of grammars?
    &lt;/p&gt;
    &lt;p&gt;
      Knuth does this by reducing the class of DPDA languages and the class
      of grammars to their lowest common denominator, which is the language.
      And, of course, the &quot;language&quot; in the usage of Parsing Theory
      is a set of strings, without consideration of their syntax.
    &lt;/p&gt;
    &lt;p&gt;
      Every grammar, when stripped of its syntax, defines a language.
      So Knuth compares the language which results from stripping down
      the LR(k) grammars,
      to the language of DPDA's.
      After some very impressive mathematics,
      Knuth is able to show that the two languages are equivalent.
    &lt;/p&gt;
    &lt;p&gt;
      In theoretical mathematics, of course,
      you can define &quot;equivalent&quot; however you like.
      But if the purpose is to suggest limits in practice,
      you have to be much more careful.
      And in fact, as Knuth's paper shows,
      if you equate languages and grammars,
      you get into a very serious degree of magical thinking.
      Using the Knuth algorithm,
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;parsing LR(k) grammars for arbitrary k is hopelessly impractical;
      &lt;/li&gt;
      &lt;li&gt;parsing LR(1) grammars is impractical, but close to the boundary&lt;a id=&quot;footnote-24-ref&quot; href=&quot;#footnote-24&quot;&gt;[24]&lt;/a&gt;;
        and
      &lt;/li&gt;
      &lt;li&gt;parsing LR(0) grammars is very practical.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;A problem for the relevance
      of Knuth's proof of equivalence is that,
      if you just look at sets of strings
      without regard to syntax,
      LR(1) and LR(k) are equivalent.
      That means that from the sets-of-strings point of view,
      hopelessly impractical and
      borderline impractical are the same thing.
    &lt;/p&gt;
    &lt;p&gt;
      Worse, both LR(1) and LR(k) are equivalent to LR(0)
      for most applications.
      If you add
      an explicit end marker to an LR(1) language,
      which in most applications is easy to do&lt;a id=&quot;footnote-25-ref&quot; href=&quot;#footnote-25&quot;&gt;[25]&lt;/a&gt;,
      your LR(1) language becomes LR(0).
      Therefore, for most applications,
    &lt;/p&gt;
    &lt;center&gt;
      LR(k) = LR(1) = LR(0)
    &lt;/center&gt;
    &lt;p&gt;
      This means that, in the world of sets-of-strings,
      extremely impractical and very practical are usually the same thing.
    &lt;/p&gt;
    &lt;p&gt;
      Clearly the world of sets of strings
      is a magical one,
      in which we can easily transport ourselves across the
      boundary between practical and impractical.
      We can take visions of a magical world back into the world of practice,
      but we cannot assume they will be helpful.
      In that light,
      it is no surprise that
      Joop Leo will show how to extend practical
      parsing well beyond LR(k).&lt;a id=&quot;footnote-26-ref&quot; href=&quot;#footnote-26&quot;&gt;[26]&lt;/a&gt;
    &lt;/p&gt;
    &lt;h2&gt;Comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      I encourage
      those who want to know more about the story of Parsing Theory
      to look at my
      &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
        Parsing: a timeline 3.0&lt;/a&gt;.
      In particular,
      &quot;Timeline 3.0&quot; tells the story of the search for a good
      LR(k) subclass,
      and what happened afterwards.
    &lt;/p&gt;
    &lt;p&gt;
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
    &lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p id=&quot;footnote-1&quot;&gt;1.
        The well-known
        &lt;a href=&quot;https://en.wikipedia.org/wiki/Design_Patterns&quot;&gt;
          &lt;cite&gt;Design Patterns&lt;/cite&gt;
          book&lt;/a&gt;
        (aka &quot;the Gang of 4 book&quot;)
        has a section on this.
        The Gang of 4
        call Language-Oriented Programming
        their &quot;Interpreter pattern&quot;.
        That section amply illustrates the main obstacle to use
        of the pattern -- lack of adequate parsing tools.
        I talk more about this in my two blog posts on
        the Interpreter pattern:
        &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/bnf_to_ast.html&quot;&gt;
          BNF to AST&lt;/a&gt;
        and
        &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/interpreter.html&quot;&gt;
          The Interpreter Design Pattern&lt;/a&gt;.
 &lt;a href=&quot;#footnote-1-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-2&quot;&gt;2.
        This post takes the form of a timeline, and
        is intended to be incorporated in my
        &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
          Parsing: a timeline&lt;/a&gt;.
        The earlier entires in this post borrow heavily from
        &lt;a href=&quot;http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html&quot;&gt;
          a previous blog post&lt;/a&gt;.
 &lt;a href=&quot;#footnote-2-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-3&quot;&gt;3.
        Bloomfield, Leonard,
        &quot;A set of Postulates
        for the Science of Language&quot;,
        &lt;cite&gt;Language&lt;/cite&gt;, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
 &lt;a href=&quot;#footnote-3-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-4&quot;&gt;4.
        Bloomfield 1926, definition 4 on p. 154.
 &lt;a href=&quot;#footnote-4-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-5&quot;&gt;5.
        Bloomfield, Leonard.
        &lt;cite&gt;Language&lt;/cite&gt;.
        Holt, Rinehart and Winston, 1933, p. 140.
 &lt;a href=&quot;#footnote-5-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-6&quot;&gt;6.
        Harris, Randy Allen,
        &lt;cite&gt;The Linguistics Wars&lt;/cite&gt;,
        Oxford University Press, 1993,
        pp 31-34, p. 37.
 &lt;a href=&quot;#footnote-6-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-7&quot;&gt;7.
        The quote is on p. 114 of
        Chomsky, Noam.
        &quot;Three models for the description of language.&quot;
        &lt;cite&gt;IRE Transactions on information theory&lt;/cite&gt;,
        vol. 2, issue 3, September 1956, pp. 113-124.
        In case there is any doubt Chomsky's &quot;strings&quot;
        are Bloomfield's utterances,
        Chomsky also calls his strings,
        &quot;utterances&quot;.
        For example in Chomsky, Noam,
        &lt;cite&gt;Syntactic Structures&lt;/cite&gt;,
        2nd ed.,
        Mouton de Gruyter, 2002, on p. 15:
        &quot;Any grammar of a language will project the finite and somewhat accidental
        corpus of observed utterances to a set (presumably infinite)
        of grammatical utterances.&quot;
 &lt;a href=&quot;#footnote-7-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-8&quot;&gt;8.
        Chomsky 1956, p. 118, p. 123.
 &lt;a href=&quot;#footnote-8-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-9&quot;&gt;9.
        Chomsky, Noam.
        “A Review of B. F. Skinner’s Verbal Behavior”.
        &lt;cite&gt;Language&lt;/cite&gt;,
        Volume 35, No. 1, 1959, 26-58.
        &lt;a href=&quot;https://chomsky.info/1967____/&quot;&gt;
          https://chomsky.info/1967____/&lt;/a&gt;
        accessed on 3 June 2018.
 &lt;a href=&quot;#footnote-9-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-10&quot;&gt;10.
        See in particular, Section IX of Chomsky 1959.
 &lt;a href=&quot;#footnote-10-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-11&quot;&gt;11.
        Chomsky, Noam.
        &lt;cite&gt;Topics in the Theory of Generative Grammar&lt;/cite&gt;.
        De Gruyter, 1978, p. 20.
        (The quote occurs in footnote 7 starting on p. 19.)
 &lt;a href=&quot;#footnote-11-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-12&quot;&gt;12.
        Carpenter, Brian E., and Robert W. Doran.
        &quot;The other Turing machine.&quot;
        &lt;cite&gt;The Computer Journal&lt;/cite&gt;, vol. 20, issue 3, 1 January 1977, pp. 269-279.
 &lt;a href=&quot;#footnote-12-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-13&quot;&gt;13.
        Samelson, Klaus, and Friedrich L. Bauer. &quot;Sequentielle formelübersetzung.&quot; it-Information Technology 1.1-4 (1959): 176-182.
 &lt;a href=&quot;#footnote-13-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-14&quot;&gt;14.
        Oettinger, Anthony.
        &quot;Automatic Syntactic Analysis and the Pushdown Store&quot;
        &lt;cite&gt;Proceedings of Symposia in Applied Mathematics&lt;/cite&gt;,
        Volume 12,
        American Mathematical Society, 1961.
 &lt;a href=&quot;#footnote-14-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-15&quot;&gt;15.
        Oettinger 1961, p. 106.
 &lt;a href=&quot;#footnote-15-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-16&quot;&gt;16.
        Oettinger 1961, p. 127.
 &lt;a href=&quot;#footnote-16-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-17&quot;&gt;17.
        Knuth, Donald E.
        &quot;On the translation of languages from left to right.&quot;
        &lt;cite&gt;Information and Control&lt;/cite&gt;, Volume 8, Issue 6, December 1965, pp. 607-639.
        &lt;a href=&quot;https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&amp;acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d&quot;&gt;
          https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&amp;acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d&lt;/a&gt;, accessed 24 April 2018.
 &lt;a href=&quot;#footnote-17-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-18&quot;&gt;18.
        Knuth 1965, p. 607, in the abstract.
 &lt;a href=&quot;#footnote-18-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-19&quot;&gt;19.
        Knuth 1961, pp. 637-639.
 &lt;a href=&quot;#footnote-19-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-20&quot;&gt;20.
        &quot;Finally, we might mention another generalization of LR(k)&quot;
        (Knuth 1965, p. 638); and
        &quot;One might choose to call this left-to-right translation,
        although we had to back up a finite amount.&quot;
        (p. 639).
 &lt;a href=&quot;#footnote-20-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-21&quot;&gt;21.
        Knuth's skepticism of more general Chomskyan approaches
        is suggested by his own plans for his (not yet released) Chapter
        12 of the
        &lt;cite&gt;Art of Computer Programming&lt;/cite&gt;,
        in which he planned to use pre-Chomskyan bottom-up methods. (See
        Knuth, Donald E., &quot;The genesis of attribute grammars&quot;,
        &lt;cite&gt;Attribute Grammars and Their Applications&lt;/cite&gt;,
        Springer, September 1990, p. 3.)
 &lt;a href=&quot;#footnote-21-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-22&quot;&gt;22.
        The story of the research followup to Knuth's LR(k) paper is told
        in my
        &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
          Parsing: a timeline 3.0&lt;/a&gt;.
 &lt;a href=&quot;#footnote-22-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-23&quot;&gt;23.
        Knuth 1965, p. 608.
 &lt;a href=&quot;#footnote-23-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-24&quot;&gt;24.
          Given the capacity of computer memories in 1965,
          LR(1) was clearly impractical.
          With the huge computer memories of 2018,
          that could be reconsidered, but LR(1) is still restrictive
          and has poor error-handling,
          and few have looked at the possibility.
 &lt;a href=&quot;#footnote-24-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-25&quot;&gt;25.
        Some parsing applications, such as those which receive their input &quot;on-line&quot;,
        can not determine the size of their input in advance.
        For these applications adding an end marker to their input is
        inconvenient or impossible.
 &lt;a href=&quot;#footnote-25-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-26&quot;&gt;26.
        Joop M. I. M.
        &quot;A general context-free parsing algorithm running in linear time on every LR (k) grammar without using lookahead.&quot;
        &lt;cite&gt;Theoretical computer science&lt;/cite&gt;, Volume 82, Issue 1, 22 May 1991, pp. 165-176.
        &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/030439759190180A&quot;&gt;
          https://www.sciencedirect.com/science/article/pii/030439759190180A&lt;/a&gt;, accessed 24 April 2018.
 &lt;a href=&quot;#footnote-26-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Is a language just a set of strings?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;blockquote&gt;
	But to my mind, though I am native here&lt;br&gt;
	And to the manner born, it is a custom&lt;br&gt;
	More honor’d in the breach than the observance.&lt;a id=&quot;footnote-1-ref&quot; href=&quot;#footnote-1&quot;&gt;[1]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;h2&gt;Chomsky's &quot;Three Models&quot; paper&lt;/h2&gt;
    &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      Important papers produce important mistakes.
      A paper can contain a great many errors,
      and they will have no effect if the paper is ignored.
      On the other hand,
      even the good methods of
      a great paper can go badly wrong
      when its methods
      outlive the reasons for using them.
    &lt;/p&gt;
    &lt;p&gt;
      Chomsky's &quot;Three Models&quot; paper&lt;a id=&quot;footnote-2-ref&quot; href=&quot;#footnote-2&quot;&gt;[2]&lt;/a&gt;
      is about as influential
      as a paper can get.
      Just 12 pages,
      it's the paper in which the most-cited scholar of our
      time first outlined his ideas.
      Even at the time,
      linguists described its effect on their field as
      &quot;Copernician&quot;.&lt;a id=&quot;footnote-3-ref&quot; href=&quot;#footnote-3&quot;&gt;[3]&lt;/a&gt;
      Bringing new rigor into what had been seen as a &quot;soft&quot;
      science, it turned lots of heads outside linguistics.
      It belongs on anyone's list of the most important scientific papers ever.&lt;a id=&quot;footnote-4-ref&quot; href=&quot;#footnote-4&quot;&gt;[4]&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      Given its significance,
      it is almost incidental that
      &quot;Three models&quot; is also the foundation paper of computer Parsing Theory,
      the subject of these blog posts.
      Chomsky does not consider himself a computer scientist
      and, after founding our field,
      has paid little attention to it.
      But in fact,
      the Chomskyan model has been even more dominant
      in computer parsing than in Chomsky's own
      field of linguistics.
    &lt;/p&gt;
    &lt;p&gt;
      &quot;Three Models&quot; places Chomksy among the great mathematicians
      of all time.
      True, the elegance and rigor of Chomsky's proofs
      better befit a slumming linguist
      than they would a professional mathematician.
      But at its heart,
      mathematics is not a technical field,
      or even about problem-solving --
      at its most fundamental,
      it is about framing problems so that they
      &lt;b&gt;can&lt;/b&gt; be solved.
      And Chomsky's skill at framing problems is astonishing.
    &lt;/p&gt;
    &lt;h2&gt;A brilliant simplification&lt;/h2&gt;
    &lt;p&gt;
      In 1956,
      Chomsky had a new approach to linguistics,
      and wanted to prove that his approach to language
      did things that
      the previous approach,
      based on finite-state models,
      could not.
      (&quot;Finite-state&quot; models, also known as Markov chains,
      are the predecessors of the regular expressions
      of today.)
      Brilliantly,
      Chomsky sets out to do this with extremely minimal definition of what
      a language is.
    &lt;/p&gt;
    &lt;blockquote&gt;
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of sysbols.  If A is an alphabet, we shall say that
      anything formed by concatenating the symbols of A is a string in
      A. By a grammar of the language L we mean a device of some sort that
      produces all of the strings that are sentences of L and only these.&lt;a id=&quot;footnote-5-ref&quot; href=&quot;#footnote-5&quot;&gt;[5]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;Yes, you read that right --
    Chomsky uses a definition of language which has nothing to
    do with language actually meaning anything.
    A language, for the purposes of the math in &quot;Three Models&quot;,
    is nothing but a list of strings.
    Similarly, a grammar is just something that enumerates
    those strings.
    The grammar does not have to provide any clue as to what
    the strings might mean.
    &lt;/p&gt;
    &lt;p&gt;
        For example, Chomsky would require of a French grammar that one of the
	strings that it lists be
    &lt;/p&gt;
    &lt;blockquote&gt;
      (42) Ceci n'est pas une phrase vraie.
    &lt;/blockquote&gt;
    &lt;p&gt;But for the purposes of his demonstration,
    Chomsky does not require of his &quot;grammar&quot; that it
    give us any guidance as to what sentence (42) might mean.
    &lt;p&gt;
    Chomsky shows that there are English sentences that his &quot;grammar&quot; would
    list,
    and which a finite-state &quot;grammar&quot; would not list.
    Clearly if the finite-state grammar cannot even produce a sentence
    as one of a list,
    it is not adequate as a model of that language,
    at least as far as that sentence goes.
    &lt;p&gt;
    &lt;/p&gt;
    Chomsky shows that there is,
    in fact,
    a large, significant class of sentences that
    his &quot;grammars&quot; can list,
    but which the finite-state grammars cannot list.
    Chomsky presents this as
    very strong evidence that
    his grammars will make better models
    of language
    than finite-state grammars can.
    &lt;/p&gt;
    &lt;h2&gt;Other considerations&lt;/h2&gt;
    &lt;p&gt;
    In addition to simplifying the math, Chomsky has two other good
    reasons to avoid dealing with meaning.
    A second reason is that
    semantics is a treacherously dangerous field of study.
    If you can make your point,
    and don't have to drag in semantics,
    you are crazy to do otherwise.
    Sentence (42), above,
    is just one example of the pitfalls
    that await those tackle who semantic issues.
    It echoes
    &lt;a href=&quot;https://en.wikipedia.org/wiki/The_Treachery_of_Images&quot;&gt;a famous Magritte&lt;a&gt;
    and translates to &quot;This is not a true sentence&quot;.
    &lt;/p&gt;
    &lt;p&gt;
    A third reason is that most linguists of Chomsky's time
    were Bloomfieldians.
    Bloomfield defined language as follows:
    &lt;/p&gt;
    &lt;blockquote&gt;
    The totality of utterances that can be made in a speech
    community is the &lt;b&gt;language&lt;/b&gt;
    of that speech-community.&lt;a id=&quot;footnote-6-ref&quot; href=&quot;#footnote-6&quot;&gt;[6]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
    Bloomfield says &quot;totality&quot; instead of &quot;set&quot;
    and &quot;utterances&quot; instead of &quot;strings&quot;,
    but for our purposes in this post the idea is the same --
    the definition is without regard to the meaning
    of the members of the set.&lt;a id=&quot;footnote-7-ref&quot; href=&quot;#footnote-7&quot;&gt;[7]&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
    Bloomfield's omission of semantics is not accidental.
    Bloomfield wanted to establish linguistics as a
    science, and for Bloomfield
    claiming to know the meaning of
    a sentence was dangerously close to
    claiming to be able to read minds.
    You cannot base your work on mind-reading and expect people to
    believe that you are doing science.
    Bloomfield therefore suggested avoiding,
    totally if possible,
    any discussion of semantics.
    Most readers of Chomsky's paper in 1956 were Bloomfieldians --
    Chomsky has studied under a Bloomfieldian,
    and originally was seen as one.&lt;a id=&quot;footnote-8-ref&quot; href=&quot;#footnote-8&quot;&gt;[8]&lt;/a&gt;
    By excluding semantics from his own model of language,
    Chomsky was making his paper maximally acceptable to
    his readership.
    &lt;/p&gt;
    &lt;h2&gt;Semantics sneaks back in&lt;/h2&gt;
    &lt;p&gt;
    But you did not have to read Chomsky's mind,
    or predict the future,
    to see that Chomsky
    was a lot more interested in semantics than
    Bloomfield was.
    Already in &quot;Three Models&quot;,
    he is suggesting that his model is superior to
    its predecessors,
    because his model,
    when an utterance is ambiguous,
    produces multiple derivations to reflect that.
    Even better, these multiple derivations &quot;look&quot; like natural representations
    of the difference between meanings.&lt;a id=&quot;footnote-9-ref&quot; href=&quot;#footnote-9&quot;&gt;[9]&lt;/a&gt;
    These insights,
    which dropped effortlessly out of Chomsky's grammars,
    were well beyond what the finite-state models were providing.
    &lt;/p&gt;
    &lt;p&gt;
    By itself,
    Chomsky's argument, that his grammars were better
    because they could list more sentences,
    might have carried the day.
    With the demonstration that his grammars could do
    more than list sentences,
    but also could proivde insight into the structure and semantics
    of sentences,
    Chomsky's case was compelling.
    Young linguists wanted theoretical tools with this
    kind of power and those few
    older linguists not convinced struggled to find
    reasons why the young linguists could not
    have what they wanted.
    &lt;/p&gt;
    &lt;p&gt;
    In later years,
    Chomsky made it quite clear what his position was:
    &lt;blockquote&gt;
    [...] it would be absurd to develop
    a general syntactic theory
    without assigning an absolutely
    crucial role to semantic considerations,
    since obviously the necessity to support
    semantic interpretation is one of the primary
    requirements
    that the structures
    generated by the syntactic component of a grammar
    must meet.&lt;a id=&quot;footnote-10-ref&quot; href=&quot;#footnote-10&quot;&gt;[10]&lt;/a&gt;
    &lt;/blockquote&gt;
    Compare this to Bloomfield:
    &lt;blockquote&gt;
    The statement of meanings is therefore the weak point in
    language-study, and will remain so until human knowledge
    advances very far beyond its present state. In practice, we define the
    meaning of a linguistic form, wherever we can, in terms of some
    other science.&lt;a id=&quot;footnote-11-ref&quot; href=&quot;#footnote-11&quot;&gt;[11]&lt;/a&gt;
    &lt;/blockquote&gt;
    It is easy to see why linguists found Chomsky's
    expansion of their horizons irresistable.
    &lt;p&gt;
    &lt;/p&gt;
    &lt;h2&gt;The tradition&lt;/h2&gt;
    &lt;p&gt;Given the immense prestige of &quot;Three models&quot;,
    it is unsurprising that it was closely studied by
    the pioneers of parsing theory.
    Unfortunately, what they picked up was not
    Chomsky's emphasis on the overriding importance
    of semantics,
    but the narrow definition of language
    that Chomsky had adopted from Bloomfield
    for tactical purposes.
    In the classic Aho and Ullman 1972 textbook, we have
    &lt;/p&gt;
    &lt;blockquote&gt;
    A language over an alphabet &amp;Sigma;
    is a set of strings over an alphabet &amp;Sigma;.
    This definition encompasses almost everyone's notion of a language.&lt;a id=&quot;footnote-12-ref&quot; href=&quot;#footnote-12&quot;&gt;[12]&lt;/a&gt;
    &lt;/blockquote&gt;
    If this &quot;encompasses&quot; my notion of a language,
    it does so only in the sense that an avalanche encompasses
    a skier.
    &lt;/p&gt;
    &lt;p&gt;
    From 1988, thirty years after Chomsky,
    here is another authoritative textbook of Parsing Theory
    defining &quot;language&quot;:
    &lt;/p&gt;
    &lt;blockquote&gt;
      A set V is an alphabet (or a vocabulary) if it is finite and
      nonempty.
      The elements
      of an alphabet V are called the symbols (or letters or characters) of
      V.
      A language L over V is any subset of the free monoid V*.
      The elements
      of a language L are called sentences of L.&lt;a id=&quot;footnote-13-ref&quot; href=&quot;#footnote-13&quot;&gt;[13]&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;The language is now that of abstract algebra,
    but the idea is the same -- pure Bloomfield.&lt;a id=&quot;footnote-14-ref&quot; href=&quot;#footnote-14&quot;&gt;[14]&lt;/a&gt;
    &lt;/p&gt;
    &lt;h2&gt;The problem&lt;/h2&gt;
    &lt;p&gt;
    Interesting, you might be saying, that
    some textbook definitions are not everything they could be,
    but is there any effect on the daily practice of
    programming?
    &lt;/p&gt;
    &lt;p&gt;
    The languages human beings use with each other
    are powerful,
    varied, flexible and endlessly retargetable.
    The parsers we use to communicate with computers
    are restrictive, repetitive in form,
    difficult to reprogram,
    and prohibitively hard to retarget.
    Is this because humans have a preternatural language ability?
    &lt;/p&gt;
    &lt;p&gt;
    Or is there something wrong with the way we
    go about talking to computers?
    How the Theory of Parsing literature defines the term
    &quot;language&quot; may seem
    of only pedantic interest.
    But I will argue that it is a mistake which has everything
    to do with the limits of modern computer languages.
    &lt;p&gt;
    What is the problem with defining a language as a set of strings?
    Here is one example of how the textbook definition
    affects daily practice.
    Call one grammar &lt;tt&gt;SENSE&lt;/tt&gt;:
    &lt;/p&gt;
    &lt;pre id=&quot;g-structure-op&quot;&gt;&lt;tt&gt;
      SENSE ::= E
      E ::= E + T
      E ::= T
      T ::= T * P
      T ::= P
      P ::= number
    &lt;/tt&gt;&lt;/pre&gt;
    &lt;p&gt;
    And call another grammar &lt;tt&gt;STRING&lt;/tt&gt;:
    &lt;/p&gt;
    &lt;pre id=&quot;g-string-op&quot;&gt;&lt;tt&gt;
      STRING ::= E
      E  ::= P OP E
      OP ::= '*'
      OP ::= '+'
      P  ::= number
    &lt;/tt&gt;&lt;/pre&gt;
    &lt;p&gt;
    If you define a language as a set of strings,
    both
    &lt;tt&gt;SENSE&lt;/tt&gt;
    and &lt;tt&gt;STRING&lt;/tt&gt;
    recognize the same language.
    But it's a very different story if you
    take the intended meaning as
    that of traditional arithmetic expressions,
    and consider
    the meaning of the two grammars.
    &lt;/p&gt;
    &lt;p&gt;
    &lt;tt&gt;SENSE&lt;/tt&gt;
    recognizes the associativity and precedence of the two operators --
    the parse tree it produces could be used directly to evaluate an arithmetic
    expression and the answer would always be correct.
    The parse tree that &lt;tt&gt;STRING&lt;/tt&gt; produces, if evaluated directly,
    will very often produce a wrong answer -- it does not capture
    the structure of an arithmetic expression.
    In order to produce correct results,
    the output of &lt;tt&gt;STRING&lt;/tt&gt; could be put through a second phase,
    but that is the point --
    &lt;tt&gt;STRING&lt;/tt&gt; left crucial parts of the job of parsing undone,
    and either some other logic does the job &lt;tt&gt;STRING&lt;/tt&gt; did not do,
    or a wrong answer results.
    &lt;p&gt;
    It is much easier to write a parser for &lt;tt&gt;STRING&lt;/tt&gt;
    than it is for &lt;tt&gt;SENSE&lt;/tt&gt;.
    Encouraged by a theory that minimizes the
    difference,
    many implementations attempt to make do with &lt;tt&gt;STRING&lt;/tt&gt;.
    &lt;/p&gt;
    &lt;p&gt;
    But that is not the worst of it.
    The idea that a language is a set of strings
    has guided research,
    and steered it away from the most promising lines.
    How, I hope to explain in the future.&lt;a id=&quot;footnote-15-ref&quot; href=&quot;#footnote-15&quot;&gt;[15]&lt;/a&gt;
    &lt;/p&gt;
    &lt;h2&gt;Comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      The background material for this post is in my
    &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
    Parsing: a timeline 3.0&lt;/a&gt;,
    and this post may be considered a supplement to &quot;Timelime&quot;.
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
    &lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p id=&quot;footnote-1&quot;&gt;1.
	&lt;cite&gt;Hamlet&lt;/cite&gt;, Act I, scene iv.
 &lt;a href=&quot;#footnote-1-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-2&quot;&gt;2.
      Chomsky, Noam.
      &quot;Three models for the description of language.&quot;
      &lt;cite&gt;IRE Transactions on information theory&lt;/cite&gt;,
      vol. 2, issue 3, September 1956, pp. 113-124.
 &lt;a href=&quot;#footnote-2-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-3&quot;&gt;3.
	Harris, Randy Allen,
	&lt;cite&gt;The Linguistics Wars&lt;/cite&gt;,
	Oxford University Press, 1993,
	pp 33.
 &lt;a href=&quot;#footnote-3-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-4&quot;&gt;4.
      In this post I am treating the &quot;Three models&quot; paper
      as the &quot;first&quot;
      work of Chomskyan linguistics.
      Other choices can be justified.
      The next year, 1957,
      Chomsky published a book covering the same material: &lt;cite&gt;Syntactic Structures&lt;/cite&gt;.
      &lt;cite&gt;Syntactic Structures&lt;/cite&gt;
      was much more accessible,
      and attracted much more attention --
      the Chomskyan revolution did not really begin before
      it came out.
      On the other hand,
      both of these draw their material from Chomsky's
      1000-page
      &lt;cite&gt;Logical Structure of Linguistic Theory&lt;/cite&gt;,
      which was completed in June 1955.
      But &lt;cite&gt;Logical Structure of Linguistic Theory&lt;/cite&gt;
      was not published until 1975
      and then only in part.
      (See
      &lt;a href=&quot;https://www.journals.uchicago.edu/doi/full/10.1086/686177&quot;&gt;
      Radick, Gregory,
      &quot;The Unmaking of a Modern Synthesis: Noam Chomsky, Charles Hockett, and the Politics of Behaviorism, 1955–1965&quot;
      &lt;/a&gt;.)
 &lt;a href=&quot;#footnote-4-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-5&quot;&gt;5.
      Chomsky 1956, p. 114.
 &lt;a href=&quot;#footnote-5-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-6&quot;&gt;6.
    Bloomfield, Leonard,
    &quot;A set of Postulates
    for the Science of Language&quot;,
    &lt;cite&gt;Language&lt;/cite&gt;, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
    The quote is definition 4 on p. 154.
 &lt;a href=&quot;#footnote-6-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-7&quot;&gt;7.
    In case there is any doubt as to the link between
    the Chomsky and Bloomfield definitions,
    Chomsky also calls his strings,
    &quot;utterances&quot;.
    See Chomsky, Noam, &lt;cite&gt;Syntactic Structures&lt;/cite&gt;,
    2nd ed.,
    Mouton de Gruyter, 2002, p. 49
 &lt;a href=&quot;#footnote-7-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-8&quot;&gt;8.
    Harris 1993, pp 31-34, p. 37.
 &lt;a href=&quot;#footnote-8-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-9&quot;&gt;9.
    Chomsky 1956, p. 118, p. 123.
 &lt;a href=&quot;#footnote-9-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-10&quot;&gt;10.
    Chomsky, Noam.
    &lt;cite&gt;Topics in the Theory of Generative Grammar&lt;/cite&gt;.
    De Gruyter, 1978, p. 20.
    (The quote occurs in footnote 7 starting on p. 19.)
 &lt;a href=&quot;#footnote-10-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-11&quot;&gt;11.
    Bloomfield, Leonard.
    &lt;cite&gt;Language&lt;/cite&gt;.
    Holt, Rinehart and Winston, 1933, p. 140.
 &lt;a href=&quot;#footnote-11-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-12&quot;&gt;12.
    Aho, Alfred V., and Jeffrey D. Ullman.
    &lt;cite&gt;The theory of parsing, translation, and compiling&lt;/cite&gt;.
    Vol. 1. Prentice-Hall, 1972, p. 16.
 &lt;a href=&quot;#footnote-12-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-13&quot;&gt;13.
      Sippu, Seppo and Soisalon-Soininen, Eljas.
      &lt;cite&gt;Parsing Theory&lt;/cite&gt;, Volume I,
      Springer-Verlag, 1988,
      p. 11.
 &lt;a href=&quot;#footnote-13-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-14&quot;&gt;14.
    A welcome errancy from tradition, however, arrives with
    Grune, D. and Jacobs, C. J. H., &lt;cite&gt;Parsing Techniques: A Practical Guide&lt;/cite&gt;,
    2nd edition, Springer, 2008.
    On pp. 5-7, they attribute the traditional &quot;set of strings&quot; definition
    to &quot;formal linguistics&quot;.
    They
    point out that the computer scientist requires a grammar to
    not only list a set of strings, but provide a
    &quot;structure&quot; for each of them.&lt;br&gt;&lt;br&gt;
    As an aside,
    Grune and Jacobs often depart from the &quot;just stick to the math&quot;
    approach taken by other textbooks parsing theory.
    They often give the history and motivation behind the math.
    My own work owes much to them.
 &lt;a href=&quot;#footnote-14-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-15&quot;&gt;15.
    Readers who want to peek ahead can look at my
    &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
    Parsing: a timeline 3.0&lt;/a&gt;.
    The tale is told there is from a somewhat different point of view,
    but no reader of &quot;Timeline&quot; will be much surprised by where
    I take this line of thought.
 &lt;a href=&quot;#footnote-15-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Parsers and Useful Power</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/fast_power.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      What do parser
      users want?
      What makes a parser&lt;a id=&quot;footnote-1-ref&quot; href=&quot;#footnote-1&quot;&gt;[1]&lt;/a&gt;
      successful?
      In this post I will look at
      one aspect of
      that question,
      in light of an episode in
      the history of parsing.
    &lt;/p&gt;
    &lt;h2&gt;Irons 1961&lt;/h2&gt;
    &lt;p&gt;
      The first paper
      fully describing a parser was Irons 1961&lt;a id=&quot;footnote-2-ref&quot; href=&quot;#footnote-2&quot;&gt;[2]&lt;/a&gt;.
      The Irons parser was what is called &quot;general&quot;,
      meaning that it can parse all of
      the &quot;context-free grammars&quot;.
      That makes it
      far more powerful than most parsers
      in practical use today.
    &lt;/p&gt;
    &lt;p&gt;
      But the Irons algorithm was not always fast in the general case.
      Irons 1961 used backtracking
      to achieve its power,
      so it would go exponential for many useful grammars.
    &lt;/p&gt;
    &lt;p&gt;
      Among the grammars Irons 1961 could not parse quickly
      were those containing the all-important arithmetic expressions.
      Irons 1961 gave way to recursive descent.
    &lt;/p&gt;
    &lt;p&gt;
      Recursive descent (RD) in its pure form,
      could not parse arithmetic expressions at all,
      but it could be customized with procedural code.
      That is, it could call specialized parsers which were
      reliably fast for specific sections of the input.
      The Irons parser was declarative,
      and not easy to cusomtize.
    &lt;/p&gt;
    &lt;h2&gt;Raw power versus useful power&lt;/h2&gt;
    &lt;p&gt;
      The contest between Irons parsing and recursive descent took place
      before the theory for analyzing algorithms was fully formed.&lt;a id=&quot;footnote-3-ref&quot; href=&quot;#footnote-3&quot;&gt;[3]&lt;/a&gt;
      In retrospect, we can say that,
      except in specialized uses,
      an acceptable parser for most practical uses
      must be linear or quasi-linear.&lt;a id=&quot;footnote-4-ref&quot; href=&quot;#footnote-4&quot;&gt;[4]&lt;/a&gt;
      That is,
      the &quot;useful power&quot; of a parser is the class
      of grammars that it will parse in quasi-linear time.&lt;a id=&quot;footnote-5-ref&quot; href=&quot;#footnote-5&quot;&gt;[5]&lt;/a&gt;
    &lt;/p&gt;
    &lt;p&gt;
      Useful power turns out to be more important,
      in practice,
      than raw power.
      Recursive descent won out over the
      Irons algorithm because,
      while the Irons algorithm had vastly more raw power,
      RD had slightly more &quot;useful power&quot;.
    &lt;p&gt;
      It is nice to have raw power as well -- it means an algorithm can take on some specialized tasks.
      And raw power provides a kind of &quot;soft failure&quot; debugging mode for grammars with,
      for example, unintended ambiguities.
      But, in the eyes of the programming community, the more important measure of a parser
      is its useful power -- the class of grammars that it will parse at quasi-linear speed.
    &lt;/p&gt;
    &lt;h2&gt;Stating the obvious?&lt;/h2&gt;
    &lt;p&gt;
    That useful power is more important than raw power may seem,
    in retrospect,
    obvious.
    But in fact, it remains a live issue.
    In practice raw power and useful power are often confused.
    The parsing literature is not always as helpful as it could be:
    it can be hard to determine what the
    useful power of an algorithm is.
    &lt;/p&gt;
    &lt;p&gt;
      And the Irons experiment with raw power is often repeated,
      in hopes of a different result.
      Very often,
      a new algorithm is a hybrid of two others:
      an algorithm with a lot of raw power,
      but which can go quadratic or worse;
      and a fast algorithm which lacks power.
      When the power of the fast algorithm fails,
      the hybrid algorithm switches over
      to the algorithm with raw power.
    &lt;/p&gt;
    &lt;p&gt;
      It is a sort of
      cross-breeding of algorithms.
      The hope is that the hybrid algorithm has the best
      features of each of its parents.
      This works a lot better in botany than it does in parsing.
      Once you have a successful cross in a plant,
      you can breed from the successful hybrid
      and expect good things to happen.
      In botany,
      the individual crosses can have an extremely high
      failure rate,
      and cross-breeding can still succeed.
      But it's different when you cross algorithms:
      Even after you've succeeded with one parse,
      the next parse from your hybrid is a fresh new toss of the dice.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      To learn about
      my own parsing project,
      Marpa&lt;a id=&quot;footnote-6-ref&quot; href=&quot;#footnote-6&quot;&gt;[6]&lt;/a&gt;,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
    &lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p id=&quot;footnote-1&quot;&gt;1.
      By &quot;parser&quot; in this post,
      I will mean a programmer's
      most powerful toolbox parser --
      what might be called the &quot;flagship&quot; parser.
      No parser will ever be the right one for all uses.
 &lt;a href=&quot;#footnote-1-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-2&quot;&gt;2.
	For the reference to Irons, see
        &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
          V3 of my &quot;Parsing: A Timeline&quot;&lt;/a&gt;.
	  The &quot;Timeline&quot; contains the background material for this post.
 &lt;a href=&quot;#footnote-2-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-3&quot;&gt;3.
      Even the term &quot;analysis of algorithms&quot; did not exist until 1969:
      see &lt;a href=&quot;https://web.archive.org/web/20160828152021/http://www-cs-faculty.stanford.edu/~uno/news.html&quot;&gt;
      Knuth, &quot;Recent News&quot;&lt;/a&gt;.
 &lt;a href=&quot;#footnote-3-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-4&quot;&gt;4.
        For more about &quot;linear&quot; and &quot;quasi-linear&quot;,
	including definitions,
	see
        &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;
          V3 of my &quot;Parsing: A Timeline&quot;&lt;/a&gt;,
        in particular its 'Term: linear' section.
 &lt;a href=&quot;#footnote-4-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-5&quot;&gt;5.
	While it is clearly the consensus among practitioners and theoreticians
	that, for parsing,
	practical time is quasi-linear or better,
	there are those who argue that worse-than-quasi-linear parsers
	are often the right ones for the job,
	and that research on them has been unwisely neglected.
	The dissenters are not without a case:
	For example, in natural language, while sentences are in theory
	infinite in length, in practice their average size is fixed.
	And while very long difficult-to-parse sentences do occur in some texts, such as
	older ones, it is normal for a human reader
	to have to spend extra time on them.
	So it may be unreasonable to insist that a parsing algorithm be
	quasi-linear in this application.
 &lt;a href=&quot;#footnote-5-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;p id=&quot;footnote-6&quot;&gt;6.
      Marpa's useful power is LR-regular,
      which properly contains
      every class of grammar in practical use: regular expressions,
      LALR,
      LL(k) for all k,
      LR(k) for all k,
      and the LL-regular grammars.
      &lt;!-- For LLR, see Theorem 3, p. 448 in
      https://ris.utwente.nl/ws/portalfiles/portal/6126669,
      Nijholt, &quot;On the parsing of LL-regular grammars&quot; --&gt;
 &lt;a href=&quot;#footnote-6-ref&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Version 3 of &quot;Parsing: a timeline&quot;</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/04/timeline_v3.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
      My most popular blog posts by far have been my two versions of &quot;Parsing: a timeline&quot;.
      I have just created
      &lt;a href=&quot;https://jeffreykegler.github.io/personal/timeline_v3&quot;&gt;a
        3rd version&lt;/a&gt;,
      which has so many changes
      that it might be considered a new work.
      The new version is less Marpa-centric and several times as long.
      It covers new topics, including combinator and monadic
      parsing, and operator expression parsing.
      And sources are now provided for all material.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      For more about
      Marpa, my own parsing project,
      there is the
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  </channel>
</rss>
