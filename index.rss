<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>What parser do birds use?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/03/parus.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;blockquote&gt;
      &quot;Here we provide, to our knowledge, the first unambiguous experimental evidence for compositional syntax in a non-human vocal system.&quot;
      --
      &lt;a href=&quot;http://www.nature.com/ncomms/2016/160308/ncomms10986/full/ncomms10986.html&quot;&gt;
        &quot;Experimental evidence for compositional syntax in bird calls&quot;
        Toshitaka N. Suzuki,	David Wheatcroft	&amp;amp; Michael Griesser
        &lt;emph&gt;Nature Communications&lt;/emph&gt;
        7, Article number: 10986
      &lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      In this post I look at a subset of the language
      of the
      &lt;a href=&quot;https://en.wikipedia.org/wiki/Great_tit&quot;&gt;
        Japanese great tit&lt;/a&gt;,
      also known as Parus major.
      The above cited article presents evidence
      that bird brains can parse this language.
      What about standard modern computer parsing methods?
      Here is the subset -- probably a tiny one --
      of the language actually used by Parus major.
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
      S ::= ABC
      S ::= D
      S ::= ABC D
      S ::= D ABC
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
    &lt;/p&gt;
    &lt;h2&gt;Classifying the Parus major grammar&lt;/h2&gt;
    &lt;p&gt;Grammophone is a very handy
      &lt;a href=&quot;http://mdaines.github.io/grammophone/#&quot;&gt;
        new tool&lt;/a&gt;
      for classifying grammars.
      It's own parser is somewhat limited, so that it requires a period
      to mark the end of a rule.
      The above grammar is in Marpa's SLIF format, which is smart enough to
      use the &quot;&lt;tt&gt;::=&lt;/tt&gt;&quot;
      operator to spot the beginning and end of rules,
      just as the human eye does.
      Here's the same grammar converted into a form acceptable to Grammophone:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
      S -&gt; ABC .
      S -&gt; D .
      S -&gt; ABC D .
      S -&gt; D ABC .
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      Grammophone tells us that the Parus major grammar is not LL(1),
      but that it is LALR(1).
    &lt;/p&gt;
    &lt;h2&gt;What does this mean?&lt;/h2&gt;
    &lt;p&gt;LL(1) is the class of grammar parseable by top-down methods:
      it's the best class for characterizing most parsers in current use,
      including recursive descent, PEG,
      and Perl 6 grammars.
      All of these parsers fall short of dealing with the Parus major language.
    &lt;/p&gt;&lt;p&gt;
      LALR(1) can handle this subset of Parus's language,
      but we can assume they would also reject it.
      LALR(1), in the form of bison and yacc,
      had been popular, but it has horrible error handling properties.
      When the input is correct and within its limits,
      an LALR-driven parser can produce a parse,
      But if the input is not correct, LALR parsers do not produce
      a useful analysis of what went wrong.
      If Parus hears &quot;d abc d&quot;,
      a parser like Marpa, on the other hand, can produce something like
      this:
    &lt;/p&gt;
    &lt;blockquote&gt;&lt;pre&gt;
# * String before error: abc d\s
# * The error was at line 1, column 7, and at character 0x0064 'd', ...
# * here: d
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      LALR(1), while able to handle this subset of Parus's language,
      has its own limits, and whether it could handle the full
      complexity of Parus
      language is a serious question.
      But it's a question that in practice would probably not arise.
      Parus uses its language in predatory contexts,
      and one can assume that a Parus with a preference for parsers whose
      error handling is on an LALR(1) level
      will not be keeping its alleles in the gene pool for very
      long.
    &lt;/p&gt;&lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      Those readers content with sub-Parus parsing methods may stop reading here.
      Those with greater parsing ambitions, however, may
      wish to learn more about Marpa.
      A Marpa test script for parsing the Parus subset is in
      &lt;a href=https://gist.github.com/jeffreykegler/b5b8ba349b8c6e5c2e54&gt;a Github gist.&lt;/a&gt;
      Marpa has a
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;semi-official web site, maintained by Ron Savage&lt;/a&gt;.
      The official, but more limited, Marpa website
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;is my personal one&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>What are the reasonable computer languages?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/01/lrr.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;blockquote&gt;
      &quot;You see things; and you say 'Why?' But I dream things that never were; and I say 'Why not?'&quot;
      --
      &lt;a href=&quot;http://www.bartleby.com/73/465.html&quot;&gt;George Bernard Shaw&lt;/a&gt;
    &lt;/blockquote&gt;
    &lt;p&gt;
      In the 1960's and 1970's computer languages were evolving rapidly.
      It was not clear which way they were headed.
      Would most programming be done with
      general-purpose languages?
      Or would programmers create a language for every task domain?
      Or even for every project?
      And, if lots of languages were going to be created,
      what kinds of languages would be needed?
    &lt;/p&gt;
    &lt;p&gt;
      It was in that context that &amp;#268;ulik and Cohen,
      in
      &lt;a href=&quot;http://www.sciencedirect.com/science/article/pii/S0022000073800509&quot;&gt;a
        1973 paper&lt;/a&gt;,
      outlined what they thought programmers would want and should have.
      In keeping with the spirit of the time,
      it was quite a lot:
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;Programmers would want to extend their grammars with new syntax,
        including new
        kinds of expressions.&lt;/li&gt;
      &lt;li&gt;Programmers would also want to use tools that automatically generated new syntax.&lt;/li&gt;
      &lt;li&gt;Programmers would not want to, and especially
        in the case of auto-generated syntax
        would usually not be able to,
        massage the syntax into very restricted forms.
        Instead, programmers would create grammars and languages
        which required unlimited lookahead to disambiguate,
        and they would require parsers which could handle these grammars.&lt;/li&gt;
      &lt;li&gt;Finally, programmers would need to be able to rely on
        all of this parsing being done in linear time.&lt;/li&gt;
    &lt;/ul&gt;&lt;p&gt;
      Today, we think we know that
      &amp;#268;ulik and Cohen's vision was naive,
      because we think we know that parsing technology cannot support it.
      We think we know that parsing is much harder than they thought.
    &lt;/p&gt;
    &lt;h2&gt;The eyeball grammars&lt;/h2&gt;
    &lt;p&gt;As a thought problem, consider the &quot;eyeball&quot; class of grammars.
      The &quot;eyeball&quot; class of grammars contains all the grammars that a human
      can parse at a glance.
      If a grammar is in the eyeball class,
      but a computer cannot parse it,
      it presents an interesting choice.  Either,
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;your computer is not using the strongest practical algorithm; or
      &lt;/li&gt;
      &lt;li&gt;your mind is using some power which cannot be reduced to a machine computation.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      There are some people out there (I am one of them)
      who don't believe that everything the mind can do reduces
      to a machine computation.
      But even those people
      will tend to go for the choice in this case:
      There must be some practical computer parsing algorithm which
      can do at least as well at parsing as a human
      can do by &quot;eyeball&quot;.
      In other words, the class of &quot;reasonable grammars&quot; should
      contain the eyeball class.
    &lt;/p&gt;
    &lt;p&gt;
      &amp;#268;ulik and Cohen's candidate for the class of &quot;reasonable grammars&quot;
      were the grammars that
      a deterministic parse engine
      could parse if it had a lookahead that was infinite,
      but restricted to distinguishing between regular expressions.
      They called these the LR-regular, or LRR, grammars.
      And the LRR grammars
      &lt;b&gt;do&lt;/b&gt;
      in fact seem to be a good first approximation
      to the eyeball class.
      They do not allow lookahead that contains things
      that you have to count, like palindromes.
      And, while I'd be hard put to eyeball every possible string for every possible regular expression,
      intuitively the concept of scanning for a regular expression
      does seem close to capturing the idea of glancing through a text looking for a telltale pattern.
    &lt;/p&gt;
    &lt;h2&gt;So what happened?&lt;/h2&gt;
    &lt;p&gt;
      Alas, the algorithm in the &amp;#268;ulik and Cohen paper turned out to be impractical.
      But in 1991, Joop Leo discovered a way to adopt Earley's algorithm to parse the LRR grammars
      in linear time, without doing the lookahead.
      And Leo's algorithm does have a practical implementation:
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;Marpa&lt;/a&gt;.
    &lt;/p&gt;
    &lt;h2&gt;References, comments, etc.&lt;/h2&gt;
    &lt;p&gt;
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Top-down parsing is guessing</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/12/topdown.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;
      Top-down parsing is guessing.  Literally.
      Bottom-up parsing is looking.
    &lt;/p&gt;
    &lt;p&gt;The way you'll often hear that phrased is that top-down parsing is
      looking, starting at the top,
      and bottom-up parsing is looking, starting at the bottom.
      But that is misleading, because the input is at the bottom --
      at the top there is nothing to look at.
      A usable top-down parser
      &lt;b&gt;must&lt;/b&gt;
      have a bottom-up component,
      even if that component is just lookahead.
    &lt;/p&gt;
    &lt;p&gt;A more generous, but still accurate, way to describe the top-down
      component of parsers is &quot;prediction&quot;.
      And prediction is, indeed, a very useful component of a parser,
      when used in combination with other techniques.
    &lt;/p&gt;
    &lt;p&gt;Of course, if a parser does nothing but predict,
      it can predict only one input.
      Top-down parsing must always be combined with a bottom-up
      component.
      This bottom-up component may be as modest as lookahead, but it
      &lt;b&gt;must&lt;/b&gt;
      be there or else top-down parsing is really not parsing at all.
    &lt;/p&gt;
    &lt;h2&gt;So why is top-down parsing used so much?&lt;/h2&gt;
    &lt;p&gt;Top-down parsing may be unusable in its pure form,
      but from one point of view that is irrelevant.
      Top-down parsing's biggest advantage is that it is highly flexible --
      there's no reason to stick to its &quot;pure&quot; form.
    &lt;/p&gt;
    &lt;p&gt;A top-down parser can be written as a series of subroutine calls --
      a technique called recursive descent.
      Recursive descent
      allows you to hook in custom-written bottom-up logic at every
      top-down choice point,
      and it is a technique which is
      completely understandable to programmers with little or no training
      in parsing theory.
      When dealing with recursive descent parsers,
      it is more useful to be a seasoned, far-thinking programmer
      than it is to be a mathematician.
      This makes recursive descent very appealing to
      seasoned, far-thinking programmers,
      and they are the audience that counts.
    &lt;/p&gt;
    &lt;h2&gt;Switching techniques&lt;/h2&gt;
    &lt;p&gt;You can even use the flexibility of top-down to switch
      away from top-down parsing.
      For example, you could claim that a top-down parser could do anything my
      own parser
      (&lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;Marpa&lt;/a&gt;)
      could do, because a recursive descent parser can call
      a Marpa parser.
    &lt;/p&gt;
    &lt;p&gt;
      A less dramatic switchoff, and one that still leaves the parser with a good claim to be
      basically top-down, is very common.
      Arithmetic expressions are essential for a computer language.
      But they are also
      among the many things
      top-down parsing cannot handle, even with ordinary lookahead.
      Even so, most computer languages these days are parsed top-down --
      by recursive descent.
      These recursive descent parsers deal with expressions
      by temporarily handing control over to an bottom-up operator
      precedence parser.
      Neither of these parsers is
      extremely smart about the hand-over and hand-back
      -- it is up to the programmer to make sure the two play together nicely.
      But used with caution, this approach works.
    &lt;/p&gt;
    &lt;h2&gt;Top-down parsing and language-oriented programming&lt;/h2&gt;
    &lt;p&gt;But what about taking top-down methods into the future of language-oriented programming,
      extensible languages, and grammars which write grammars?
      Here we are forced to confront the reality -- that the effectiveness of
      top-down parsing comes entirely from the foreign elements that are added to it.
      Starting from a basis of top-down parsing is literally starting
      with nothing.
      As I have shown in more detail
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/12/composable.html&quot;&gt;elsewhere&lt;/a&gt;,
      top-down techniques simply do not have enough horsepower to deal with grammar-driven programming.
    &lt;/p&gt;
    &lt;p&gt;Perl 6 grammars are top-down -- PEG with lots of extensions.
      These extensions
      include backtracking, backtracking control,
      a new style of tie-breaking and lots of opportunity
      for the programmer to intervene and customize everything.
      But behind it all is a top-down parse engine.
    &lt;/p&gt;
    &lt;p&gt;One aspect of Perl 6 grammars might be seen as breaking
      out of the top-down trap.
      That trick of switching over to a
      bottom-up operator precedence parser for expressions,
      which I mentioned above,
      is built into Perl 6 and semi-automated.
      (I say semi-automated because making sure the two parsers &quot;play nice&quot;
      with each other is not automated -- that's still up to the programmer.)
    &lt;/p&gt;
    &lt;p&gt;As far as I know, this semi-automation of expression handling
      is new with Perl 6 grammars, and it
      may prove handy for duplicating what is done
      in recursive descent parsers.
      But it adds no new technique to those already in use.
      And features
      like
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;mulitple types of expression, which can be told apart
        based on their context,&lt;/li&gt;
      &lt;li&gt;&lt;i&gt;n&lt;/i&gt;-ary expressions for arbitrary
        &lt;i&gt;n&lt;/i&gt;, and&lt;/li&gt;
      &lt;li&gt;the autogeneration of multiple rules, each allowing
        a different precedence scheme,
        for expressions of arbitrary arity and associativity,
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      all of which are available and in current use in
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;Marpa&lt;/a&gt;,
      are impossible for the technology behind Perl 6 grammars.
    &lt;/p&gt;
    &lt;p&gt;I am a fan of the Perl 6 effort.
      Obviously, I have doubts about one specific set of hopes for Perl 6 grammars.
      But these hopes have not been central to the Perl 6 effort,
      and I will be an eager student of the Perl 6 team's work over the coming months.
    &lt;/p&gt;
    &lt;h2&gt;Comments&lt;/h2&gt;
    &lt;p&gt;
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Grammar reuse</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/12/composable.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;Every year the Perl 6 community creates an &quot;Advent&quot; series of posts.
      I always follow these, but
      &lt;a href=&quot;https://perl6advent.wordpress.com/2015/12/08/day-8-grammars-generating-grammars/&quot;&gt;one
        in particular&lt;/a&gt;
      caught my attention this year.
      It presents a vision of a future where programming is language-driven.
      A vision that I share.
      The post went on to encourage its readers to follow up on this vision,
      and suggested an approach.
      But I do not think the particular approach suggested would be fruitful.
      In this post I'll explain why.
    &lt;/p&gt;
    &lt;h2&gt;Reuse&lt;/h2&gt;
    &lt;p&gt;The focus of the Advent post was language-driven programming,
      and that is the aspect that excites me most.
      But the points that I wish to make are more easily understood if
      I root them in a narrower, but more familiar issue
      -- grammar reuse.
    &lt;/p&gt;
    &lt;p&gt;
      Most programmers will be very familiar with grammar reuse from regular expressions.
      In the regular expression (&quot;RE&quot;) world,
      programming by cutting and pasting
      is very practical and often practiced.
    &lt;/p&gt;
    &lt;p&gt;
      For this post I will consider grammar reusability to be the ability
      to join two grammars and create a third.
      This is also sometimes called grammar composition.
      For this purpose, I will widen the term &quot;grammar&quot; to include
      RE's and PEG parser specifications.
      Ideally, when you compose two grammars,
      what you get is
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;a language you can reasonably predict, and
      &lt;/li&gt;
      &lt;li&gt;if each of the two original grammars can be parsed in reasonable time,
        a language that can be parsed in reasonable time.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      Not all language representations are reusable.
      RE's are, and BNF is.
      PEG looks like a combination of BNF and RE's,
      but PEG, in fact, is its own very special form of parser specification.
      And PEG parser specifications are
      one of the least reusable language representations ever invented.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and regular expressions&lt;/h2&gt;
    &lt;p&gt;RE's are as well-behaved under
      reuse as a language representation can get.
      The combination of two RE's is always another RE,
      and you can reasonably determine what language the combined RE recognizes by
      examining it.
      Further, every RE is parseable in linear time.
    &lt;/p&gt;
    &lt;p&gt;The one downside, often mentioned by critics, is that RE's
      do not scale in terms of readability.
      Here, however, the problem is not really one of reusability.
      The problem is that RE's are quite limited in their capabilities,
      and programmers often exploit the excellent behavior of RE's under reuse
      to push them into applications for which RE's just do not have the power.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and PEG&lt;/h2&gt;
    &lt;p&gt;When programmers first look at PEG syntax, they often think they've encountered
      paradise. They see both BNF and RE's, and imagine they'll have the
      best of each.
      But the convenient behavior of
      RE's depends on their unambiguity.
      You simply cannot write
      an unambiguous RE -- it's impossible.
    &lt;/p&gt;
    &lt;p&gt;
      More powerful and more flexible, BNF allows you to describe many more grammars --
      including ambiguous ones.
      How does PEG resolve this?  With a Gordian knot approach.
      Whenever it encounters an ambiguity, it throws all but one of the choices away.
      The author of the PEG specification gets some control over what is thrown away --
      he specifies an order of preference for the choices.
      But degree of control is less than it seems,
      and in practice PEG is
      the nitroglycerin of parsing -- marvelous when it works,
      but tricky and dangerous.
    &lt;/p&gt;
    &lt;p&gt;
      Consider these 3 PEG specifications:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
	(&quot;a&quot;|&quot;aa&quot;)&quot;a&quot;
	(&quot;aa&quot;|&quot;a&quot;)&quot;a&quot;
	A = &quot;a&quot;A&quot;a&quot;/&quot;aa&quot;&lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;
      All three clearly accept only strings which are repetitions
      of the letter &quot;&lt;tt&gt;a&lt;/tt&gt;&quot;.
      But which strings?
      For the answers,
      suggestions for dealing with PEG if you are committed to it,
      and more,
      look at
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html&quot;&gt;
        my previous post on PEG&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;
      When getting an RE or a BNF grammar to work,
      you can go back to the grammar and ask
      yourself &quot;Does my grammar look like my intended language?&quot;.
      With PEG, this is not really possible.
      With practice, you might get used to figuring out single line PEG specs
      like the first two above.
      (If you can get the last one, you're amazing.)
      But tracing these through multiple rule layers required by useful grammars is,
      in practice, not really possible.
    &lt;/p&gt;
    &lt;p&gt;
      In real life,
      PEG specifications are written by hacking them until the test suite works.
      And, once you get a PEG specification to pass the test suite for a practical-sized grammar,
      you are very happy to leave it alone.
      Trying to compose two PEG specifications is rolling the dice with the odds against you.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and the native Perl 6 parser&lt;/h2&gt;
    &lt;p&gt;
      The native Perl 6 parser is an extended PEG parser.
      The extensions are very interesting from the PEG point of view.
      The PEG &quot;tie breaking&quot; has been changed,
      and backtracking can be used.
      These features mean the Perl 6 parser can be extended to languages
      well beyond what
      ordinary PEG parsers can handle.
      But, if you use the extra features, reuse will be even trickier than
      if you stuck with vanilla PEG.
    &lt;/p&gt;
    &lt;h2&gt;Reuse and general BNF parsing&lt;/h2&gt;
    &lt;p&gt;
      As mentioned, general BNF is reusable, and so general BNF parsers like
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;Marpa&lt;/a&gt;
      are as reusable as regular expressions, with two caveats.
      First, if the two grammars are not doing their own lexing, their lexers will have
      to be compatible.
    &lt;/p&gt;
    &lt;p&gt;Second,
      with regular expressions you had the advantage that
      &lt;b&gt;every&lt;/b&gt;
      regular expression parses in linear time, so that speed was guaranteed to be acceptable.
      Marpa users reuse grammars and pieces of grammars all the time.
      The result is always the language specified by the merged BNF,
      and I've never heard anyone complain that performance deterioriated.
    &lt;/p&gt;&lt;p&gt;
      But, while it may not happen often,
      it is possible to combine two Marpa grammars that run in linear time
      and end up with one that does not.
      You can guarantee your merged Marpa grammar will stay linear if you follow 2 rules:
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;keep the grammar unambiguous;&lt;/li&gt;
      &lt;li&gt;don't use an unmarked middle recursion.&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      Unmarked middle recursions are not things you're likely to need a lot: they
      are those palindromes where you have to count to find the middle:
      grammars like &quot;&lt;tt&gt;A ::= a | a A a&lt;/tt&gt;&quot;.
      If you use a middle recursion at all, it is almost certainly going to
      be marked, like &quot;&lt;tt&gt;A ::= b | a A a&lt;/tt&gt;&quot;,
      which generates strings like &quot;&lt;tt&gt;aabaa&lt;/tt&gt;&quot;.
      With Marpa, as with RE's, reuse is easy and practical.
      And, as I hope to show in a future post, unlike RE's,
      Marpa opens the road to language-driven programming.
    &lt;/p&gt;
    &lt;h2&gt;Perl 6&lt;/h2&gt;
    &lt;p&gt;I'm a fan of the Perl 6 effort.
      I certainly
      &lt;b&gt;should&lt;/b&gt;
      be a supporter, after the many favors they've done for me
      and the Marpa community over the years.
      The considerations of this post
      will disappoint some of the
      hopes for applications of the native Perl 6 parser.
      But these applications have not been central to the Perl 6 effort,
      of which I will be an eager student over the coming months.
    &lt;/p&gt;
    &lt;h2&gt;Comments&lt;/h2&gt;
    &lt;p&gt;
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
      Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  <item>
    <title>Fast handy languages</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/08/fast_handy.html</link>
    <description>&lt;html&gt;
  &lt;head&gt;
  &lt;/head&gt;
  &lt;body&gt;&lt;p&gt;
      &lt;!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      --&gt;
    &lt;/p&gt;
    &lt;p&gt;Back around 1980, I had access to UNIX and a language I wanted to parse.
      I knew that UNIX had all the latest CS tools.
      So I expected to type in my BNF and &quot;Presto, Language!&quot;.
    &lt;/p&gt;
    &lt;p&gt;Not so easy, I was told.
      Languages were difficult things created with complex tools
      written by experts who understood the issues.
      I recall thinking that,
      while English had a syntax that is
      as hard as they come,
      toddlers manage to parse it
      just fine.
      But experts are experts,
      and more so at second-hand.
    &lt;/p&gt;
    &lt;p&gt;I was steered to an LALR-based parser called yacc.
      (Readers may be more familiar with bison, a yacc successor.)
      LALR had extended the class of quickly parseable grammars a bit
      beyond recursive descent.
      But recursive descent was simple in principle,
      and its limits were easy to discover and work around.
      LALR, on the hand, was OK when it worked, but
      figuring out why it failed when it failed
      was more like decryption than debugging,
      and this was the case both with parser development
      and run-time errors.
      I soon gave up on yacc
      and found another way to solve my problem.
    &lt;/p&gt;
    &lt;p&gt;Few people complained about yacc on the Internet.
      If you noise it about that you are unable
      to figure out how to use
      what everybody says is the state-of-the-art tool,
      the conclusions drawn may not be the ones you want.
      But my experience seems to have been more than common.
    &lt;/p&gt;
    &lt;p&gt;LALR's claim to fame was that it was the basis of the
      industry-standard C compiler.
      Over three decades,
      its maintainers suffered amid the general silence.
      But by 2006, they'd had enough.
      GCC (the new industry standard)
      ripped its LALR engine out.
      By then
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html&quot;&gt;the
        trend back to recursive descent&lt;/a&gt;
      was well underway.
    &lt;/p&gt;
    &lt;h3&gt;A surprise discovery&lt;/h3&gt;
    &lt;p&gt;Back in the 1970's,
      there had been more powerful alternatives
      to LALR and recursive descent.
      But they were
      &lt;a href=&quot;http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/04/fast_enough.html&quot;&gt;reputed
        to be slow&lt;/a&gt;.
    &lt;/p&gt;
    &lt;p&gt;For some applications slow is OK.
      In 2007 I decided that a parsing tool that parsed
      all context-free languages at state-of-the-art speeds,
      slow or fast as the case might be,
      would be a useful addition to programmer toolkits.
      And I ran into a surprise.
    &lt;/p&gt;
    &lt;p&gt;Hidden in the literature was an amazing discovery --
      an 1991 article by Joop Leo that
      described how to modify Earley's
      algorithm to be fast for every language class in practical use.
      (When I say &quot;fast&quot; in this article, I will mean &quot;linear&quot;.)
      Leo's article had been almost completely ignored --
      my project (&lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;Marpa&lt;/a&gt;)
      would become its first
      practical implementation.
    &lt;/p&gt;
    &lt;h3&gt;Second-order languages&lt;/h3&gt;
    &lt;p&gt;The implications of Leo's discovery go well beyond speed.
      If you can rely on the BNF that you write always producing
      a practical parser, you can auto-generate your language.
      In fact,
      you can write languages which write languages.
    &lt;/p&gt;
    &lt;h3&gt;Which languages are fast?&lt;/h3&gt;
    &lt;p&gt;The Leo/Earley algorithm is not fast
      for every BNF-expressible language.
      BNF is powerful, and you can write exponentially
      ambiguous languages in it.
      But programmers these days
      mostly care about unambiguous languages --
      we are accustomed to tools and techniques
      that parse only a subset of these.
    &lt;/p&gt;
    &lt;p&gt;
      As I've said, Marpa is fast for every language
      class in practical use today.
      Marpa is almost certainly fast for any language
      that a modern programmer has in mind.
      Unless you peek ahead at the hints I am about to give you,
      in fact, it is actually
      &lt;b&gt;hard&lt;/b&gt;
      to write an unambiguous
      grammar that goes non-linear on Marpa.
      Simply mixing up lots of left, right and middle recursions
      will
      &lt;b&gt;not&lt;/b&gt;
      be enough to make an
      unambiguous grammar go non-linear.
      You will also need to violate a rule
      in the set that
      I am about to give you.
    &lt;/p&gt;
    &lt;p&gt;To guarantee that Marpa is fast for your BNF language,
      follow three rules:
    &lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Rule 1: Your BNF must be unambiguous.
      &lt;/li&gt;
      &lt;li&gt;Rule 2: Your BNF must have no &quot;unmarked&quot; middle recursions.
      &lt;/li&gt;
      &lt;li&gt;Rule 3: All of the right-recursive symbols
        in your BNF must be dedicated
        to right recursion.
      &lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;Rule 3 turns out to be very easy to obey.
      I discuss it in detail in the next section,
      which will be about how to break these rules and
      get away with it.
    &lt;/p&gt;
    &lt;p&gt;Before we do that,
      let's look at what an &quot;unmarked&quot; middle recursion is.
      Here's an example of a &quot;marked&quot; middle recursion:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       M ::= 'b'
       M ::= 'a' M 'a'
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      Here the &quot;b&quot; symbol is the marker.
      This marked middle recursion generates sequences like
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       b
       a b a
       a a b a a
    &lt;/pre&gt;&lt;/blockquote&gt;
    &lt;p&gt;Now for an &quot;unmarked&quot; middle recursion:
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       M ::= 'a' 'a'
       M ::= 'a' M 'a'
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      This unmarked middle recursion generates sequences like
    &lt;/p&gt;&lt;blockquote&gt;&lt;pre&gt;
       a a
       a a a a
       a a a a a a
    &lt;/pre&gt;&lt;/blockquote&gt;&lt;p&gt;
      In this middle recursion there is no marker.
      To know where the middle is,
      you have to scan all the way to the end,
      and then count back.
    &lt;/p&gt;
    &lt;p&gt;A rule of thumb is that if you can &quot;eyeball&quot; the middle
      of a long sequence,
      the recursion is marked.
      If you can't, it is unmarked.
      Unfortunately, we can't characterize exactly what a marker
      must look like -- a marker can encode the moves of a Turing machine,
      so marker detection is undecidable.
    &lt;/p&gt;
    &lt;h3&gt;How to get away with breaking the rules&lt;/h3&gt;
    &lt;p&gt;The rules about ambiguity and recursions are &quot;soft&quot;.
      If you only use limited ambiguity and
      keep your rule-breaking recursions short,
      your parser will stay fast.
    &lt;/p&gt;
    &lt;p&gt;Above, I promised to explain rule 3, which insisted that
      a right recursive symbol be &quot;dedicated&quot;.
      A right recursive symbol is &quot;dedicated&quot; if it appears only
      as the recursive symbol in a right recursion.
      If your grammar is unambiguous, but you've used an &quot;undedicated&quot;
      right-recursive symbol, that is easy to fix.
      Just rewrite the grammar, replacing the &quot;undedicated&quot; symbol
      with two different symbols.
      Dedicate one of the two to the right recursion,
      and use the other symbol everywhere else.
    &lt;/p&gt;
    &lt;h3&gt;When NOT to use Marpa&lt;/h3&gt;
    &lt;p&gt;The languages I have described as &quot;fast&quot; for Marpa
      include all those in practical use and many more.
      But do you really want to use Marpa for all of them?
      Here are four cases for which Marpa is probably not
      your best alternative.
    &lt;/p&gt;
    &lt;p&gt;The first case: a language that parses easily with a regular
      expression.
      The regular expression will be much faster.
      Don't walk away from a good thing.
    &lt;/p&gt;
    &lt;p&gt;The second case:
      a language
      that is easily parsed using a single
      loop and some state that fits into constant space.
      This parser might be very easy to write and maintain.
      If you are using a much slower higher level language,
      Marpa's optimized C language
      may be a win on CPU speed.
      But, as before, if you have a good thing,
      don't walk away from it.
    &lt;/p&gt;&lt;p&gt;The third case:
      a variation on the second.
      Here your single loop might be getting out of hand,
      making you yearn for the syntax-driven convenience
      of Marpa,
      but your state still fits into constant space.
      In its current implementation, Marpa keeps all of its
      parse tables forever, so Marpa does
      &lt;b&gt;not&lt;/b&gt;
      parse in constant space.
      Keeping the tables
      allows Marpa to deal with the full structure of its
      input, in a way that a SAX-ish approaches cannot.
      But if space requirements are an issue,
      and your application allows a simplified constant-space
      approach,
      Marpa's power and convenience may not be enough to
      make up for that.
    &lt;/p&gt;
    &lt;p&gt;The fourth case:
      a language that
    &lt;/p&gt;&lt;ul&gt;
      &lt;li&gt;is very small;
      &lt;/li&gt;
      &lt;li&gt;changes slowly or not at all, and does not grow in complexity;
      &lt;/li&gt;
      &lt;li&gt;merits careful hand-optimization, and has available the staff
        to do it;
      &lt;/li&gt;
      &lt;li&gt;merits and has available the kind of on-going support that will
        keep your code optimized under changing circumstances; and
      &lt;/li&gt;
      &lt;li&gt;is easily parseable via recursive descent:&lt;/li&gt;
    &lt;/ul&gt;
    &lt;p&gt;
      It is rare that all of these are the case,
      but when that happens,
      recursive descent is often preferable to Marpa.
      Lua and JSON
      are two languages which meet the above criteria.
      In Lua's case, it targets platforms with very restricted memories,
      which is an additional reason to prefer recursive descent --
      Marpa has a relatively large footprint.
    &lt;/p&gt;
    &lt;p&gt;It was not good luck that made
      both Lua and JSON good targets for recursive descent --
      they were designed around its limits.
      JSON is a favorite test target of Marpa for just these reasons.
      There are carefully hand-optimized C language parsers for us to
      benchmark against.
    &lt;/p&gt;
    &lt;p&gt;We get closer and closer,
      but Marpa will
      never beat small hand-optimized JSON parsers in software.
      However, while recursive descent is a technique for hand-writing parsers,
      Marpa is a mathematical algorithm.
      Someday,
      instructions for manipulating Earley items could be implemented directly
      in silicon.
      When and if that day comes,
      Earley's algorithm will beat recursive descent even at
      parsing the grammars that were designed for it.
    &lt;/p&gt;
    &lt;h3&gt;Comments&lt;/h3&gt;
    &lt;p&gt;Comments on this post can be made in
      &lt;a href=&quot;http://groups.google.com/group/marpa-parser&quot;&gt;
        Marpa's Google group&lt;/a&gt;,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      &lt;a href=&quot;http://savage.net.au/Marpa.html&quot;&gt;the
        official web site maintained by Ron Savage&lt;/a&gt;.
      I also have
      &lt;a href=&quot;http://jeffreykegler.github.io/Marpa-web-site/&quot;&gt;a Marpa web site&lt;/a&gt;.
    &lt;/p&gt;
  &lt;/body&gt;
&lt;/html&gt;</description>
  </item>
  </channel>
</rss>
