<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title>How to parse HTML, part 3</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2011/12/14#how-to-parse-html-part-3</link>
    <description>&lt;p&gt;
When a solution has the same shape
as the problem,
it is a very good thing,
and not just because it looks pretty.
In
&lt;a href=&quot;#PREVIOUS&quot;&gt;previous posts&lt;/a&gt;,
I have described
&lt;a href=&quot;https://metacpan.org/module/Marpa::HTML&quot;&gt;Marpa::HTML&lt;/a&gt;,
a Marpa-based, &quot;Ruby Slippers&quot;
approach to parsing liberal
and defective HTML.
A major advantage
of
&lt;a href=&quot;https://metacpan.org/module/Marpa::HTML&quot;&gt;Marpa::HTML&lt;/a&gt;
is that it looks like
the problem it solves.
&lt;/p&gt;
&lt;h2&gt;HTML parsing: the problem&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
The problem of parsing an HTML document
is essentially
the problem of finding
the hierarchy of its HTML elements.
Conceptually,
HTML elements are delimited by start and end tags.
&lt;li&gt;
The HTML standards specify that certain of the
start and end tags can be omitted.

&lt;li&gt;
In liberal and defective HTML,
any HTML tag might be missing.
&lt;li&gt;
In liberal and defective HTML,
unknown and spurious tags
may be present in the physical input.
&lt;/ol&gt;
&lt;h2&gt;HTML parsing: the solution&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
The parse engine uses an over-strict grammar,
one which requires all HTML start and end tags.
&lt;li&gt;
When the parse engine runs into a token it cannot accept,
if there is exactly one start or end tag which it
could accept at that point,
the parser uses &quot;the Ruby Slippers&quot;.
It invents a virtual token representing the desired
tag, and feeds it to the parse engine.
&lt;li&gt;
If there is more than one virtual token is possible,
&lt;a href=&quot;https://metacpan.org/module/Marpa::HTML&quot;&gt;Marpa::HTML&lt;/a&gt;
chooses a token to pass on to the parse engine.
In the current
implementation,
this is done using rules of thumb.

&lt;li&gt;
If no virtual token is possible,
the physical token is treated as &quot;cruft&quot;.
The grammar allows cruft to be a part of the contents
of any HTML element,
and the application can decide what to do
with it.
&lt;/ol&gt;
&lt;p&gt;
This outline of the solution
follows the structure of the problem point for point.
In turn, the code follows this outline.
It may seem
that I just stated the painfully obvious,
but in fact the design of
the parsers in use today typically
does NOT reflect the structure of their target languages
in any straightforward way.
In particular, the more a parser is
considered &quot;production quality&quot;,
the less likely its code will bear any resemblance to
the problem it is solving.
&lt;/p&gt;
&lt;h2&gt;Toward hackable parsers&lt;/h2&gt;
&lt;p&gt;
A lot could be said about the
aesthetics and philosophy of this.
In this post,
let me cut straight to the bottom line.
&lt;/p&gt;
&lt;p&gt;
First and least important,
it is usually easier to code a solution which looks like the problem.
I say &quot;least important,&quot;
because this perspective views the problem as static,
and if the problem is static you can code it up
and forget it.
It does not matter too much whether
the coding effort is fast,
if it only has to be done once.
But what if the problem keeps changing?
&lt;/p&gt;
&lt;p&gt;
You might say that most parsing is of the static type,
and that's true.
But that is
because previous technology has left little
choice in the matter.
I believe that,
if programmers had the option of hacking production-quality
parsers, they'd be doing it all the time.
&lt;/p&gt;

&lt;p&gt;
In the past,
hacking production quality parsers has been,
for practical purposes, impossible.
Look at those existing utilities which do work with, for
example, C, HTML or Perl.
These usually do NOT even attempt to leverage the production parser
for these languages.
Instead these tools use a new parser,
one created from scratch.
One consequence is that
they must tolerate a considerable amount
of approximation in the parsing.
&lt;/p&gt;
&lt;p&gt;
Why don't programmers take the production parsers for a language
as the basis for tools working with that language?
If you look at those production parsers,
you'll see why.
They reflect the structure of the languages so little,
and are so complex,
that they simply are unusable as a starting point
for tools.
&lt;p&gt;
&lt;/p&gt;
A Marpa-powered &quot;Ruby Slippers&quot; approach to HTML,
like the one implemented in 
&lt;a href=&quot;https://metacpan.org/module/Marpa::HTML&quot;&gt;Marpa::HTML&lt;/a&gt;
but with its HTML interpretation layer rewritten in C,
would be very competitive as a production HTML parser.
Not the least of its advantages would be that it would make
an excellent basis for HTML utilities.
&lt;p&gt;
&lt;h2&gt;Notes&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;a name=&quot;PREVIOUS&quot;&gt;&quot;previous posts&quot;&lt;/a&gt;:
The previous posts in this series were
&quot;&lt;a href=&quot;http://blogs.perl.org/users/jeffrey_kegler/2011/11/how-to-parse-html.html&quot;&gt;How
to parse HTML&lt;/a&gt;&quot;
and 
&quot;&lt;a href=&quot;http://blogs.perl.org/users/jeffrey_kegler/2011/12/how-to-parse-html-part-2.html&quot;&gt;How
to parse HTML, part 2&lt;/a&gt;&quot;.
&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description>
  </item>
  </channel>
</rss>
