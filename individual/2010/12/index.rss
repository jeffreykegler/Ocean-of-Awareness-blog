<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title> Why the Bovicidal Rage? (Killing Yacc: 4)</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/12/26#why-the-bovicidal-rage-killing-yacc-4</link>
    <description>&lt;p&gt;
&lt;a href=&quot;http://www.flickr.com/photos/publicenergy/3299967437/in/set-72157614350778728/&quot;&gt;&lt;img alt=&quot;3299967437_6bae3ce6a8_z.jpg&quot; src=&quot;http://blogs.perl.org/users/jeffrey_kegler/3299967437_6bae3ce6a8_z.jpg&quot; width=&quot;640&quot; height=&quot;426&quot; class=&quot;mt-image-none&quot; style=&quot;&quot; /&gt;&lt;/a&gt;
&lt;kbd&gt;yacc&lt;/kbd&gt;
was a
major breakthrough.
For the first time, automatic generation of
of efficient, production-quality parsers was possible
for languages of practical interest.
Yacc-generated parsers had reasonable memory footprints.
They ran in linear time.

&lt;p&gt;&lt;a href=&quot;#NOTE1&quot;&gt;But error reporting was overlooked.&lt;/a&gt;
Then as now, the focus in analyzing algorithms was on power
-- what kinds of grammar an algorithm can parse --
and on resource consumption.
This leaves out something big.

&lt;p&gt;Our frameworks for analyzing things affect what we believe.
We find it hard to recognize a problem if our
framework makes us unable to  articulate it.
Complaints about &lt;var&gt;yacc&lt;/var&gt; tended to be kept to oneself.
But while &lt;var&gt;yacc&lt;/var&gt;'s
overt reputation flourished,
programmers were undergoing an almost Pavlovian
conditioning against it --
a conditioning through pain.

&lt;p&gt;With &lt;var&gt;yacc&lt;/var&gt;, you no longer need to write
your own parser.
If you put your grammar into a form that &lt;var&gt;yacc&lt;/var&gt; will accept,

&lt;var&gt;yacc&lt;/var&gt; writes your parser for you.
But over the years, people noticed -- it usually takes longer
to put a grammar into a form that &lt;var&gt;yacc&lt;/var&gt; will accept
than it does to write a recursive descent parser from scratch.
This is almost always true when someone was using &lt;var&gt;yacc&lt;/var&gt;
for the first time.
&lt;a href=&quot;#NOTE2&quot;&gt;
But it is usually true for &lt;var&gt;yacc&lt;/var&gt;
experts as well.&lt;/a&gt;

&lt;p&gt;Certain tools do have a high initial cost.
They make this acceptable with a payoff over
the lifetime of the software.
But in this respect also,

&lt;var&gt;yacc&lt;/var&gt; does worse than hand-written recursive descent --
much worse.

&lt;p&gt;The structure of a recursive descent parser reflects the structure
of the language being parsed.
Small changes in the language tend to require only small changes in the parser.
Major changes usually affect only that part of the grammar actually changed.
And how the recursive descent parser must change, and why,
is usually obvious even to a programmer
who does not make a specialty of parsing.

&lt;p&gt;
For a &lt;var&gt;yacc&lt;/var&gt; grammar, the change process
is not incremental -- each iteration is almost like
starting from scratch.
&lt;var&gt;yacc&lt;/var&gt; parsers work using LALR automata.
Small changes in the grammar often cause big changes to the LALR states.
Tracing the logic behind these changes is a challenge
even to those
familiar with the underlying math --
one which they usually find not worth the time and effort.
This is one reason that those experienced with &lt;var&gt;yacc&lt;/var&gt; find it nearly
as hard to use as beginners did -- for real-world problems,
understanding the LALR states is simply too hard for anyone.
Experts, like non-experts, are forced to fix &lt;var&gt;yacc&lt;/var&gt;

grammars using trial and error.

&lt;p&gt;&lt;var&gt;yacc&lt;/var&gt;'s demands
follow the nearly incomprehensible logic of the LALR
automaton.
Adapting a grammar to &lt;var&gt;yacc&lt;/var&gt; is a
struggle, during which you
watch your grammar become less and less a
reflection of what you were originally trying to do.
One of the most difficult tasks
in programming, it is almost one of the most unsatisifying
and unrewarding.

&lt;h2&gt;The Fourth Requirement for Replacing &lt;var&gt;yacc&lt;/var&gt;:
Easy Diagnosis of Grammar Problems&lt;/h2&gt;

&lt;p&gt;The easiest way to deal with grammar problems is to arrange for them
not to happen.
You can do this if you have a notation for the grammar which is
&lt;ol&gt;
&lt;li&gt;A natural and intuitive way to express the grammar, and which
&lt;li&gt;Makes it literally impossible to specify a problem grammar.
&lt;/ol&gt;

&lt;p&gt;The notation for
&lt;a href=&quot;#NOTE3&quot;&gt;regular expressions&lt;/a&gt;
has these two properties,
and that is a major reason for the enduring popularity
of regular expressions.
Once you get used to its limits,
regular expression notation becomes a natural way to
describe languages.
And regular expression notation
makes it impossible to specify
anything that is &lt;b&gt;not&lt;/b&gt; a regular expression.

&lt;p&gt;More powerful parsing algorithms have these same two properties
when they accept all context-free grammars (as
&lt;a href=&quot;http://search.cpan.org/dist/Marpa/&quot;&gt;Marpa&lt;/a&gt;
does).
The context-free grammars are those you can write in BNF, and vice versa.
BNF is  (again, modulo some getting-used-to) a natural and intuitive way
to express a language, and you don't have to worry about specifying a language
which is harder than context-free -- the BNF notation makes that impossible.

&lt;p&gt;&lt;var&gt;yacc&lt;/var&gt; uses BNF as its notation
for expressing grammars, but the most natural way to express a practical
grammar in BNF is almost never an LALR grammar.
No natural or intutive notation is known
that describes only LALR grammars, even after 40 years of
considerable interest in them.
I have to doubt that such a notation ever will be found.

&lt;h2&gt;Notes&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;NOTE1&quot;&gt;Note 1: &lt;/a&gt;
It has definitely been the tradition to
understate the importance of error reporting,
or even to ignore it entirely.
But I should point out
that I have not consulted Knuth's and DeRemer's original papers,
which are behind paywalls.
Also, things seem to be getting better, particularly with the arrival
of the
&lt;a href=&quot;http://www.few.vu.nl/~dick/PT2Ed.html&quot;&gt;
very important textbook by Grune and Jacobs&lt;/a&gt;,
which does devote significant attention to the error
reporting properties of the various algorithms.

&lt;p&gt;&lt;a name=&quot;NOTE2&quot;&gt;Note 2: &lt;/a&gt;
Perhaps because reporting that you found it impossible to
use one of the standard
tools in our field
was more likely to produce conclusions
about your competence than about the tool,
the tradition among &lt;var&gt;yacc&lt;/var&gt; users was to suffer in silence.
One good 1995 account of trials with &lt;var&gt;yacc&lt;/var&gt; is
&lt;a href=&quot;http://groups.google.com/group/comp.compilers/msg/a5d260aa50c05685?hl=en&amp;dmode=source&quot;&gt;
this contribution to comp.compilers&lt;/a&gt;.
And there is one person who I
believe has an intuitive understanding of LALR: Larry Wall.
Certainly I doubt there is anyone alive whose practical knowledge
of LALR is better than Larry's.
That makes it very significant that
Perl 6 does not use

&lt;var&gt;yacc&lt;/var&gt; or any other LALR based parser.

&lt;p&gt;&lt;a name=&quot;NOTE3&quot;&gt;Note 3: &lt;/a&gt;
Here I am talking about pure regular expressions.
Perl regexes are much more powerful than regular expressions,
and the power comes with tradeoffs:
The notation is no longer
as simple or intuitive.</description>
  </item>
  <item>
    <title>Killing Yacc: 1, 2 &amp; 3</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/12/15#killing-yacc-1-2-3</link>
    <description>&lt;h2&gt;The Good, the Bad and The Ugly&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&quot;Angeleyescleef.jpg&quot; src=&quot;http://blogs.perl.org/users/jeffrey_kegler/Angeleyescleef.jpg&quot; width=&quot;410&quot; height=&quot;243&quot; class=&quot;mt-image-right&quot; style=&quot;float: right; margin: 0 0 20px 20px;&quot; /&gt;The recent discussions about yacc made me
feel a bit like Lee Van Cleef in an old
spaghetti Western.
Cast alongside Clint Eastwood, Van Cleef watched
with great concern as one attempt after another
was made on Eastwood's life.
Van Cleef didn't mind Eastwood getting killed --
he just wanted to be the one to do it.

&lt;p&gt;As some of you will have recognized, I am talking about
a very interesting discussion started by
&lt;a href=&quot;http://arxiv.org/abs/1010.5023&quot;&gt;
a paper
by Might and Darais
entitled &quot;Yacc is Dead&quot;&lt;/a&gt;.

I am finding it very much worth reading as an example
of clear and precise mathematical writing.
With respect to the parser itself,
my opinion is that
&lt;a href=&quot;http://research.swtch.com/2010/12/yacc-is-not-dead.html&quot;&gt;
Russ Cox's extremely well-informed
blog post, &quot;Yacc is not Dead&quot;,&lt;/a&gt;
is an accurate assessment.

&lt;p&gt;Might, Darais and Cox all devote considerable attention to
exactly what it will take to send yacc on to its Final
Reward.
I see six requirements:
Three from Might &amp; Darais, one suggested by
Cox, and two that I have added.
The rest of this post will be about three of these requirements,
all of which focus on parsing speed.

&lt;h2&gt;Requirement 1: Handle Arbitrary Context-Free Grammars in O(n**3)&lt;/h2&gt;

&lt;p&gt;For a parser to be convenient, it should take anything you can write in BNF
and parse it.
And it should do this in &quot;reasonable&quot; time.
This enables a programmer to work on a grammar that does not fit a
restricted theoretical framework (LL, LR, LALR, etc.).
The programmer then has the choice:
she can tighten the grammar up to make it faster,
or she can decide that, for her application,
worse-than-linear speed is acceptable.

&lt;p&gt;This requirement is one of those in the Might-Darais paper, but Russ Cox
adds a further requirement:
&quot;Reasonable time&quot; means O(n**3).
Might and Darais do not categorize their algorithm's speed for arbitrary context-free
grammars, but Cox says that it is exponential (O(e**n)).

&lt;p&gt;Cox's tightening of this requirement makes sense.
Depending on the application, exponential time can make a grammar unuseable
in practice.
Several algorithms are known which parse arbitrary context-free grammars in
O(n**3).
&lt;a href=&quot;http://search.cpan.org/dist/Marpa/&quot;&gt;
Marpa&lt;/a&gt;
is one of these.
(Marpa, for those new to this blog, is a parsing algorithm I've been working on.
It is based on Earley's algorithm, and includes several major enhancements to
it from the academic literature.)

&lt;h2&gt;Requirement 2: Handle &quot;Average&quot; Grammars in Linear Time&lt;/h2&gt;
&lt;p&gt;When it needs to parse long inputs, an algorithm has to run in linear (O(n)) time.
&quot;Average&quot; grammars -- grammars for languages where the inputs are expected to be long --
should parse in linear time.
For example, in production programming, the code files can become quite large.
Perhaps HTML files should not be huge, but they often are.
And it is quite reasonable to expect XML files to be arbitrarily long.
For all of these, and for any language expected to fit into the same kinds
of production environment, linear-time parsing is a must.
This requirement is based on the second requirement in the Might-Darais paper.
Might and Darais only say that such parsing should be &quot;efficient&quot;.
Russ Cox is more specific: he requires that the time be linear.


&lt;p&gt;Marpa's behavior for &quot;average&quot; grammars is excellent, for any reasonable
definition of &quot;average&quot;.
Marpa parses all LR-regular grammars in linear time.
LR-regular is a huge class of grammars, and it includes the LR(k) grammars
for all values of k.
In practical terms, this means that, if a grammar is parseable by recursive descent,
by yacc, or by a regular expression, then it is parsed by Marpa in linear time.

&lt;h2&gt;Requirement 3: A Sound Theoretical Basis&lt;/h2&gt;
&lt;p&gt;This third requirement is Russ Cox's, and it is a real insight on his part.
A sound theoretical basis is more important than it may seem.
Over the years I have seen many new parsing algorithms introduced, only to
disappear.
The algorithms which drop from sight 
are those whose speed claims are based on speculation
and/or initial tests, but not on theory.

&lt;p&gt;You might think that
testing can replace theory, but typically it can't.
The only real way to test
that a new algorithm is efficient enough for production is to use it in
production.
Few production compiler writers are going to risk the use of a new algorithm
before there is solid theory to back their leap of faith.

&lt;p&gt;The Might-Darais paper is
beautifully written mathematics.
But, as Russ Cox notes,
nowhere does it characterize its speed claims in mathematical terms.
We know that the Might-Darais algorithm will parse some grammars very quickly.
We know that, for others, it will be painfully slow.
Which ones will be which?

&lt;p&gt;Marpa does well in
backing up its speed claims.
Marpa is an Earley parser.
In his 40-year old paper,
Jay Earley proved that his algorithm parses
arbitrary context-free grammars in O(n**3) time.
Marpa incorporates Joop Leo's 1991 modification of the Earley parser.
In that paper, Leo proves that his algorithm
parses LR-regular grammars in linear time.
When it comes to speed claims, Marpa is doing things by the book.</description>
  </item>
  <item>
    <title>Miniminalism: Philosophy, Tradition or Lunacy?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/12/04#miniminalism-philosophy-tradition-or-lunacy</link>
    <description>&lt;p&gt;Thanks to cpantesters I'm doing something that amazes me --
as I write C, it is taken and tested for me on all sorts
of different systems.
This is a stern test of my ability to write
portable C code,
and, in general I'm quite pleased with how I've done.
So far only one mistake
can clearly be laid
at my door.

&lt;p&gt;That mistake doesn't put me in a good light --
mistakes seldom do.
But we learn more from mistakes than from successes.
In this case, the mistake exposes a habit of mine that
programmers of the modern school will find a bit lunatic.
And perhaps it will give me a good way to point out how
that lunacy shaped your training as well,
no matter how recent it was.

&lt;p&gt;The cpantesters do wonderful things, but they can't be
all things to all people.
In their feedback to C programmers who cause segment violations,
they make the oracle at Delphi look like the neighborhood gossip.
What the C programmer who lets his pointers stray
sees is almost nothing -- the &lt;var&gt;prove&lt;/var&gt; utility will
report every test after the segment violation as
a failure, without further explanation.
It's not even explicitly stated that the problem is
a segment violation, but I've learned to suspect that
is what is happening
whenever &lt;var&gt;prove&lt;/var&gt; gives me the silent treatment.

&lt;p&gt;I could always contact the cpantester
and ask him for a stack trace,
or even send him additional tests to run.
But he's a volunteer and my C programming mishaps
will not necessarily be his highest priority.
He may be especially disinclined to give the issue a priority
if he suspects that I have not done my homework.
It behooved me to
try to solve the problem on my own first.

&lt;p&gt;I got lucky.
Rereading my recent changes,
I noted where doing a fairly
complex calculation might save 4 bytes in a memory
allocation.
Problem was, I'd seen this the first time and
decided I could save 5 bytes.  Oops.

&lt;p&gt;On most systems,
&lt;var&gt;malloc()&lt;/var&gt; was rounding up the
allocation.
But on a strict few, I was given exactly the allocation I asked
for and made to pay the price of my folly.

&lt;p&gt;I replaced the calculation with a simpler one.
Cpantesters reports for my fix are in and they run 100% clean.
The simpler calculation for the &lt;var&gt;malloc()&lt;/var&gt; buffer
allocates, worst case, 4 bytes too many.
From the optimization standpoint, it may even be better
because it saves a few CPU cycles.
Not that either way
it makes a difference that could be measured.


&lt;p&gt;At this point,
a number of my readers will have decided that I'm a lunatic.
Why was I going out of my way to save 4 lousy bytes?
Without directly addressing the charge of lunacy,
I would draw the candid reader's attention
to my training.
Nobody is drilling young programmers
on the importance of saving 4 bytes any more,
but the spirit of minimalism that inspired my misjudgement
is more than alive today.
It's dominant.

&lt;p&gt;I started programming in 1970, remote
time-sharing on a PDP-8.
As a user, I had available a virtual PDP-8 in its
minimal configuration.
That was 4096 12-bit words: 6K in modern terms.
When you work in a memory that small, you sweat the details.

&lt;p&gt;Though I didn't think so at the time,
in giving me those 6K virtual bytes, the
operating system was being generous.
The typical configuration of a PDP-8 for the educational
market was 36K bytes of memory, both physical and virtual.
This was expected to serve around 20 time-shared users.

&lt;p&gt;It was the
generation that learned to program on this kind of hardware
which created
the concepts of programming language, development environment and operating system
which are dominant today.
As often as not they created not just our current concepts,
but the actual technology we are using.

&lt;p&gt;Perhaps one example will inspire the reader to think of others.
LISP remains influential.
Its spirit was the flame that kindled a long series
of single-paradigm languages.
But given the size of the machines available in 1958, how many
paradigms could a language have implemented?
When we study and emulate LISP,
we are studying and emulating deep ideas.
To an equal extent, however, when we study and emulate
LISP, we are studying the art
of saving every byte as if it were our last.</description>
  </item>
  </channel>
</rss>
