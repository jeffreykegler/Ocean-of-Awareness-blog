<?xml version="1.0"?>
<!-- name="generator" content="blosxom/2.0" -->
<!DOCTYPE rss PUBLIC "-//Netscape Communications//DTD RSS 0.91//EN" "http://my.netscape.com/publish/formats/rss-0.91.dtd">

<rss version="0.91">
  <channel>
    <title>Ocean of Awareness   </title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog</link>
    <description>Ocean of Awareness.</description>
    <language>en</language>

  <item>
    <title> Why the Bovicidal Rage? (Killing Yacc: 4)</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/12/26#why-the-bovicidal-rage-killing-yacc-4</link>
    <description>&lt;p&gt;
&lt;a href=&quot;http://www.flickr.com/photos/publicenergy/3299967437/in/set-72157614350778728/&quot;&gt;&lt;img alt=&quot;3299967437_6bae3ce6a8_z.jpg&quot; src=&quot;http://blogs.perl.org/users/jeffrey_kegler/3299967437_6bae3ce6a8_z.jpg&quot; width=&quot;640&quot; height=&quot;426&quot; class=&quot;mt-image-none&quot; style=&quot;&quot; /&gt;&lt;/a&gt;
&lt;kbd&gt;yacc&lt;/kbd&gt;
was a
major breakthrough.
For the first time, automatic generation of
of efficient, production-quality parsers was possible
for languages of practical interest.
Yacc-generated parsers had reasonable memory footprints.
They ran in linear time.

&lt;p&gt;&lt;a href=&quot;#NOTE1&quot;&gt;But error reporting was overlooked.&lt;/a&gt;
Then as now, the focus in analyzing algorithms was on power
-- what kinds of grammar an algorithm can parse --
and on resource consumption.
This leaves out something big.

&lt;p&gt;Our frameworks for analyzing things affect what we believe.
We find it hard to recognize a problem if our
framework makes us unable to  articulate it.
Complaints about &lt;var&gt;yacc&lt;/var&gt; tended to be kept to oneself.
But while &lt;var&gt;yacc&lt;/var&gt;'s
overt reputation flourished,
programmers were undergoing an almost Pavlovian
conditioning against it --
a conditioning through pain.

&lt;p&gt;With &lt;var&gt;yacc&lt;/var&gt;, you no longer need to write
your own parser.
If you put your grammar into a form that &lt;var&gt;yacc&lt;/var&gt; will accept,

&lt;var&gt;yacc&lt;/var&gt; writes your parser for you.
But over the years, people noticed -- it usually takes longer
to put a grammar into a form that &lt;var&gt;yacc&lt;/var&gt; will accept
than it does to write a recursive descent parser from scratch.
This is almost always true when someone was using &lt;var&gt;yacc&lt;/var&gt;
for the first time.
&lt;a href=&quot;#NOTE2&quot;&gt;
But it is usually true for &lt;var&gt;yacc&lt;/var&gt;
experts as well.&lt;/a&gt;

&lt;p&gt;Certain tools do have a high initial cost.
They make this acceptable with a payoff over
the lifetime of the software.
But in this respect also,

&lt;var&gt;yacc&lt;/var&gt; does worse than hand-written recursive descent --
much worse.

&lt;p&gt;The structure of a recursive descent parser reflects the structure
of the language being parsed.
Small changes in the language tend to require only small changes in the parser.
Major changes usually affect only that part of the grammar actually changed.
And how the recursive descent parser must change, and why,
is usually obvious even to a programmer
who does not make a specialty of parsing.

&lt;p&gt;
For a &lt;var&gt;yacc&lt;/var&gt; grammar, the change process
is not incremental -- each iteration is almost like
starting from scratch.
&lt;var&gt;yacc&lt;/var&gt; parsers work using LALR automata.
Small changes in the grammar often cause big changes to the LALR states.
Tracing the logic behind these changes is a challenge
even to those
familiar with the underlying math --
one which they usually find not worth the time and effort.
This is one reason that those experienced with &lt;var&gt;yacc&lt;/var&gt; find it nearly
as hard to use as beginners did -- for real-world problems,
understanding the LALR states is simply too hard for anyone.
Experts, like non-experts, are forced to fix &lt;var&gt;yacc&lt;/var&gt;

grammars using trial and error.

&lt;p&gt;&lt;var&gt;yacc&lt;/var&gt;'s demands
follow the nearly incomprehensible logic of the LALR
automaton.
Adapting a grammar to &lt;var&gt;yacc&lt;/var&gt; is a
struggle, during which you
watch your grammar become less and less a
reflection of what you were originally trying to do.
One of the most difficult tasks
in programming, it is almost one of the most unsatisifying
and unrewarding.

&lt;h2&gt;The Fourth Requirement for Replacing &lt;var&gt;yacc&lt;/var&gt;:
Easy Diagnosis of Grammar Problems&lt;/h2&gt;

&lt;p&gt;The easiest way to deal with grammar problems is to arrange for them
not to happen.
You can do this if you have a notation for the grammar which is
&lt;ol&gt;
&lt;li&gt;A natural and intuitive way to express the grammar, and which
&lt;li&gt;Makes it literally impossible to specify a problem grammar.
&lt;/ol&gt;

&lt;p&gt;The notation for
&lt;a href=&quot;#NOTE3&quot;&gt;regular expressions&lt;/a&gt;
has these two properties,
and that is a major reason for the enduring popularity
of regular expressions.
Once you get used to its limits,
regular expression notation becomes a natural way to
describe languages.
And regular expression notation
makes it impossible to specify
anything that is &lt;b&gt;not&lt;/b&gt; a regular expression.

&lt;p&gt;More powerful parsing algorithms have these same two properties
when they accept all context-free grammars (as
&lt;a href=&quot;http://search.cpan.org/dist/Marpa/&quot;&gt;Marpa&lt;/a&gt;
does).
The context-free grammars are those you can write in BNF, and vice versa.
BNF is  (again, modulo some getting-used-to) a natural and intuitive way
to express a language, and you don't have to worry about specifying a language
which is harder than context-free -- the BNF notation makes that impossible.

&lt;p&gt;&lt;var&gt;yacc&lt;/var&gt; uses BNF as its notation
for expressing grammars, but the most natural way to express a practical
grammar in BNF is almost never an LALR grammar.
No natural or intutive notation is known
that describes only LALR grammars, even after 40 years of
considerable interest in them.
I have to doubt that such a notation ever will be found.

&lt;h2&gt;Notes&lt;/h2&gt;

&lt;p&gt;&lt;a name=&quot;NOTE1&quot;&gt;Note 1: &lt;/a&gt;
It has definitely been the tradition to
understate the importance of error reporting,
or even to ignore it entirely.
But I should point out
that I have not consulted Knuth's and DeRemer's original papers,
which are behind paywalls.
Also, things seem to be getting better, particularly with the arrival
of the
&lt;a href=&quot;http://www.few.vu.nl/~dick/PT2Ed.html&quot;&gt;
very important textbook by Grune and Jacobs&lt;/a&gt;,
which does devote significant attention to the error
reporting properties of the various algorithms.

&lt;p&gt;&lt;a name=&quot;NOTE2&quot;&gt;Note 2: &lt;/a&gt;
Perhaps because reporting that you found it impossible to
use one of the standard
tools in our field
was more likely to produce conclusions
about your competence than about the tool,
the tradition among &lt;var&gt;yacc&lt;/var&gt; users was to suffer in silence.
One good 1995 account of trials with &lt;var&gt;yacc&lt;/var&gt; is
&lt;a href=&quot;http://groups.google.com/group/comp.compilers/msg/a5d260aa50c05685?hl=en&amp;dmode=source&quot;&gt;
this contribution to comp.compilers&lt;/a&gt;.
And there is one person who I
believe has an intuitive understanding of LALR: Larry Wall.
Certainly I doubt there is anyone alive whose practical knowledge
of LALR is better than Larry's.
That makes it very significant that
Perl 6 does not use

&lt;var&gt;yacc&lt;/var&gt; or any other LALR based parser.

&lt;p&gt;&lt;a name=&quot;NOTE3&quot;&gt;Note 3: &lt;/a&gt;
Here I am talking about pure regular expressions.
Perl regexes are much more powerful than regular expressions,
and the power comes with tradeoffs:
The notation is no longer
as simple or intuitive.</description>
  </item>
  <item>
    <title>Killing Yacc: 1, 2 &amp; 3</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/12/15#killing-yacc-1-2-3</link>
    <description>&lt;h2&gt;The Good, the Bad and The Ugly&lt;/h2&gt;
&lt;p&gt;&lt;img alt=&quot;Angeleyescleef.jpg&quot; src=&quot;http://blogs.perl.org/users/jeffrey_kegler/Angeleyescleef.jpg&quot; width=&quot;410&quot; height=&quot;243&quot; class=&quot;mt-image-right&quot; style=&quot;float: right; margin: 0 0 20px 20px;&quot; /&gt;The recent discussions about yacc made me
feel a bit like Lee Van Cleef in an old
spaghetti Western.
Cast alongside Clint Eastwood, Van Cleef watched
with great concern as one attempt after another
was made on Eastwood's life.
Van Cleef didn't mind Eastwood getting killed --
he just wanted to be the one to do it.

&lt;p&gt;As some of you will have recognized, I am talking about
a very interesting discussion started by
&lt;a href=&quot;http://arxiv.org/abs/1010.5023&quot;&gt;
a paper
by Might and Darais
entitled &quot;Yacc is Dead&quot;&lt;/a&gt;.

I am finding it very much worth reading as an example
of clear and precise mathematical writing.
With respect to the parser itself,
my opinion is that
&lt;a href=&quot;http://research.swtch.com/2010/12/yacc-is-not-dead.html&quot;&gt;
Russ Cox's extremely well-informed
blog post, &quot;Yacc is not Dead&quot;,&lt;/a&gt;
is an accurate assessment.

&lt;p&gt;Might, Darais and Cox all devote considerable attention to
exactly what it will take to send yacc on to its Final
Reward.
I see six requirements:
Three from Might &amp; Darais, one suggested by
Cox, and two that I have added.
The rest of this post will be about three of these requirements,
all of which focus on parsing speed.

&lt;h2&gt;Requirement 1: Handle Arbitrary Context-Free Grammars in O(n**3)&lt;/h2&gt;

&lt;p&gt;For a parser to be convenient, it should take anything you can write in BNF
and parse it.
And it should do this in &quot;reasonable&quot; time.
This enables a programmer to work on a grammar that does not fit a
restricted theoretical framework (LL, LR, LALR, etc.).
The programmer then has the choice:
she can tighten the grammar up to make it faster,
or she can decide that, for her application,
worse-than-linear speed is acceptable.

&lt;p&gt;This requirement is one of those in the Might-Darais paper, but Russ Cox
adds a further requirement:
&quot;Reasonable time&quot; means O(n**3).
Might and Darais do not categorize their algorithm's speed for arbitrary context-free
grammars, but Cox says that it is exponential (O(e**n)).

&lt;p&gt;Cox's tightening of this requirement makes sense.
Depending on the application, exponential time can make a grammar unuseable
in practice.
Several algorithms are known which parse arbitrary context-free grammars in
O(n**3).
&lt;a href=&quot;http://search.cpan.org/dist/Marpa/&quot;&gt;
Marpa&lt;/a&gt;
is one of these.
(Marpa, for those new to this blog, is a parsing algorithm I've been working on.
It is based on Earley's algorithm, and includes several major enhancements to
it from the academic literature.)

&lt;h2&gt;Requirement 2: Handle &quot;Average&quot; Grammars in Linear Time&lt;/h2&gt;
&lt;p&gt;When it needs to parse long inputs, an algorithm has to run in linear (O(n)) time.
&quot;Average&quot; grammars -- grammars for languages where the inputs are expected to be long --
should parse in linear time.
For example, in production programming, the code files can become quite large.
Perhaps HTML files should not be huge, but they often are.
And it is quite reasonable to expect XML files to be arbitrarily long.
For all of these, and for any language expected to fit into the same kinds
of production environment, linear-time parsing is a must.
This requirement is based on the second requirement in the Might-Darais paper.
Might and Darais only say that such parsing should be &quot;efficient&quot;.
Russ Cox is more specific: he requires that the time be linear.


&lt;p&gt;Marpa's behavior for &quot;average&quot; grammars is excellent, for any reasonable
definition of &quot;average&quot;.
Marpa parses all LR-regular grammars in linear time.
LR-regular is a huge class of grammars, and it includes the LR(k) grammars
for all values of k.
In practical terms, this means that, if a grammar is parseable by recursive descent,
by yacc, or by a regular expression, then it is parsed by Marpa in linear time.

&lt;h2&gt;Requirement 3: A Sound Theoretical Basis&lt;/h2&gt;
&lt;p&gt;This third requirement is Russ Cox's, and it is a real insight on his part.
A sound theoretical basis is more important than it may seem.
Over the years I have seen many new parsing algorithms introduced, only to
disappear.
The algorithms which drop from sight 
are those whose speed claims are based on speculation
and/or initial tests, but not on theory.

&lt;p&gt;You might think that
testing can replace theory, but typically it can't.
The only real way to test
that a new algorithm is efficient enough for production is to use it in
production.
Few production compiler writers are going to risk the use of a new algorithm
before there is solid theory to back their leap of faith.

&lt;p&gt;The Might-Darais paper is
beautifully written mathematics.
But, as Russ Cox notes,
nowhere does it characterize its speed claims in mathematical terms.
We know that the Might-Darais algorithm will parse some grammars very quickly.
We know that, for others, it will be painfully slow.
Which ones will be which?

&lt;p&gt;Marpa does well in
backing up its speed claims.
Marpa is an Earley parser.
In his 40-year old paper,
Jay Earley proved that his algorithm parses
arbitrary context-free grammars in O(n**3) time.
Marpa incorporates Joop Leo's 1991 modification of the Earley parser.
In that paper, Leo proves that his algorithm
parses LR-regular grammars in linear time.
When it comes to speed claims, Marpa is doing things by the book.</description>
  </item>
  <item>
    <title>Miniminalism: Philosophy, Tradition or Lunacy?</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/12/04#miniminalism-philosophy-tradition-or-lunacy</link>
    <description>&lt;p&gt;Thanks to cpantesters I'm doing something that amazes me --
as I write C, it is taken and tested for me on all sorts
of different systems.
This is a stern test of my ability to write
portable C code,
and, in general I'm quite pleased with how I've done.
So far only one mistake
can clearly be laid
at my door.

&lt;p&gt;That mistake doesn't put me in a good light --
mistakes seldom do.
But we learn more from mistakes than from successes.
In this case, the mistake exposes a habit of mine that
programmers of the modern school will find a bit lunatic.
And perhaps it will give me a good way to point out how
that lunacy shaped your training as well,
no matter how recent it was.

&lt;p&gt;The cpantesters do wonderful things, but they can't be
all things to all people.
In their feedback to C programmers who cause segment violations,
they make the oracle at Delphi look like the neighborhood gossip.
What the C programmer who lets his pointers stray
sees is almost nothing -- the &lt;var&gt;prove&lt;/var&gt; utility will
report every test after the segment violation as
a failure, without further explanation.
It's not even explicitly stated that the problem is
a segment violation, but I've learned to suspect that
is what is happening
whenever &lt;var&gt;prove&lt;/var&gt; gives me the silent treatment.

&lt;p&gt;I could always contact the cpantester
and ask him for a stack trace,
or even send him additional tests to run.
But he's a volunteer and my C programming mishaps
will not necessarily be his highest priority.
He may be especially disinclined to give the issue a priority
if he suspects that I have not done my homework.
It behooved me to
try to solve the problem on my own first.

&lt;p&gt;I got lucky.
Rereading my recent changes,
I noted where doing a fairly
complex calculation might save 4 bytes in a memory
allocation.
Problem was, I'd seen this the first time and
decided I could save 5 bytes.  Oops.

&lt;p&gt;On most systems,
&lt;var&gt;malloc()&lt;/var&gt; was rounding up the
allocation.
But on a strict few, I was given exactly the allocation I asked
for and made to pay the price of my folly.

&lt;p&gt;I replaced the calculation with a simpler one.
Cpantesters reports for my fix are in and they run 100% clean.
The simpler calculation for the &lt;var&gt;malloc()&lt;/var&gt; buffer
allocates, worst case, 4 bytes too many.
From the optimization standpoint, it may even be better
because it saves a few CPU cycles.
Not that either way
it makes a difference that could be measured.


&lt;p&gt;At this point,
a number of my readers will have decided that I'm a lunatic.
Why was I going out of my way to save 4 lousy bytes?
Without directly addressing the charge of lunacy,
I would draw the candid reader's attention
to my training.
Nobody is drilling young programmers
on the importance of saving 4 bytes any more,
but the spirit of minimalism that inspired my misjudgement
is more than alive today.
It's dominant.

&lt;p&gt;I started programming in 1970, remote
time-sharing on a PDP-8.
As a user, I had available a virtual PDP-8 in its
minimal configuration.
That was 4096 12-bit words: 6K in modern terms.
When you work in a memory that small, you sweat the details.

&lt;p&gt;Though I didn't think so at the time,
in giving me those 6K virtual bytes, the
operating system was being generous.
The typical configuration of a PDP-8 for the educational
market was 36K bytes of memory, both physical and virtual.
This was expected to serve around 20 time-shared users.

&lt;p&gt;It was the
generation that learned to program on this kind of hardware
which created
the concepts of programming language, development environment and operating system
which are dominant today.
As often as not they created not just our current concepts,
but the actual technology we are using.

&lt;p&gt;Perhaps one example will inspire the reader to think of others.
LISP remains influential.
Its spirit was the flame that kindled a long series
of single-paradigm languages.
But given the size of the machines available in 1958, how many
paradigms could a language have implemented?
When we study and emulate LISP,
we are studying and emulating deep ideas.
To an equal extent, however, when we study and emulate
LISP, we are studying the art
of saving every byte as if it were our last.</description>
  </item>
  <item>
    <title>Better than Literate!</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/11/17#better-than-literate</link>
    <description>&lt;p&gt;I'm converting Marpa
to XS and have started to use Cweb.
Cweb is
the C version of the &quot;literate programming&quot; system pioneered
by Don Knuth.
I'm pleasantly surprised by it.
Cweb adds fun to the programming experience
and is helping in more ways than the phrase
&quot;literate programming&quot; would suggest.

&lt;p&gt;
One very important feature of Cweb
is something that would seem to be a nuisance,
or at best an implementation detail.
The &lt;file&gt;.w&lt;/file&gt; file
which contains the Cweb is now the &quot;source&quot;.
The
&lt;file&gt;.c&lt;/file&gt; and
&lt;file&gt;.h&lt;/file&gt; files are now &quot;built files&quot;.
I am no longer working with the C language
&quot;translation units&quot;.
(&quot;Translation unit&quot;
is standards-committee-speak for
the text which is
directly input into the C compiler.
The term is used in attempt to distance the standard from
file conventions.)

&lt;p&gt;
Moving one step back from the &quot;translation unit&quot;
separates the presentation of the code
from the issues of linkage and scope.
Over the years,
I'd gotten used to
organizing my code
around the visibility rules for the compiler and
the linker.
With Cweb,
I can
lay out C code
in the same way that I think about it.
It surprises me how liberating this is.

&lt;p&gt;Suppose we are adding an element to a C structure.
Typically required might be:
&lt;ol&gt;
&lt;li&gt;A &lt;var&gt;typedef&lt;/var&gt; for a type
that is particularly complex.

&lt;li&gt;The entries in the &lt;var&gt;struct&lt;/var&gt;.
&lt;li&gt;Initialization of these entries.
&lt;li&gt;&quot;Destruction&quot; of those entries: freeing any memory or other resources
they use.
&lt;li&gt;Definition of the function bodies for mutators, accessors, etc.
&lt;li&gt;Public prototypes for some of the functions.
&lt;li&gt;Private prototypes for other functions.
&lt;/ol&gt;
Often, each of these would be put at a different location.
But all these bits of code are written and debugged together.
If you ever want to change the data structures,
all these bits of code would have to be tracked down again.
True, skill at picking good names
and at performing searches on
the source can make this sort of thing tractable.
But it is also true that some of the programming skills
we've developed over the years could 
aptly be called symptoms.

&lt;p&gt;Here is some C logic which, while very small, nonetheless has 6 of the 7 
components listed above.
In Cweb I was able to put them all together.  Here's the bottom of one page of my &quot;woven&quot; Cweb code:
&lt;img alt=&quot;callback1.png&quot; src=&quot;http://blogs.perl.org/users/jeffrey_kegler/callback1.png&quot; width=&quot;723&quot; height=&quot;169&quot; class=&quot;mt-image-none&quot; style=&quot;&quot; /&gt;
&lt;p&gt;... and here is the top of the next:
&lt;img alt=&quot;callback2.png&quot; src=&quot;http://blogs.perl.org/users/jeffrey_kegler/callback2.png&quot; 
width=&quot;723&quot; height=&quot;623&quot; class=&quot;mt-image-none&quot; style=&quot;&quot; /&gt;
&lt;p&gt;For some readers, these samples may adequately illustrate my point.
For those curious about the code, I'll close with a few
explanations.  The code, as I said, is for the XS version of Marpa.
For those not familiar, &lt;a href=&quot;http://search.cpan.org/~jkegl/Marpa-0.200000/&quot;&gt;Marpa&lt;/a&gt;
is a general BNF parser generator -- it parses from any grammar that you can write in BNF.
If the grammar is of any of the kinds currently in practical use
(yacc, LR(k), LALR, LL, recursive descent, etc.), this parsing is in linear
time.

&lt;p&gt;Marpa uses Perl callbacks, and so the XS version must be able to call back
to Perl closures.
So where is all the Perl logic in my example?

&lt;p&gt;For the XS conversion, I'm separating the code into three layers:

&lt;ol&gt;
&lt;li&gt;
&lt;var&gt;libmarpa&lt;/var&gt;
is a &quot;pure C&quot; library, which implements the core
of the Marpa algorithm.
&lt;var&gt;libmarpa&lt;/var&gt;
is agnostic about whether it is called from Perl,
from another high level language, and even from other C code.
&lt;li&gt;There's a &quot;glue&quot; layer to tie the Perl code to &lt;var&gt;libmarpa&lt;/var&gt;.
This is in Perl's XS language.
XS can be thought of as a C dialect.
Special preprocessing converts XS into C code,
which is then compiled.
&lt;li&gt;Finally, there's a &quot;pure Perl&quot; wrapper.  User interface
issues are handled at this level.
&lt;/ol&gt;

&lt;p&gt;I am only using Cweb for &lt;var&gt;libmarpa&lt;/var&gt;.
(Cweb only understands C language.)

&lt;p&gt;Since &lt;var&gt;libmarpa&lt;/var&gt; has
no Perl-specific logic, its role in dealing with Perl callbacks is very simple.
&lt;var&gt;libmarpa&lt;/var&gt; needs code to store the callback (which from its point
of view is just a C function pointer),
and some other code to perform the callback.
C does not have closures, so to implement a poor man's version
of these, the callback is allowed an argment,
which can be examined and set.
It comes out to a couple of dozen lines of code,
the majority of which are declarations and data definitions.

&lt;p&gt;One thing is missing from the example -- &quot;destructor&quot; logic.
The pointers in the example do not &quot;own&quot; the things that they point to,
so there is no need to deallocate anything.
When allocation and deallocation are separate, it is easy to forget
whether you omitted deallocation logic because it was unnecessary,
or because you forgot.
Using Cweb, I always put allocation and deallocation together.
Perhaps that's not enough to make memory leaks a thing of the past,
but it certainly makes life easier.</description>
  </item>
  <item>
    <title>Marpa's Sudden Portability Adventure</title>
    <link>http://jeffreykegler.github.com/Ocean-of-Awareness-blog/2010/11/02#marpas-sudden-portability-adventure</link>
    <description>&lt;p&gt;I've made bold claims for the portability of
&lt;a href=&quot;http://search.cpan.org/~jkegl/Marpa-0.200000/&quot;&gt;Marpa&lt;/a&gt;.
[
Marpa is a general BNF parser generator -- it parses from any grammar that you can write in BNF.
If the grammar is of one of the kinds currently in practical use
(yacc, LR(k), LALR, LL, recursive descent, etc.), this parsing is in linear
time. ]

&lt;p&gt;The boldness of my claims evinced
no ambition to test them under fire.
But, when my main development box (a 2-year Dell laptop running Ubuntu)
suddenly died,
my only other choice for a development platform was a MacBook G4 running
Mac OS Tiger.
So test my portability claims is what I had to do.

&lt;p&gt;Not that my brags about portability had been without foundation.
I upload development versions for cpantesters regularly.
(By the way, to all of you at cpantesters: a big &quot;thank you&quot;.)
As far as the configuration, build and runtime environments went,
I had a lot of reason to be confident.

&lt;p&gt;The problem is that
I am now working on the XS version of Marpa.
When I was working on
the Pure Perl version of
&lt;a href=&quot;http://search.cpan.org/~jkegl/Marpa-0.200000/&quot;&gt;Marpa&lt;/a&gt;,
my maintenance and development environments were not separate from
the configuration, build and runtime environments.
Development was also in Pure Perl,
with no additional tools required.
But for the XS version of Marpa,
maintenance and development require GNU autotools,
as well as Don Knuth's TeX and Cweb.

&lt;p&gt;Let me emphasize that you can, now and in the future,
configure, build and run Marpa::XS on boxes without GNU autotools
or TeX.
It's just that
you can't properly work on or patch the source code
without them.
Also, it is worth emphasizing that, in the open source spirit,
Marpa::XS ships with all the files necessary for maintainance
and development.
So you always have the &quot;source&quot; in the fullest sense,
and all the tools you need to develop are available free
with source.
But part of making Marpa::XS as portable as possible is ensuring that
it will configure, build and run happily
in environments which don't have all
the development tools it requires.

&lt;p&gt;Which brings me back to my crisis.
I could be confident about configuring, building and running my development
version of Marpa::XS on a wide variety of platforms.
The cpantesters had proved this for me.
Less certain was whether my
development environment could be recreated.

&lt;p&gt;The sudden switch involved a sudden change of chips (Intel to PPC),
and operating systems
(the latest Ubuntu to a Mac OS that is two major releases out of date).
More dramatic changes are possible (both are POSIX systems),
but for a port suddenly forced on me, this was more than
change enough, thank you.

&lt;p&gt;I hoped everything I needed was on MacPorts.
That was assuming that vintage 2005 systems, running
a Mac OS version two major
releases obsolete,
were a priority with the helpful folks at MacPorts.

&lt;p&gt;In the event, I was back developing with 24 hours.
The only change I had to make to my codebase was to three lines
in Build.PL, which specified module versions.
Most of the 24 hours was taken up with compiles.
The Macbook G4 was never considered a fire-breather,
even back in 2005 when it was new.</description>
  </item>
  </channel>
</rss>
