<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Why is parsing considered solved?</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Sun, 03 Jun 2018</h3>
<br />
<center><a name="knuth_1965"> <h2>Why is parsing considered solved?</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <p>It is often said that parsing is a "solved problem".
    Given the level of frustration with the state of the art,
    the underuse of the very powerful technique of
    Language-Oriented Programming due to problematic tools<a id="footnote-1-ref" href="#footnote-1">[1]</a>,
    and the vast superiority of human parsing ability
    over computers,
    this requires explanation.
    </p>
    <p>
    On what grounds would someone say that parsing is "solved"?
    To understand this,
    we need to look at the history of Parsing Theory.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    In fact, we'll have to start decades before computer Parsing Theory
    exists,
    with a now nearly-extinct school of linguistics,
    and its desire to put the field on strictly
    scientific basis.
    </p>
    <h2>"Language" as of 1929</h2>
    <p>In 1929 Leonard Bloomfield,
      as part of his effort to create a linguistics that
      would be taken seriously as a science,
      published his "Postulates".<a id="footnote-3-ref" href="#footnote-3">[3]</a>
      The "Postulates" include his definition of language:
    </p><blockquote>
      The totality of utterances that can be made in a speech
      community is the
      <b>language</b>
      of that speech-community.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </blockquote><p>
      There is no reference in this definition to the usual view,
      that the utterances of a language "mean" something.
      This omission is not accidental:
    </p><blockquote>
      The statement of meanings is therefore the weak point in
      language-study, and will remain so until human knowledge
      advances very far beyond its present state. In practice, we define the
      meaning of a linguistic form, wherever we can, in terms of some
      other science.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </blockquote><p>
      Bloomfield is passing the buck,
      because the behaviorist science of his time rejects
      any claims about mental states as
      unverifiable statements -- essentially,
      as claims to be able to read minds.
      "Hard" sciences like physics, chemistry and even
      biology avoid dealing with unverifiable mental states.
      Bloomfield and the behaviorists want to make the methods of linguistics
      as close to hard science as possible.
    </p>
    <p>
      Draconian as Bloomfield's exclusion of meaning is,
      it is a big success.
      Known as structural linguistics,
      Bloomfield's approach dominates lingustics for
      the next couple of decades.
    </p>
    <h2>1955: Noam Chomsky graduates</h2>
    <p>
      Noam Chomsky earns his PhD at the Universtity of Pennsylvania.
      His teacher, Zelig Harris, is a prominent Bloomfieldian,
      and Chomsky's early work is thought to be in the Bloomfield school.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
      Chomsky becomes a professor at MIT.
      MIT does not have a linguistics department,
      and Chomsky is free to teach his own approach to the subject.
    </p>
    <h2>The term "language" as of 1956</h2>
    <p>Chomsky publishes his "Three models" paper,
      one of the most important papers of all time.
      His definition of language now uses the terminology
      of set theory,
      but its substance comes from Bloomfield:
    </p><blockquote>
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of sysbols.  If A is an alphabet, we shall say that
      anything formed by concatenating the symbols of A is a string in
      A. By a grammar of the language L we mean a device of some sort that
      produces all of the strings that are sentences of L and only these.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </blockquote>
    <p>
      But already in "Three Models",
      Chomsky readily brings in semantics,
      when it serves his purposes.
      For a semantically ambiguous utterance,
      Chomsky's new model produces multiple syntactic derivations.
      Each of these syntactic derivations
      "look" like the natural representation
      of one of the meanings,
      and Chomsky points out that this is a very
      desirable property for a model to have.<a id="footnote-8-ref" href="#footnote-8">[8]</a>
    </p>
    <h2>Chomsky 1959</h2>
    <p>In 1959, Chomsky reviews a book by B.F. Skinner's on linguistics.<a id="footnote-9-ref" href="#footnote-9">[9]</a>
    Skinner is the most prominent behaviorist of the time.
    </p>
    <p>
    Chomsky's review removes all doubt about where he stands
    on behaviorism
    or on the relevance of linguistics to the study of meaning.<a id="footnote-10-ref" href="#footnote-10">[10]</a>
    His review galvanizes the opposition to behaviorism, and
    Chomsky establishes himself as behavorism's most
    prominent and effective critic.
    </p>
    <p>
      In later years,
      Chomsky will make it clear that he had had no intention of
      following in the behaviorist tradition,
      by avoiding considerations of meaning, aka semantics:
    </p><blockquote>
      [...] it would be absurd to develop
      a general syntactic theory
      without assigning an absolutely
      crucial role to semantic considerations,
      since obviously the necessity to support
      semantic interpretation is one of the primary
      requirements
      that the structures
      generated by the syntactic component of a grammar
      must meet.<a id="footnote-11-ref" href="#footnote-11">[11]</a>
    </blockquote>
    <h2>Oettinger 1961</h2>
    <p>
      While the stack itself goes back to Turing<a id="footnote-12-ref" href="#footnote-12">[12]</a>,
      its significance for parsing becomes an object
      of interest in itself with
      Samuelson and Bauer's 1959 paper<a id="footnote-13-ref" href="#footnote-13">[13]</a>.
      Mathematical study of stacks as models of computing begins with Anthony Oettinger's 1961 paper.<a id="footnote-14-ref" href="#footnote-14">[14]</a></p>
    <p>Oettinger 1961 is full of evidence that stacks
      (which he calls "pushdown stores") are very new.
      Oettinger, for example, does not use the terms "push" or "pop",
      but instead describes operations on his pushdown stores using
      a set of vector operations which will later form the basis
      of the APL language.
    </p>
    <p>
      As of 1961, all algorithms with acceptable speed are using
      stacks with various modifications.
      Oettinger expresses a hope:
    </p>
    <blockquote>
      The development of a theory of pushdown algorithms should
      hopefully lead to systematic techniques for generating
      algorithms satisfying given requirements to replace
      the ad hoc invention of each new algorithm.<a id="footnote-15-ref" href="#footnote-15">[15]</a>
    </blockquote>
    <p>Oettinger defines 4 languages, all of sets of strings.<a id="footnote-16-ref" href="#footnote-16">[16]</a>
      Oettinger's pushdown stores
      will eventually be called
      deterministic pushdown automata (DPDA's) and
      become the basis of a model of language.
      Oettinger hopes this model will
      be an adequate basis both for natural language
      (Russian translation is Oettinger's area of research)
      and for computing languages like ALGOL.
      For Russian translation,
      DPDA's will prove totally inadequate.
    <p>
    </p>
      But as a hoped-for basis for a theory of computer language parsing,
      Oettinger DPDA's have a much longer life.
      The focus of Parsing Theory research over the next ten years will be
      discovering a theory of DPDA-based parsing.
      And once discovered,
      this theory will dominate the academic literature
      on parsing for a much longer time.
    </p>
    <h2>Knuth 1965</h2>
    <p>DPDA-based parsing theory soon attracts the attention of Computer Science's
    best technical mathematician, Donald Knuth.
    In his pivotal LR(k) paper,<a id="footnote-17-ref" href="#footnote-17">[17]</a>
      Knuth sets out a theory that explains
      all the "tricks"<a id="footnote-18-ref" href="#footnote-18">[18]</a>
      used for efficient parsing up to that time.
      Knuth sets out a comprehensive theory of stack-based
      parsing algorithms.
      For a start, Knuth shows that stack-based parsing is
      equivalent to a new and unexpected class of grammars
      LR(k), and he provides a parsing algorithm for them.
    </p>
    <p>
      Knuth's new algorithm might be expected to be "the one to rule
      them all".
      Unfortunately, while deterministic and linear,
      is not practical -- it requires huge tables well beyond
      the memory capabilities of the time.
      This does not suggest to Knuth that the DPDA-based model
      is inappropriate as a model of practical parsing --
      instead it suggest to him, and to the field,
      that the boundary of practical parsing lies inside the
      LR(k) grammars.
    </p>
    <p>
    The idea that the solution to the parsing problem must be
    DPDA-based is not without foundation.
    In 1965, the limits of computer technology are severe.
    For practitioners,
    any parsing technique that required more than a DPDA --
    that is, more than state
    machine and a stack,
    was not likely to happen.
    After all,
    four years earlier, stacks had been bleeding edge.
    </p>
    <p>
    To be sure,
      Knuth, in his program for further research<a id="footnote-19-ref" href="#footnote-19">[19]</a>,
      does suggests investigation of parsers for superclasses
      of LR(k).
      He even describes his own superclass of LR(k):
      LR(k,t), which is LR(k) more aggressive lookahead.
      But he is clearly unenthusiastic about LR(k,t)<a id="footnote-20-ref" href="#footnote-20">[20]</a>
      It is reasonable to suppose,
      that Knuth is even more negative about more general approaches that
      he does not bother to mention.<a id="footnote-21-ref" href="#footnote-21">[21]</a>
    </p>
    <p>
      In any case, those reading Knuth's LR(k) focused almost
      exclusively on his suggestions for research within the DPDA-based
      model.
      These included grammars rewrites;
      streamlining of the LR(k) tables;
      or research into LR(k) subclasses.
      It is LR(k) subclassing that will receive the most attention.
    </p>
    <p>
      Knuth is certainly aware that DPDA determinism and
      linear time behavior are not the same thing.<a id="footnote-22-ref" href="#footnote-22">[22]</a>
      An algorithm can be more powerful than a DPDA,
      while still being linear.
      But linearity is a stand-in for "practical",
      and, with his discovery that even DPDA-based
      algorithms can be impractical,
      Knuth, and the research community,
      decide that it is extremely unlikely than more
      powerful computing models will also be faster in
      practice.
    </p>
    <p>
       Why was such a powerful skepticism based on the results for one
       computing model of computing?
       Stacks, as we now call them, are a natural model of computing,
       so it is reasonable to think they form a step on the hierarchy
       of tradeoffs of power against practical speed.
       But a very important was the proof that LR(k) grammars were
       "equivalent" to DPDA's.
       And central to the acceptance of this proof as relevant
       was a confusion about the use of the term "language".
    </p>
    <p>With his 1965, Knuth, Computer Science's greatest mathematician 
    disposes of the DPDA problem.
    Knuth's exhausting and exhilarating 39-page
    demonstration of mathematical virtuousity
    almost "runs the board"
    of open problems in parsing,
    and his section on "open problems" is read
    as a definitive program for further research.
    </p>
    <p>
    Knuth does not quite solve the problem,
    but he sets the framework within which a solution
    will be found --
    or so it seems.
    Because, while Knuth's math is correct,
    a confusion about the term "language"
    makes his conclusions unreliable.
    </p>
    <h2>The term "language" as of 1965</h2>
    <p>
    Knuth defines language as follows:
    </p>
    <blockquote>
    The language defined by G is<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    { &alpha; | S => &alpha; and &alpha; is a string over T }<br>
    namely, the set of all terminal string derivable from S by using
    the productions of G as substitution rules.<a id="footnote-23-ref" href="#footnote-23">[23]</a>
    </blockquote>
    (Here G is a grammar whose start symbol is S and whose set
    of terminals is T.)
    This is clearly the behavorist definition of language
    translated into set-theoretic terms.
    </p>
    <p>Knuth proves, to the satisfaction of the profession,
    the "equivalence" of LR(k) and DPDA's.
    LR(k) is a class of grammars and the DPDA model is of
    a language -- a set of strings.
    At first glance, this is an "apples and oranges" comparison --
    how do you prove the equivalence of a language and a grammar.
    </p>
    <p>Knuth does this by reducing the language and the class grammar
    of grammars to a lowest common denominator --
    a grammar defines a language, so he compares the LR(k) language
    to the DPDA language.
    It takes some impressive mathematics,
    but Knuth is able to show that the two languages are equivalent.
    But note that the question whether LR(k) is an impassable
    barrier for parsing grammars -- not languages.
    </p>
    Punning a class of grammars as a class of languages does not work --
    in fact, as Knuth shows, it produces a considerable amount of magical
    thinking. 
    Using the Knuth algorithm
    <ul>
    <li>Parsing LR(k) grammars for an arbitrary is hopelessly impractical.
    </li>
    <li>Parsing LR(1) grammars is almost practical, but not quite.<a id="footnote-24-ref" href="#footnote-24">[24]</a>
    </li>
    <li>Parsing LR(0) grammars is quite practical.
    </li>
    </ul>
    </p>The problem for Knuth's proof of equivalence is that,
    if you consider languages, LR(1) and LR(k) are equivalent.
    And in fact, both are almost equivalent to LR(0) -- if you add
    an explicit end marker to a language
    (which in most applications is easy to do<a id="footnote-25-ref" href="#footnote-25">[25]</a>)
    then LR(k) = LR(1) = LR(0).
    </p>
    <p>
    That is, in language terms, the hopelessly impractical
    is equivalent to the borderline impractical.
    And these, for most applications, are equivalent to the
    very practical LR(0).
    When thinking in terms of languages,
    we can transport ourselves across the
    same practical/impractical boundary that we are claiming
    to show is, in practice, impassable for grammars.
    </p>
    <p>
    What for languages is reasonable thinking,
    is magical thinking when it comes to grammars.
    This suggests that reasoning based on the equivalence
    of languages may not be helpful for deciding
    what is practical for parsing grammars --
    in fact, it suggests that this kind of reasoning
    could be seriously misleading.
    </p>
    <p>In that light,
    it should be no surprise that,
    in 1991, Joop Leo showed how to extend practical
    parsing well beyond the LR(k) and DPDA models.<a id="footnote-26-ref" href="#footnote-26">[26]</a>
    </p>
    <h2>Comments, etc.</h2>
    <p>
      The background material for this post is in my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3>
    Parsing: a timeline 3.0</a>,
    and this post may be considered a supplement to "Timelime".
    I encourage
    those who want to know more about the story of Parsing Theory
    to look at my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3>
    Parsing: a timeline 3.0</a>.
    For example, "Timeline 3.0" tells the story of the search for a good
    LR(k) subclass,
    and what happened afterwards.
    </p>
    <p>
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
    The well-known <a href="https://en.wikipedia.org/wiki/Design_Patterns">
    <cite>Design Patterns</cite> book</a>
    (aka "the Gang of 4 book")
    has a section on this which call Language-oriented programmer
    its "Interpreter pattern".
    This amply illustrates the main obstacle to use
    of the pattern -- lack of adequate parsing tools.
    I talk much more about this in my two blog posts on 
    the Interpreter pattern:
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/bnf_to_ast.html">
    BNF to AST</a>
    and 
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/interpreter.html">
    The Interpreter Design Pattern</a>.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
      This post takes the form of a timeline, and
      is intended to be incorporated in my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3>.
      Parsing: a timeline</a>.
      The earlier entires in this post borrow heavily from
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html">
	    a previous blog post</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
        Bloomfield, Leonard,
        "A set of Postulates
        for the Science of Language",
        <cite>Language</cite>, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
        Bloomfield 1926, definition 4 on p. 154.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
        Bloomfield, Leonard.
        <cite>Language</cite>.
        Holt, Rinehart and Winston, 1933, p. 140.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
        Harris, Randy Allen,
        <cite>The Linguistics Wars</cite>,
        Oxford University Press, 1993,
        pp 31-34, p. 37.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
        The quote is on p. 114 of
        Chomsky, Noam.
        "Three models for the description of language."
        <cite>IRE Transactions on information theory</cite>,
        vol. 2, issue 3, September 1956, pp. 113-124.
        In case there is any doubt Chomsky's "strings"
        are Bloomfield's utterances,
        Chomsky also calls his strings,
        "utterances".
        For example in Chomsky, Noam,
        <cite>Syntactic Structures</cite>,
        2nd ed.,
        Mouton de Gruyter, 2002, on p. 15:
        "Any grammar of a language will project the finite and somewhat accidental
        corpus of observed utterances to a set (presumably infinite)
        of grammatical utterances."
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
        Chomsky 1956, p. 118, p. 123.
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9">9.
    Chomsky, Noam.
    “A Review of B. F. Skinner’s Verbal Behavior”. <cite>Language</cite>,
    Volume 35, No. 1, 1959, 26-58.
    <a href="https://chomsky.info/1967____/">
    https://chomsky.info/1967____/</a> accessed on 3 June 2018.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10">10.
    See in particular, Section IX of Chomsky 1959.
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11">11.
        Chomsky, Noam.
        <cite>Topics in the Theory of Generative Grammar</cite>.
        De Gruyter, 1978, p. 20.
        (The quote occurs in footnote 7 starting on p. 19.)
 <a href="#footnote-11-ref">&#8617;</a></p>
<p id="footnote-12">12.
        Carpenter, Brian E., and Robert W. Doran.
        "The other Turing machine."
        <cite>The Computer Journal</cite>, vol. 20, issue 3, 1 January 1977, pp. 269-279.
 <a href="#footnote-12-ref">&#8617;</a></p>
<p id="footnote-13">13.
        Samelson, Klaus, and Friedrich L. Bauer. "Sequentielle formelübersetzung." it-Information Technology 1.1-4 (1959): 176-182.
 <a href="#footnote-13-ref">&#8617;</a></p>
<p id="footnote-14">14.
          Oettinger, Anthony.
          "Automatic Syntactic Analysis and the Pushdown Store"
          <cite>Proceedings of Symposia in Applied Mathematics</cite>,
          Volume 12,
          American Mathematical Society, 1961.
 <a href="#footnote-14-ref">&#8617;</a></p>
<p id="footnote-15">15.
        Oettinger 1961, p. 127.
 <a href="#footnote-15-ref">&#8617;</a></p>
<p id="footnote-16">16.
    Oettinger 1961, p. 106.
 <a href="#footnote-16-ref">&#8617;</a></p>
<p id="footnote-17">17.
    Knuth, Donald E.
    "On the translation of languages from left to right."
    <cite>Information and Control</cite>, Volume 8, Issue 6, December 1965, pp. 607-639.
    <a href="https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d">
    https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d</a>, accessed 24 April 2018.
 <a href="#footnote-17-ref">&#8617;</a></p>
<p id="footnote-18">18.
      Knuth 1965, p. 607, in the abstract.
 <a href="#footnote-18-ref">&#8617;</a></p>
<p id="footnote-19">19.
      Knuth 1961, pp. 637-639.
 <a href="#footnote-19-ref">&#8617;</a></p>
<p id="footnote-20">20.
      "Finally, we might mention another generalization of LR(k)"
      (Knuth 1965, p. 638); and
      "One might choose to call this left-to-right translation,
      although we had to back up a finite amount."
      (p. 639).
 <a href="#footnote-20-ref">&#8617;</a></p>
<p id="footnote-21">21.
      Knuth's skepticism for more general Chomskyan approaches
      is suggested by his own plans for his (not yet released) Chapter
      12 of the <cite>Art of Computer Programming</cite>,
      in which he planned to use pre-Chomskyan bottom-up methods. (See
      Knuth, Donald E., "The genesis of attribute grammars",
      <cite>Attribute Grammars and Their Applications</cite>,
      Springer, September 1990, p. 3.)
 <a href="#footnote-21-ref">&#8617;</a></p>
<p id="footnote-22">22.
        Knuth 1965, p. 607: "execution time at worst
        proportional to the length of the string being parsed."
 <a href="#footnote-22-ref">&#8617;</a></p>
<p id="footnote-23">23.
    Knuth 1965, p. 608.
 <a href="#footnote-23-ref">&#8617;</a></p>
<p id="footnote-24">24.
    Given the capacity of computer memories in 1965,
    LR(1) was clearly impractical.
    Today, that could be reconsidered, but LR(1) is still restrictive
    and has poor error-handling,
    so few practitioners have bothered with it.
 <a href="#footnote-24-ref">&#8617;</a></p>
<p id="footnote-25">25.
    Some parsing applications, such as those which receive their input "on-line",
    can not determine the size of their input in advance.
    For these applications adding an end marker to their input is
    inconvenient or impossible.
 <a href="#footnote-25-ref">&#8617;</a></p>
<p id="footnote-26">26.
 Joop M. I. M.
 "A general context-free parsing algorithm running in linear time on every LR (k) grammar without using lookahead."
 <cite>Theoretical computer science</cite>, Volume 82, Issue 1, 22 May 1991, pp. 165-176.
 <a href="https://www.sciencedirect.com/science/article/pii/030439759190180A">
 https://www.sciencedirect.com/science/article/pii/030439759190180A</a>, accessed 24 April 2018.
 <a href="#footnote-26-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 11:35 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
