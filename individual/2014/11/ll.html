<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Sat, 15 Nov 2014</h3>
<br />
<center><a name="ll"> <h2>Parsing: Top-down versus bottom-up</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>Why did the invention of bottom-up parsing bring
    with it such hope?
    Indeed, despite bottom-up parsing's
     near-total abandonment by practitioners,
     among theoreticians bottom-up still has not lost its hold.
     Even today,
    in parsing texts and courses,
    bottom-up parsing frequently gets as much or more
    attention than
    the more widely used top-down parsing.
    Why?
    <p>Many treatments of these two approaches
    are either too high-level or two low-level.
    Overly high-level treatments reduce the two approaches to buzzwords,
    and the comparision to a recitation of received wisdom.
    Overly low-level treatments get immersed in the minutiae of implementation,
    so that the comparison ends up no more helpful than placing
    two unrelated code listings placed side by side.
    In this post I hope to clarify why the two advocates of each
    parsing approach took that positions
    they did,
    and to suggest a way forward.
    </p>
    <h3>Top-down parsing</h3>
    <p>The basic idea of top-down parsing is
      as brutally simple as anything in programming:
      you look at the next token and decide then and there
      where it fits into the parse tree.
      Starting at the top, you add pieces and once
      you've looked at every token,
      you have your parse tree.
      <p>
      Since this idea, in its purest form,
      is too simple to get anything done,
      top-down parsing is almost
      always combined with with lookahead.
      Lookahead of one token helps a lot.
      But longer lookaheads
      are very sparsely used.
      They just aren't that helpful,
      and since
      the number of possible lookaheads grows exponentially,
      they get very expensive very fast.
    </p><p>Top-down parsing has an issue with left recursion.
      It's straightforward to see why.
      Take
      an open-ended expression like
    </p><blockquote><pre>
    a + b + c + d + e + f + [....]</pre></blockquote>
    <p>
      Here the plus signs continue off to the right,
      and all of them should go above the first one in the parse tree.
      So we put the first plus sign into a top-down parse
      tree without having dealt with all those plus signs that follow it.
      Even in the simplest expression,
      there is no way of even counting the plus signs
      without looking to the right,
      quite possibly a very long way to the right.
      And for anything but the simplest expressions,
      this rightward-looking needs to get
      rather sophisticated
      There are ways dealing with this difficulty,
      but all of them share one thing in common --
      they are trying to make top-down parsing into
      something that it is not.
    </p><h3>Advantages of top-down parsing</h3>
    <p>Top-down parsing does not look at the right context in any systematic way,
    and in the 1970's it was hard to believe that
      that as as good as we can do.
      (It's not all that easy to believe today.)
      But before looking at alternatives,
      I want to emphasize that its extreme simplicity
      is also top-down parsing's great strength.
      Because a top-down parsing is extremely simple,
      it is very easy to figure out what it is doing.
      And easy to figure out means easy to customize.
    </p><p>
      Take another of the many constructs incomprehensible to
      a top-down parser:
    </p><blockquote><pre>
    2 * 3 * 4 + 5 * 6
    </pre></blockquote><p>
      How do top-down parsers typically handle this?
      Simple: as soon as they realize they are faced
      with an expression, they give up on top-down
      parsing and switch to a special-purpose algorithm.
    </p><p>These two properties -- easy to understand
      and easy to customize --
      have catapulted top-down parsing
      to the top of the heap.
      Behind their different presentations,
      combinator parsing, PEG, and recursive descent are
      all top-down parsers.
    </p><h3>Bottom-up parsing</h3>
    <p>Few theoreticians of the 1970's imagined that top-down parsing would
      be the end of the story.
      It seemed almost paradoxical that, while looking to the right in
      ad hoc ways
      migh help, there was no systematic way to
      to exploit the right context.
      Certainly,
      when reading sentences like these, you'd think
      a human must be making more than casual use of right context.
    </p><p>Don Knuth in 19xx found an algorithm to exploit
      right context.
      Knuth's LR algorithm was,
      like top-down parsing as I have described it,
      deterministic.
      Determinism was thought to be essential because determinism
      allowing more than one choice easily leads to an explosion in the
      number possibilities being considered at once.
      Preventing an explosion in the number of possibilities
      guaranteed that the parse can be done in linear time.
    </p><p>Simplistically, Knuth's suggestion was to,
      instead of fully deciding the
      parse at every location, to make what I will call "subdecisions" --
      decisions as to how the
      piece at that location is used, but which allow the subdecision
      to be put into a larger context,
      to be decided later.
      Don proposed a stack of subdecisions -- if we have a subdecision
      but we cannot decide its full context, we push it onto a stack.
      When you encounter the context, you pop it off the stack
      and make the full decision.
      <p>This approach is called shift/reduce parsing,
      where shift mean "shift onto the stack"
      and reduce meant "use a rule to reduce the top to the stack, in effect
      deciding one piece of the parse from the bottom up."
      My descriptions below will not describe all the details of shift/reduction --
      Instead I will simplify matters by looking only at the operators,
      and these two crucial decisions about them:
      <ul>
      <dl>
      <dd>subdeciding<dt>making a bottom-up subdecision immediately; or
      <dd>LIFOing<dt>postponing the subdecision in a last-in-first-out manner.
      </dl>
      </ul>
      Actually implementing a shift-reduce parser is more complicated,
      but the details are well-covered in a number of place.
      <a href=http://en.wikipedia.org/wiki/Shift-reduce_parser:>The Wikipedia article</a>,
      for example, is excellent.
    </p><p>Like top-down parsing, bottom-up parsing is usually combined with lookahead.
      For the same lookahead, a bottom-up parser parses not everything that a
      top-down parser can handle,
      and more.
    </p><p>
      To preserve determinism,
      you have to know,
      at every location,
      whether to "subdecide" or "LIFO",
      and if "subdeciding", which rule to use to make the subdecision,
      But the problem with left recursion disappeared.
      In the example from above,
    </p><blockquote><pre>
    a + b + c + d + e + f + [....]</pre></blockquote>
    <p>
      you never need to use LIFO -- you just simply make one subdecision after another
      until you run out of plus signs.
      The problem that the top-down parser had
      because we don't need to know about the plus signs to come -- they
      will go into nodes of the parse tree above the one we are building,
      and we are building bottom-up.
      When we run out of plus signs, we also have hit the top of the parse tree
      and we are done.
    </p>
    <p>For a bottom-up parser, right recursion is harder, but not much.
    </p><blockquote><pre>
    a = b = c = d = e = f = [....]</pre></blockquote>
  <p>
      Here you simply push every token onto the LIFO.
      Once your LIFO contains the entire input,
       you pop the assignments off the stack
      one by one, building the tree bottom-up.
      When the stack is empty you are done.
      Essentially, you do the exactly same thing you did for left recursion,
      but you use the LIFO to reverse the order.
      <p>
      Arithmetic expressions like
    </p><blockquote><pre>
    2 * 3 * 4 + 5 * 6</pre></blockquote>
    <p>
      are handled by combining the methods just described.
      For this one
      you immediately build all the multiplications in the first set
      in a subparse,
      and push it onto the LIFO.
      Next, you push the plus sign onto the LIFO.
      Then you build all the multiplications in the next set into a subparse,
      pushing the result
      onto the LIFO.
      At this point, the stack contains a subparses,
      an addition sign,
      and another subparse.
      The two subparses will be the operands for the addition.
      You pop it the subparse operands, and the addition sign,
      and build a new subparse from them.
      This subparse is your parse tree.
    </p><p>
      As mentioned, I omitted a lot of details,
      and most of which will stay omitted.
      But one omission requires attention:
      to preserve determinism,
      we have to know,
      at every location,
      whether to shift or reduce,
      and when reducing, what reduction to make.
      Above, I assumed that we had a way to figure this out.
      In fact,
      finding what a practical way to make the necessary
      shift/reduce and reduce/reduce decisions was a very far from trivial
      task.
      By the 197-, it was thought a practical way had been found,
      and around 19-- a parser generator based on it was released as yacc.
      (Readers today may be more familiar with yacc's successor, bison.)
    </p>
    <h3>The advantages and disadvantages of bottom-up parsing</h3>
    <p>
      With yacc, it looked as if all the most failures of top-down parsing were solved.
      We now had a parsing algorithm that could readily and directly
      parse left recursions and arithmetic expressions.
      Theoreticians thought they'd found the Holy Grail.
      When the textbooks focused on bottom-up parsing came out,
      they were not always able to restrain the urge
      to portray parser writers as knights in armor.
    </p><p>But not every medieval romance has a happy ending.
      As I've
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html">described
      elsewhere</a>,
      this story ended badly.
      Bottom-up parsing was driven by tables which made the algorithm fast
      for correct inputs, but unable to accurately diagnose faulty ones.
      The subset of grammars parsed was still not quite large enough,
      even for conservative language designers.
      And bottom-up parsing was very unfriendly to custom hacks,
      which made its shortcomings loom large.
      It is much harder to work around a problem in a bottom-up
      parser than than it was to deal with a similar problem
      in a top-down parser.
      After years of experience,
      top-down parsing has re-emerged as the
      algorithm of choice.
    </p><h3>Table parsing</h3>
    <p>For many, the return to top-down parsing
      answers the question that we posed earlier:
      "Is there any systematic way to exploit right context when parsing?"
      This answer turns out to be a rather startling "No".
      But is this really the end of the story?
    </p><p>Assumed in all of this was that,
      for an algorithm to be linear,
      in practice it would also have to be deterministic.
      But is this actually the case?
    </p><p>It's not, in fact.
      To keep bottom-up parsing deterministic, we restricted ourselves to a stack.
      But what if we keep all possible subdecisions linked and in tables,
      and make the final decisions in another pass,
      once the tables are complete.
      (The second pass replaces the stack based
      see-sawing back and forth of the deterministic algorithm,
      so it's not an inefficiency.)
      Jay Earley in 19-- came up with an algorithm to do this,
      and in 1991 Joop Leo added a memoization to Earley's
      algorithm which makes it linear for all deterministic grammars.
    </p><p>The "deterministic grammars"
      are exactly the bottom-up parseable grammars
      with lookahead.
      So that means the Earley/Leo algorithm parses,
      in linear time,
      everything that a deterministic bottom-up parser can parse,
      and therefore every grammar that
      a deterministic top-down parser can parse.
      (In fact, the Earley/Leo algorithm is linear for a lot of
      ambiguous grammars as well.)
    </p><p>Top-down parsing had the advantage that it was easy to know where
      you are.  But the Earley/Leo algorithm has an equivalent advantage -- its
      tables know where it is, and it is easy to query them programmatically.
      In 2010, this blogger added to the Earley/Leo algorithm
      the other big advantage of top-down parsing:
      The Marpa algorithm rearranges the Earley/Leo parse engine so that you can
      stop it, perform your own logic, and restart where you left off.
      <a href=http://savage.net.au/Marpa.html">A quite useable parser based on the Marpa algorithm</a>
      is available as open source.
    </p><h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 17:53 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/ll.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
