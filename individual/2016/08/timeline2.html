<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Parsing: an expanded timeline</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Tue, 16 Aug 2016</h3>
<br />
<center><a name="timeline2"> <h2>Parsing: an expanded timeline</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p><b>The fourth century BCE</b>:
      In India, Pannini creates a grammar of the Sanskrit language.
      This is a sophisticated description of the Sanskrit language,
      exact and complete, and including pronunciation.
      Sanskrit
      could be recreated using nothing but Pannini's grammar.
      Pannini's grammar is probably the first formal system of any kind, predating Euclid.
      Even today, nothing like it exists for any other natural language
      of comparable size or corpus.
    </p>
    <p>
      Pannini is the object of serious study today.
      But in the 1940's and 1950's Pannini is almost unknown in the West.
      His work had no direct effect on the other events in this timeline.
    </p>
    <p><b>1943</b>:
      Emil Post defines and studies a formal rewriting system using
      productions.
      With this, the process of reinventing Pannini begins.
    </p><p><b>1948</b>:
      Claude Shannon publishes the foundation paper of information theory.
      Andrey Markov's finite state processes are used heavily.
    </p><p><b>1952</b>:
      Grace Hopper writes a linker-loader and
      <a href="https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers">
        describes it
        as a "compiler"</a>.
      She seems to be the first person to use this term for a computer program.
      Hopper used the term
      "compiler" in its original sense:
      "something or someone that bring other things together".
    </p>
    <p><b>1954</b>:
      At IBM, a team under John Backus begins working
      on the language which will be called FORTRAN.
      As of 1954, the term "compiler" is still being used in Hooper's looser sense,
      instead of its modern one.
      In particular, there was no implication that the output of a "compiler"
      was ready for execution by a computer.
      <!-- "http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf
        pp. 133-134
      -->
      The output of one 1954 "compiler",
      for example, produced relative addresses,
      which needed to be translated by hand before a machine could execute it.
    </p>
    </p><p><b>1955</b>:
      Noam Chomsky is awarded a Ph.D. in linguistics and accepts a teaching post at MIT.
      MIT does not have a linguistics department to tell him
      what to teach in his linguistics course,
      so Chomsky teaches his own approach,
      highly original and very mathematical.
    </p><p><b>1956</b>:
      <!-- "Three models" -->
      Chomsky publishes the paper which
      is usually considered the foundation of Western formal language theory.
      The paper advocates a natural language approach that involves
    </p><ul>
      <li>a bottom layer, using Markov's finite state processes;
      </li><li>a middle, syntactic layer, using context-free grammars and
        context-sensitive grammars; and
      </li><li>a top layer, which involves mappings or "transformations"
        of the output of the syntactic layer.
      </li></ul><p>
      These layers resemble, and will inspire,
      the lexical, syntactic and AST transformation phases
      of modern parsers.
      For finite state processes, Chomsky acknowledges Markov.
      The other layers seem to be Chomsky's own formulations --
      Chomsky does not cite Post's work.
    </p><p><b>1957</b>:
      Steven Kleene discovers regular expressions,
      a very handy notation for Markov's processes.
      Regular expressions turn out describe exactly the mathematical
      objects being studied as
      finite state automata,
      as well as some of the objects being studied as
      neural nets.
    </p><p><b>1957</b>:
      Noam Chomsky publishes what he has been teaching
      at MIT as a book.
      Titled
      <b>Syntactic Structures</b>,
      it is one of the most influential books of all time.
      The orthodoxy in 1957 is structural linguistics,
      which argued, with Sherlock Holmes, that
      "it is a capital mistake to theorize in advance of the facts".
      Structuralists start with the utterances in a language,
      and build upward.
    </p>
    <p>
      Chomsky, on the other hand, claiming that without a theory there
      are no facts: there is only noise.
      The Chomskyan approach is to start with a grammar, and use the corpus of
      the language to check its accuracy.
      It will soon come to dominate linguistics.
    </p>
    <p><b>1957</b>:
      Backus's team makes the first FORTRAN compiler
      available to IBM customers.
      FORTRAN is the first high-level language
      that will find widespread implementation.
      As of this writing,
      it is the oldest language that survives in practical use.
      FORTRAN is a line-by-line language
      and its parsing is primitive.
    </p>
    <p><b>1958</b>:
      John McCarthy's LISP appears.
      LISP goes beyond the line-by-line syntax --
      it is recursively structured.
      But the LISP interpreter does not find the
      recursive structure:
      the programmer must explicitly
      indicate the structure herself,
      using parentheses.
    <p><b>1959</b>:
      Backus invents a new notation to describe
      the IAL language.
      Backus's notation is influenced by his study of Post --
      he seems not to have read Chomsky until later.
      <!-- http://archive.computerhistory.org/resources/text/Oral_History/Backus_John/Backus_John_1.oral_history.2006.102657970.pdf
      p. 25 -->
    </p>
    <p><b>1960</b>:
      Peter Nauer,
      improves the notation Backus used to IAL,
      and uses it to describe the new ALGOL 60 language.
      It will become known as Backus-Nauer Form (BNF).
    </p><p><b>1960</b>:
      The ALGOL 60 report
      specifies, for the first time, a block structured
      language.
      ALGOL 60 is recursively structured but the structure is
      implicity -- newlines are not semantically significant,
      and parentheses indicate syntax only in a few specific cases.
      The ALGOL compiler will have to find the structure.
    </p>
    <p>
      It is a case of 1960's optimism at its best.
      As the ALGOL committee is well aware, a parsing
      algorithm capable
      of handling ALGOL 60 does not yet exist.
      But the risk they are taking will soon pay off.
    </p>
    <p><b>1960</b>:
      A.E. Gleenie publishes his description of a compiler-compiler.
      <!-- http://www.chilton-computing.org.uk/acl/literature/reports/p024.htm -->
      Glennie's "universal compiler" is more of a methodology than
      an implementation -- the compilers must be written by hand.
      Glennie credits both Chomsky and Backus, and observes that the two
      notations are "related".
      He also mentions Post's productions.
    </p>
    <p>
      Glennie may have been the first to use BNF as a description of a <b>procedure</b>
      instead of as the description of a <b>Chomsky grammar</b>,
      and he notes the distinction and calls it "important".
      In later years, this often be overlooked.
      BNF, when used as a Chomsky grammar, describes a set of strings,
      and does <b>not</b> describe how to parse strings according to the grammar.
      BNF notation, if used to describe a procedure, is a set of instructions, to be
      tried in some order, and used to process a string.
      Procedural describes a procedure first, and a language only indirectly.
      BNF used as a Chomsky grammar describes a language first, and does not
      describe a parsing procedure.
      Both procedural and Chomskyan BNF describe languages,
      but (and this is often missed) usually
      <b>not the same</b> language.
      That is, suppose D is some BNF description.
      Let P(D) be D interpreted as a procedure,
      let G(P(D)) be the language which the procedure P(D) parses,
      and let G(D) be D interpreted as a Chomsky grammar.
      Then, usually, G(D) != G(P(D)).
    </p>
    <p>
    This distinction between Chomskyan and pre-Chomskyan parsing mirrors
    the distinction between Chomskyan and non-Chomskyan linguistics.
    Non-Chomskyan parsing starts with strings and created grammars only
    when and if justified by the strings.
    Chomskyan parsing starts with the grammar,
    requires the programmer to figure out how to parse the grammar,
    and uses successful parsing strings to confirm the accuracy
    of the choice of grammar.
    </p>
    <p>
    Note that the pre-Chomsky approach is far more natural
    if your background is computer programming
    and not language theory.
    The parsing problem appears to the programmer in the form of
    strings to be parsed, exactly the starting point of pre-Chomsky
    parsing.
    Not only that, but even once the Chomskyan approach is
    pointed out,
    it does not seem very attractive.
    With the pre-Chomskyan approach,
    the examples more or less naturally lead to a parser --
    in the Chomskyan approach
    the programmer has to search for a
    black box to handle his grammar,
    and a search which has proved surprisingly
    long and difficult.
    Handling
    semantics is more natural with a Chomksyan approach,
    But, using captures, semantics
    can be added to a pre-Chomskyan parser
    and, with practice, this seems natural enough.
    It is surprising then,
    and a tribute to Chomsky's influence,
    that the first parsers were
    Chomskyan parsers,
    and that they have been dominant ever since.
    </p>
    <p><b>1961</b>:
      In January,
      Ned Irons publishes a paper describing his ALGOL 60
      parser.
      It is the first paper to describe any parser.
      The Irons algorithm is Chomskyan and top-down
      with a "left corner" element.
      The Irons algorithm
      is general,
      meaning that it can parse anything written in BNF.
      It is syntax-driven (aka declarative),
      meaning that the parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    </p>
    <p><b>1961</b>:
      Peter Lucas publishes the first
      description of a purely top-down parser.
      This can be considered to be recursive descent,
      though in Lucas's
      paper the algorithm is given
      syntax-driven implementation, useable only for
      a restricted class of grammars.
      Today we think of recursive descent as a methodology for
      writing parsers by hand.
      Hand-coded approaches became more popular
      in the 1960's due to three factors:
    </p>
    <ul>
      <li>
        Memory and CPU were both extremely limited.
        Hand-coding paid off, even when the gains were small.
      </li>
      <li>
        Non-hand coded top-down parsing,
	of the kind Lucas's syntax-driven
        approach allowed, is a very weak parsing technique.
        It was (and still is) often necessary
        to go beyond its limits.
      </li>
      <li>
        Top-down parsing is intuitive -- it essentially means calling
	subroutines.
	It therefore requires little or
        no knowledge of parsing theory.
        This makes it a good fit for hand-coding.
      </li>
    </ul>
    <p><b>1963</b>:
      Schmidt, Metcalf, and Schorre present papers
      on syntax-directed compilers at a Denver conference.
      <!-- Schorre 1964, p. D1.3-1 -->
    </p><p><b>1964</b>:
      D.V. Schorre publishes a paper on the Meta II
      "compiler writing language",
      summarizing the work of the 1963 conference.
      Schorre cites both Backus and Chomsky as sources
      for Meta II's notation, but notes that his approach
      is "entirely different" from that of Irons 1961.
      Indeed, while the parser is only sketched,
      it is clearly pre-Chomkyan.
      While Meta II is intended as a template, rather
      than something that readers can use,
      in principle it can be turned
      into a fully automated compiler-compiler.
      <!-- Schorre 1964, p. D1.3-1
    http://ibm-1401.info/Meta-II-schorre.pdf
    -->
    </p><p><b>1965</b>:
      Don Knuth invents LR parsing.
      Knuth is primarily interested
      in the mathematics.
      Knuth describes a parsing algorithm,
      but it is not thought practical.
    </p>
    <p><b>1965</b>:
      McClure publishes a description of a non-Chomskian parser.
      It consists of a set of routines which, on success,
      absorb input and produce output,
      and on failure pass control on to the next option.
    </p><p><b>1968</b>: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's core idea is to
      track everything about the parse in tables.
      Earley's algorithm is enticing, but it has three major issues:
    </p><ul>
      <li>First, there is a bug in the handling of zero-length rules.
      </li><li>Second, it is quadratic for right recursions.
      </li><li>Third, the bookkeeping required to set up the tables is,
        by the standards of 1968 hardware, daunting.
      </li></ul>
    <p><b>1969</b>:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
    </p>
    <p><b>1969</b>:
      Ken Thompson writes the "ed" editor as one of the first components
      of UNIX.
      At this point, regular expressions are an esoteric mathematical formalism.
      Through the "ed" editor and its descendants,
      regular expressions become
      an everyday
      part of the working programmer's toolkit.
    </p>
    <p><b>1972</b>:
      Aho and Ullmann publish a 2-volume textbook summarizing the theory
      of parsing to that date.
      This is still important and, sadly,
      very up-to-date -- progress in parsing theory has slowed drammatically.
      Aho and Ullman
      describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    </p><p><b>1972</b>:
      At or around this time,
      the main line of development for GTDPL parsers
      is classified secret by the US government,
      or at least so rumor has it.
      <!-- http://www.wikiwand.com/en/Talk:Metacompiler/Archive_2 -->
      The reason is a subject of speculation.
      Non-Chomsky parsers are surprisingly volative,
      and a minor change in language specification can be surprisingly
      labor intensive, so this must have a boon for some defense
      contractors.
    </p>
    <p>Outside of the US security community,
      Aho and Ullman note that "it can be quite difficult to determine
      what language is defined by a TDPL parser",
      and that proving them correct is impossible,
      since the only definition is their implementation.
      If the notion of correct is parsing the same language
      as the corresponding Chomsky parser,
      this is a test they usually fail.
      GTDPL parsers do what they do, which is something
      the programmer cannot describe.
      The best a programmer can do is to show that a GTDPL
      parsers passes a test suite.
    </p>
    <p>
      For this and other reasons,
      interest in GTDPL soon diminishes.
      This, however, will not be the end of the story.
    </p>
    <p><b>1975</b>:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    </p>
    <p><b>1977</b>:
      The first "Dragon book" comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters "LALR".
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    </p>
    <p><b>1979</b>: Bell Laboratories releases Version 7 UNIX.
      V7 includes what is, by far,
      the most comprehensive, useable and easily available
      compiler writing toolkit yet developed.
    </p><p><b>Compiler compilers</b>:
      Central to Bell Labs V7 is
      yacc, an LALR based parser generator.
      With a bit of hackery,
      yacc parses its own input language,
      as well as the language of V7's main compiler,
      the portable C compiler.
      After two decades of research,
      it seems that the parsing problem is solved.
    </p>
    <p><b>1987</b>:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    </p>
    <p><b>1991</b>:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of Earley algorithm.
      But Earley parsing is almost forgotten.
      Twenty years will pass
      before anyone writes a practical
      implementation of Leo's algorithm.
    </p>
    <p><b>1990's</b>:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is "fast but stupid".
    </p><p><b>2000</b>:
      Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    </p>
    <p><b>2002</b>:
      Aycock&Horspool publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    </p>
    <p><b>2004</b>:
      Ford publishes his paper on PEG.
      Implementers by now are avoiding YACC,
      and the Irons, Earley and  Lucas algorithms are forgotten, so it seemed
      as if there would soon be no syntax-driven algorithms in practical
      use.
      Ford's PEG fills this looming gap.
      PEG has an attractive new syntax,
      but the underlying algorithm is
      that of the old compiler-compilers,
      and nothing has been done to change
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html">
        their tricky behaviors</a>.
    </p><p><b>2006</b>:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    </p>
    <p><b>2004</b>:
      Bryan Ford introduces PEG.
      PEG is the pre-Chomsky algorithm, GTDPL,
      with two changes:
      Ford adds a new and very seductive interface,
      and he show how to use "packratting"
      so that the algortihm so is always linear.
      The capabilities, however, are still those of GTDPL,
      abandoned decades earlier because of its
      tricky behavior.
      But, Irons, Lucas and Earley are all largely forgotten,
      and with the downfall of LALR,
      GTDPL looks like it will have to do.
      <!-- Ford PEG paper -->
    </p><p><b>Today</b>:
      After five decades of parsing theory,
      the state of the are seems to be back
      where we started.
      If you took the first paper ever published describing a parsing,
      Ned Iron'original 1961 algorithm,
      changed the date,
      and translated its code from the mix of assembler and
      ALGOL into something more fashionable, say Haskell,
      you would easily republish it today,
      and bill it as
      as revolutionary and new.
    </p>
    <p>
    </p><h3>Marpa: an afterword</h3><p>
      The recollections of my teachers cover most of
      this timeline.
      My own begin around 1970.
      Very early on, as a graduate student,
      I became unhappy with the way
      the field was developing,
      and became interested in
      Earley's algorithm,
      and it was something I returned to on and off.
    </p>
    <p>
      The original vision of the 1960's was a parser that
      was
    </p><ul>
      <li>efficient,
      </li><li>practical,
      </li><li>general, and
      </li><li>syntax-driven.
      </li></ul><p>
      By 2010 this vision
      had gone the same way as many other dreams of the 1960's.
      The rhetoric stayed upbeat, but in fact
      parsing was now a field whose topic was increasingly desperate
      compromises.
    </p>
    <p>
      But early in this millenium,
      I discovered that the parser dreamed of in the 1960's
      was not just possible,
      but that the problem had been solved.
      These results were already in the literature,
      but had seen little in the way of implementation.
      Certainly they had never been implemented in a single parser.
    </p>
    <p>
      Aycock and Horspool had solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and probably no longer relevant in any case --
      caches misses are now the bottleneck.
    </p>
    <p>But while the original issues with Earley's disappeared,
      a new issue emerged.
      With a parsing algorithm as powerful as Earley's behind it,
      a syntax-driven approach can do much more than it can with
      a left parser.
      But with the experience with LALR in their collective consciousness,
      few modern programmers are prepared
      to trust a purely declarative parser.
      As Lincoln said, "Once a cat's been burned,
      he won't even sit on a cold stove."
    </p>
    <p>
      To be more easily accepted,
      Earley's needed to allow
      procedural parsing,
      not just declarative parsing.
      This also proved to be no obstacle --
      I was able to modify Earley's algorithm
      to allow procedure logic.
      Marpa (my modification of Earley's)
      allows the user to specify events --
      occurrences of symbols and rules --
      at which declarative parsing pauses.
      While paused,
      the application can call procedural logic
      and single-step forward token by token.
      The procedural logic can hand control back
      over to syntax-driven parsing at any point it likes.
      The Earley tables can provide the procedural logic with
      full knowledge of the state of the
      parse so far:
      all rules recognized
      in all possible parses so far,
      and all symbols expected.
      Earley's algorithm is now a even better companion
      for hand-written procedural logic than recursive descent.
    </p><h2>References, comments, etc.</h2>
    <p>
      For more about Marpa, there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
<br />
<p>posted at: 22:10 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/08/timeline2.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
