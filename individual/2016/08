<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Parsing: an expanded timeline</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Wed, 10 Aug 2016</h3>
<br />
<center><a name="timeline2"> <h2>Parsing: an expanded timeline</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>[ Revised 10 Aug 2016 ]
    </p>
    <p><b>The fourth century BCE</b>:
    In India, Pannini creates a grammar of Sanskrit.
    Pannini's grammar is probably the first formal system of any kind, predating Euclid.
    It is a sophisticated description of the Sanskrit language,
    exact and complete, including pronunciation.
    To this date, nothing like it exists for any other natural language of Sanskrit's,
    size and complexity,
    or with its corpus.
    Recently, Pannini's work is receiving attention,
    but in the 1950's Pannini was almost completely forgotten,
    and he had no influence on the other events in this timeline.
    </p>
    <p><b>1943</b>:
    Emil post defines and studies a formal rewriting system using
    productions.
    After two millenia, the West starts the process of catching with Pannini.
    <p><b>1952</b>:
    Grace Hopper writes a linker-loader and
    <a href="https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers">
    describes it
    as a "compiler"</a>.
    She seems to be the first person to use this term for a computer program.
    Hopper used the term
    "compiler" in its original sense:
    "something or someone that bring other things together".
    <p><b>1949</b>:
    Markov publishes an article on finite state processes.
    <p><b>1956</b>:
    Chomsky publishes a paper with the the original form
    of what is
    now known as the Chomsky hierarchy:
    <ul>
    <li>Markov's finite state processes;
    <li>Context-free grammars, and
    <li>Context-sensitive grammars.
    </ul>
    The last two seem to be Chomsky's own formulations --
    Chomsky does not cite Post's work.
    This paper marks the beginning for Western formal language theory.
    <p><b>1957</b>:
    Steven Kleene discovers regular expressions,
    a very handy notation for Markov's processes.
    These turn out also to describe exactly the same class of languages
    that is also being studied in two other forms: neural nets
    and finite state automata.
    <p><b>1957</b>:
    Noam Chomsky publishes the first book
    on his theory of language,
    <b>Syntactic Structures</b>.
    This is one of the most influential books of all time.
    In Chomsky's theory language is processed
    in multiple passes,
    and there is a morphophonemic pass,
    a syntactic pass, and a transformation pass.
    There are resemble, and inspired the lexical,
    syntactic and AST processing passes of later compilers.
    </p>
    <p>Chomsky's most important contribution is so basic
    it may be overlooked.
    At the time he wrote, linguists believed they had to work
    from the data, only constructing grammars or other representations
    as and when justified.
    Chomsky argued that this is impossible --
    the data is simply noise until there is
    a theory about its structure, and that the theory must come first.
    Chomsky made syntax-driven approaches to parsing academically 
    and philosophically respectable.
    Chomsky's effect on what follows is pervasive.
    </p>
    <p><b>1957</b>:
    John Backus' team creates FORTRAN.
    FORTRAN is not a block structured language, and is parsed line-by-line,
    using ad hoc methods.
    <p><b>1958</b>:
    John McCarthy's LISP appears.
    LISP is block structured,
    but requires the programmer to indicate the structure herself,
    with parentheses.
    <p><b>Compilers</b>:
    Readers of the literature about the 1950's will sometimes get the impression
    of an Atlantean or Panninian Age,
    a period which had access to technology more sophisticated
    that that of later times.
    This is misleading.
    The term "compiler" in the 1950's was used far more widely than it is now
    and in particular did not imply that it's output was ready to be executed by
    a computer.
    John Backus, for example,
    <a href="http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf">
    describes (pp. 133-134)</a> a 1954 "compiler"
    which produces relative addresses, that needed to be translated to
    absolute addresses by hand.
    </p>
    <p><b>1959</b>:
    Backus invents a new notation to describe
    the IAL language.
    Backus's notation is influenced by Post.
    <p><b>1960</b>:
    Peter Nauer, the author of the ALGOL 60 report, 
    improved the notation Backus used to IAL,
    and uses it to describe the new language.
    It will become known as Backus-Nauer form.
    The grammars describable by BNF are soon found to
    be exactly Chomsky's context-free grammars.
    <p><b>1960</b>:
      The ALGOL 60 report
      specifies, for the first time, a block structured
      language.
      The ALGOL committee is well aware
      that
      nobody knows how to parse such a language.
      But they believe that,
      if they specify a block-structured
      language, a parser for it will be invented.
      Risky as this approach is, it pays off ...
    </p>
    <p><b>1961</b>:
    Ned Irons publishes a paper describing his ALGOL 60
    parser.
    It is the first paper to describe any parser.
      The Irons algorithm is a left parser --
      a form of recursive descent.
      Unlike modern
      recursive descent,
      the Irons algorithm
      is general and syntax-driven.
      "General" means it can parse anything written in BNF.
      "Syntax-driven" (aka declarative) means that parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    </p>
    <p><b>1961</b>:
    Lucas publishes the first description of recursive descent
    in its pure form.
    </p>
    <p><b>Left parsing</b>:
    Lucas described a syntax-driven implementation, useable only for
    a restricted class of grammars.
    In the 1960's hand-coded approaches to recursive descent
    become more popular.
      Three factors were at work:
      <ul>
      <li>
      Memory and CPU were both extremely limited.
      Hand-coding paid off, even when the gains were small.
      <li>
      Pure left parsing, of the kind Lucas's syntax driven
      approach allowed, is a very weak parsing technique.
      It was often necessary
      to go beyond its limits.
      <li>
      Left parsing is intuitive, requiring little or
      no knowledge of parsing theory.
      This makes it a good fit for hand-coding.
      </ul>
    </p>
    <p><b>1965</b>:
    Don Knuth invents LR parsing.
      Knuth is primarily interested
      in the mathematics.
      Knuth describes a parsing algorithm,
      but it is not thought practical.
    </p>
    <p><b>1965</b>:
    McClure publishes a description of a non-Chomskian parser.
    It consists of a set of routines which, on success,
    absorb input and produce output,
    and on failure pass control on to the next option.
    <p><b>1968</b>: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's core idea is to
      track everything about the parse in tables.
      Earley's algorithm is enticing, but it has three major issues:
      <ul>
      <li>First, there is a bug in the handling of zero-length rules.
      <li>Second, it is quadratic for right recursions.
      <li>Third, the bookkeeping required to set up the tables is,
      by the standards of 1968 hardware, daunting.
      </ul>
    <p><b>1969</b>:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
    </p>
    <p><b>1972</b>:
      Aho and Ullmann describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    <p><b>1975</b>:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    </p>
    <p><b>1977</b>:
      The first "Dragon book" comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters "LALR".
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    </p>
    <p><b>1979</b>: Bell Laboratories releases Version 7 UNIX.
	V7 includes what is, by far,
		the most comprehensive, useable and easily available
		compiler writing toolkit yet developed.
	 Central to the toolkit is
	 yacc, an LALR based parser generator.
	  With a bit of hackery,
	  yacc parses its own input language,
	  as well as the language of V7's main compiler,
	  the portable C compiler.
	  After two decades of research,
	  it seems that the parsing problem is solved.
    </p>
    <p><b>1987</b>:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    </p>
    <p><b>1991</b>:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of Earley algorithm.
      But Earley parsing is almost forgotten.
      Twenty years will pass
      before anyone writes a practical
      implementation of Leo's algorithm.
    </p>
    <p><b>1990's</b>:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is "fast but stupid".
    </p><b>2000</b>:
    Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    </p>
    <p><b>2002</b>:
      Aycock&Horspool publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    </p>
    <p><b>2004</b>:
    Ford publishes his paper on PEG.
    Implementers by now are avoiding YACC,
    and the Irons, Earley and  Lucas algorithms are forgotten, so it seemed
    as if there would soon be no syntax-driven algorithms in practical
    use.
    Ford's PEG fills this looming gap.
    PEG has an attractive new syntax,
    but the underlying algorithm is
    that of the old compiler-compilers,
    and nothing has been done to change
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html">
    their tricky behaviors</a>.
    <p><b>2006</b>:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    </p>
    <p><b>2000 to today</b>:
    With the retreat from LALR comes a collapse in the
      prestige of parsing theory.
      After a half century,
      we seem to be back
      where we started.
      If you took Ned Iron's original 1961 algorithm,
      changed the names and dates,
      and translated the code from the mix of assembler and
      ALGOL into Haskell,
      you would easily republish it today,
      and bill it as 
      as revolutionary and new.
    </p>
    <p>
    <h3>Marpa</h3>
      Over the years,
      I had come back to Earley's algorithm again and again.
      Around 2010, I realized
      that the original, long-abandoned vision --
      an efficient, practical, general and syntax-driven parser --
      was now, in fact, quite possible.
      The necessary pieces had fallen into place.
    </p>
    <p>
      Aycock&Horspool have solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and probably no longer relevant in any case --
      caches misses are now the bottleneck.
    </p>
    <p>But while the original issues with Earley's disappeared,
      a new issue emerged.
      With a parsing algorithm as powerful as Earley's behind it,
      a syntax-driven approach can do much more than it can with
      a left parser.
      But with the experience with LALR in their collective consciousness,
      few modern programmers are prepared
      to trust a purely declarative parser.
      As Lincoln said, "Once a cat's been burned,
      he won't even sit on a cold stove."
    </p>
    <p>
      To be accepted, Marpa needed to allow
      procedure parsing,
      not just declarative parsing.
      So Marpa allows the user to specify events --
      occurrences of symbols and rules --
      at which declarative parsing pauses.
      While paused,
      the application can call procedural logic
      and single-step forward token by token.
      The procedural logic can hand control back
      over to syntax-driven parsing at any point it likes.
      The Earley tables can provide the procedural logic with
      full knowledge of the state of the
      parse so far:
      all rules recognized
      in all possible parses so far,
      and all symbols expected.
      Earley's algorithm is now a even better companion
      for hand-written procedural logic than recursive descent.
    <h2>References, comments, etc.</h2>
    <p>
      For more about Marpa, there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
<br />
<p>posted at: 20:20 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2017/08/timeline2.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
