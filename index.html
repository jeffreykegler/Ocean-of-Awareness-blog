<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Wed, 20 Jun 2018</h3>
<br />
<center><a name="lrecursion"> <h2>Parsing left recursions</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>Left recursion</h2>
    <p>A lot has been written about parsing left recursion.
    Unfortunately, much of it simply adds to the mystery.
    In this post, I hope to frame the subject clearly and briefly.
    <p>
    </p>I expect the reader has some idea of what left recursion is,
    and perhaps some experience of it as an issue.
    Informally, left recursion occurs when a symbol expands to something
    with itself on the left.
    This can happen directly, for example, if
    </p>
    <pre><tt>
    (1) A ::= A B
    </tt></pre>
    production <tt>(1)</tt> is in a grammar.
    Indirect left recursion happens when,
    for example,
    <pre><tt>
    (2) A ::= B C
    (3) B ::= A D
    </tt></pre>
    <tt>(2)</tt> and
    <tt>(3)</tt>
    are productions in a grammar.
    <pre><tt>
    (4) A ::= B A C
    (5) B ::= # empty
    </tt></pre>
    A grammar with productions
    <tt>(4)</tt> and
    <tt>(5)</tt>
    has a "hidden" left recursion.
    This is because
    <tt>&lt;A&gt;</tt>
    will ends up leftmost in derivations like:
    <pre><tt>
    (6) A  &#10230; B A C &#10230 A C
    </tt></pre>
    In derivation <tt>(6)</tt>,
    production <tt>(4)</tt> was applied,
    then production <tt>(5)</tt>.
    <p>For those into notation,
    a grammar is left recursive if and only if it allows a derivation of the
    form
    <pre><tt>
    (7) A  &#10230;<sup>+</sup> &beta; A &gamma; </tt> where <tt> &beta; = &epsilon;
    </tt></pre>
    In <tt>(7)</tt> <tt>&epsilon;</tt> is the empty string,
    and 
    <tt> &alpha; &#10230;<sup>+</sup> &beta;</tt>
    indicates that <tt>&alpha;</tt> derives <tt>&beta;</tt>
    in one or more rule applications.
    <h2>So, OK, what is the problem?</h2>
    <p>The problem with parsing left recursions is that if you are parsing
    using a derivation like
    <pre><tt>
    (8) A  &#10230; A B </tt>
    </tt></pre>
    then you have defined
    <tt>&lt;A&gt;</tt>
    in terms of 
    <tt>&lt;A&gt;</tt>.
    All recursions can be a problem,
    but left recursions are a particular problem because almost all practical
    parsing methods<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    proceed left to right,
    and derivations like <tt>(8)</tt> will lead many of
    the most popular algorithms straight into
    an infinite regress.
    <h2>Why do some algorithms not have a problem?</h2>
    <p>In a sense,
    all algorithms which solve the left recursion problem do
    it in the same way.
    It is just that in some,
    the solution appears in a much simpler form.
    </p>
    <p>
    The solution is at most simple in Earley's algorithm.
    That is no coincidence -- as Pingali and Bernadi<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    show,
    Earley's, despite its daunting reputation,
    is actually the most basic Chomskyan context-free parsing algorithm,
    the one from which all others derive.
    </p>
    <p>Earley's builds a table.
    The Earley table contains an initial Earley set
    and an Earley set for each token.
    The Earley set for each token
    describes the state of the parse after consuming that token.
    The basic idea is not dissimilar
    to that of the Might/Darais/Spiewak (MDS) idea of parsing by derivatives,
    and the logic for building the Earley sets resembles
    that of MDS.<a id="footnote-3-ref" href="#footnote-3">[3]</a>
    <p>
    For the purpose of studying left recursion,
    what matters is that
    each Earley set contains Earley "items".
    Some of the items are called predictions
    because they predict the occurrence of a symbol
    at that location in the input.
    </p>
    To record a left recursion in an Earley set,
    the program adds
    a prediction item for the left recursive symbol.
    It is that simple.
    </p>
    Multiple occurrences of a prediction item would be identical,
    and therefore useless.
    Therefore subsequent attempts
    to add the same prediction item are ignored,
    and recursion does not occur.
    </p>
    <h2>If some have no problem, why do others?</h2>
    <p>Besides Earley's,
    a number of other algorithms handle left recursion without
    any issue -- notably LALR 
    (aka <tt>yacc</tt> or <tt>bison</tt>) and LR.
    This re-raises the original question:
    why do some algorithms have a left recursion problem?
    </p>
    <p>The worst afflicted algorithms are the "top-down"
    parsers.
    The best known of these is recursive descent --
    a parsing methodology which, essentially, does parsing
    by calling a subroutine to handle each symbol.
    In the traditional implementation of recursive descent,
    left recursion is very problematic.
    To illustrate, if you are writing the function <tt>parse_A()</tt>
    and have a rule
    <pre><tt>
    (9) A ::= A B
    </tt></pre>
    the first thing you need do in
    <tt>parse_A()</tt>
    is to call <tt>parse_A()</tt>.
    Which must call <tt>parse_A()</tt>.
    And so, in the naive implementation, on and on forever.
    <h2>The fixed-point solution to left recursion</h2>
    <p>Over the years,
    many ways to solve the top-down left recursion issue have been
    announced.
    The MDS solution is one of the more interesting --
    interesting because it actually works<a id="footnote-4-ref" href="#footnote-4">[4]</a>,
    and because it describes all the others,
    including the Earley algorithm solution.
    MDS reduces the problem to
    the more general one of finding a "fixed point" of the recursion.
    </p>
    <p>In math, the "fixed point" of a function is an argument of
    the function which is equal to its value for that argument --
    that is, an <tt>x</tt> such that <tt>f(x)&nbsp;=&nbsp;x</tt>.
    MDS describes an algorithm which "solves" the left recursion
    for its fixed point.
    That "fixed point" can then be memoized.
    For example the value of <tt>parse_A</tt>
    can be the memoized "fixed point" value of
    <tt>&lt;A&gt;</tt>.
    </p>
    <p>The Earley solution of left recursion was, in fact, an optimized
    "fixed point".
    The computation of an Earley is the application of a set
    of rules for adding Earley items.
    This continues until no more Earley items can be added.
    In other words, the rules for building an Earley set
    are applied until they find
    their "fixed point".<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </p>
    <h2>Other top-down solutions</h2>
    <p>The MDS fixed point solution <b>does</b>
    the job,
    but as described in their paper it requires a functional
    programming language to implement,
    and it is expensive.
    In the worst case, the MDS approach is exponential,
    although they conjecture that it is linear for a large
    class of practical grammars.
    </p>
    <p>Top-down algorithms can take an "in-between strategy" --
    they can tackle those left recursions that are cheap to
    find, without computing the full "fixed point".
    Here a well-defined boundary is crucial:
    A programmer wants to know if their particular grammar will
    work,
    and whether small tweaks to their grammar will break it.
    </p>
    <p>
    Top-down can be seen as
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/12/topdown.html">
    a "guessing" strategy with hacks</a>.
    Hacks are always needed in top-down, because the input is at the bottom,
    not the top, and a useful top-down algorithm needs to look at the input.
    But the hacks can be as simple as lookahead,
    and lookahead can be implemented without seriously compromising
    the simplicity and flexibility of the original top-down approach.
    </p>
    <p>With detection and fixing of left-recursion,
    the "hack" part of the top-down strategy becomes very complicated.
    The attraction of top-down is its simplicity,
    and its resulting adapability to procedural logic.
    The point can be reached where the original strategy
    comes into question.
    </p>
    <p>
    After all,
    a recursive descent parser can straightforwardly take care of left recursion
    issues by calling an Earley parser.
    But in that case,
    why not simply use Earley's?
    </p>
    <h2>Comments, etc.</h2>
      Marpa is my own implementation of an Earley parser.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on its IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
    I probably could have said "all practical parsing methods"
    instead of "almost all".
    Right-to-left parsing methods exist,
    but they see little use.
    In any case, they only reverse the problem.
    Parsing in both directions is certainly possible but,
    as I will show,
    we do not have to go to quite that much trouble.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
        Keshav Pingali and Gianfranco Bilardi, UTCS tech report TR-2012.
        2012.
        <a href="https://apps.cs.utexas.edu/tech_reports/reports/tr/TR-2102.pdf">
          PDF accessed 9 Junk 2018</a>.
        <a href="https://www.youtube.com/watch?v=eeZ3URxd8Wc">
          Video accessed 9 June 2018</a>.
        Less accessible is
        Keshav Pingali and Gianfranco Bilardi,
        "A graphical model for context-free grammar parsing."
        Compiler Construction - 24th International Conference, CC 2015.
        Lecture Notes in Computer Science,
        Vol. 9031, pp. 3-27, Springer Verlag, 2015.
        <a href="https://www.researchgate.net/publication/286479583_A_Graphical_Model_for_Context-Free_Grammar_Parsing">
          PDF accessed 9 June 2018</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3"><b>3.</b>
        Matthew Might, David Darais and Daniel Spiewak.
	"Functional Pearl: Parsing with Derivatives."
	International Conference on Functional Programming 2011 (ICFP 2011).
	Tokyo, Japan. September, 2011. pages 189--195.
        <a href="http://matt.might.net/papers/might2011derivatives.pdf">
          PDF accessed 9 Jun 2018</a>.
        <a href="http://matt.might.net/papers/might2011derivatives-icfp-talk.pdf">
          Slides accessed 9 June 2018</a>.
        <a href="http://matt.might.net/media/mattmight-icfp2011-derivatives.mp4">
          Video accessed 9 June 2018</a>.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4"><b>4.</b>
    There have been many more attempts than implementations
    over the years,
    and even some of the most-widely used
    implementations <a href="https://www.youtube.com/watch?v=lFBEf0o-4sY&feature=youtu.be&t=6m29s">
    have
    their issues.</a>
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5"><b>5.</b>
    Recall that potential left recursions are 
    recorded as "predictions" in Earley's algorithm.
    Predictions recurse,
    but since they do not depend on the input,
    they can be precomputed.
    This means that Earley implementations can
    bring each Earley set to its fixed point
    very quickly.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6"><b>6.</b>
        Marpa has a stable implementation.
        For it, and for more information on Marpa, there are these resources:<br>
        <a href="http://savage.net.au/Marpa.html">
          Marpa website, accessed 25 April 2018</a>.</br>
        <a href="https://jeffreykegler.github.io/Marpa-web-site/">
          Kegler's website, accessed 25 April 2018</a>.</br>
        <a href="https://github.com/jeffreykegler/Marpa--R2">
          Github repo, accessed 25 April 2018.</a></br>
        <a href="https://metacpan.org/pod/Marpa::R2">
          MetaCPAN, accessed 30 April 2018.</a>.</br>
	  There is also a theory paper for Marpa:
        Kegler, Jeffrey.
        "Marpa, A Practical General Parser: The Recognizer.", 2013.
        <a href="http://dinhe.net/~aredridel/.notmine/PDFs/Parsing/KEGLER,%20Jeffrey%20-%20Marpa,%20a%20practical%20general%20parser:%20the%20recognizer.pdf">
          PDF accessed 24 April 2018</a>.
 <a href="#footnote-6-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 09:15 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/lrecursion.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 18 Jun 2018</h3>
<br />
<center><a name="combinator"> <h2>Marpa and combinator parsing</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>The missing part</h2>
    <p>
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/csg.html">
    A previous post</a>
    described how to use the current stable Marpa
    implementation as a better procedural parser.
    This post describes how the Marpa algorithm can be used as the basis
    of better combinator parsers.
    </p>
    <p>In the post on procedural parsing,
    the subparsers<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    were like combinators,
    in that they could be called recursively,
    so that a parse could be built up from components.
    Like combinators,
    each child could return,
    not just a parse,
    but a set of parses.
    And, as in combinators, once a child combinator
    returned its value,
    the parent parser could resume parsing
    at a location specified by the child combinator.
    So what was missing?
    <p>A combinator,
    in order to handle ambiguity,
    returns not a subparse, but a set of subparses.
    In the full combinator model,
    each subparse can have its own "resume location".<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    The procedural parsing post did not provide for multiple
    resume locations.
    We will now proceed to make up for that.
    </p>
    <h2>How it works</h2>
    <p>The Marpa parser has the ability to accept
    multiple subparses,
    each with its own length.
    This allows child subparses to overlap in any fashion,
    forming a mosaic as complex as the application needs.
    </p>
    </p>An Earley parser is table-driven --
    its parse tables consists of Earley sets,
    with an initial Earley set
    and one Earley set per token.
    This makes for a very simple idea of location.
    Location 0 is the location of the initial Earley set.
    Location <tt>N</tt> is the location of the Earley set after the <tt>N</tt>'th
    token has been consumed.
    </p>
    <p>Simplicity is great, but unfortunately
    this won't work for variable-length
    tokens.
    To handle those, Marpa introduces another idea of location:
    the <b>earleme</b>.
    Like Earley set locations,
    the earlemes begin at 0,
    and advance in integer sequence.
    Earley set 0 is always at earleme 0.
    Every Earley set has an earleme location.
    On the other hand,
    not every earleme has a corresponding Earley set --
    there can be "empty" earlemes.
    </p>
    <p>The lower-level interface for Marpa is Libmarpa.
    Every time Libmarpa adds a token,
    a length in earlemes must be specified.
    In the most-used higher level Marpa interfaces,
    this "earleme length" is always 1,
    which makes the Libmarpa location model collapse into the traditional one.
    </p>
    <p>
    The Libmarpa recognizer advances earleme-by-earleme.
    In the most-used higher level Marpa interfaces,
    a token ends at every earleme
    (unless of course that earleme is after end-of-input).
    This means that the most-used Marpa interfaces
    create a new Earley set every time they advance one earleme.
    Again, in this case, the Libmarpa model collapses into
    the traditional one.
    </p>
    <p>In Libmarpa and other lower-level interfaces,
    there may be cases where
    <ul>
    <li>one or more tokens
    end after the current earleme, but</li>
    <li>no tokens end <b>at</b> the current earleme.</li>
    </ul>
    In such cases the current earleme will be empty.
    </p>
    <p>This is only an outline of the basic concepts behind the
    Marpa input model.
    The formalisms are in the Marpa theory paper.<a id="footnote-3-ref" href="#footnote-3">[3]</a>
    The documentation for Libmarpa and Marpa's other low-level interfaces contains
    more accessible,
    but detailed, descriptions.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </p>
    <h2>Value added</h2>
    <h3>Left-eidetic information</h3>
    <p>As readers of my previous posts<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    will know,
    Marpa is "left-eidetic" -- the application has access to everything to its left.
    This is an advantage over the traditional implementation of combinator parsing,
    where parse information about the left context may be difficult
    or impossible to access.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </p>
    <h3>More powerful linear-time combinators</h3>
    <p>Marpa parses a superset of LR-regular grammars in linear time,
    which makes it a more powerful "building block"
    than traditionally available for combinator parsing.
    This gives the programmer of a combinator parser more options.
    </p>
    <h3>State of the art worse-than-linear combinators</h3>
    <p>In special circumstances, programmers may want to use subparsers 
    which are worse than linear -- for example, they may know that
    the string is very short.
    Marpa parses context-free grammars in state of the art time.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </p>
    <h2>The code, comments, etc.</h2>
      To learn more about Marpa,
      a good first stop is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
    In some of the descriptions of Marpa's procedural
    parsing, these subparsers are called "lexers".
    This emphasizes the usual case in current practice,
    where the subparsers are the bottom layer of the
    parsing application,
    and do not invoke their own child subparsers.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
    In notational terms, a full combinator is a function of the form
    <br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <tt>A* &#8594; &#8473;( P &#215; A* )</tt>,
    <br>
    where <tt>A</tt> is the alphabet of the grammar;
    <tt>P</tt> is a representation of a single parser
    (for example, a parse tree);
    <tt>&#8473;(X)</tt> is the power set of a set <tt>X</tt>:
    and
    <tt>X &#215; Y</tt> is the Cartesian product
    of sets <tt>X</tt> and <tt>Y</tt>.
    The subparsers
    of the procedural parsing post
    were of the form
    <br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <tt>A* &#8594; &#8473;( P ) &#215; A*</tt>.
    <br>
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3"><b>3.</b>
    Kegler, Jeffrey.<a
    href="http://dinhe.net/~aredridel/.notmine/PDFs/Parsing/KEGLER,%20Jeffrey%20-%20Marpa,%20a%20practical%20general%20parser:%20the%20recognizer.pdf">
    "Marpa, a Practical General Parser: The Recognizer".</a>
    2013.
    Section 12, "The Marpa input model", pp. 39-40.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4"><b>4.</b>
    Libmarpa API document,
    <a href="http://jeffreykegler.github.io/Marpa-web-site/libmarpa_api/stable/api_one_page.html#Input">
    the "Input" section</a>.
    Marpa::R2's NAIF interface allows
    access to the full Libmarpa input model
    and its documentation contains
    <a href="https://metacpan.org/pod/distribution/Marpa-R2/pod/Advanced/Models.pod">
    a higher-level description of Marpa's alternative input models.</a>
    There is also a thin Perl interface to Libmarpa,
    <a href="http://jeffreykegler.github.io/Marpa-web-site/libmarpa_api/stable/api_one_page.html#Input">the THIF interface</a>,
    which allows full access to the alternative input models.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5"><b>5.</b>
    For example, the
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/csg.html">
    post on procedural parsing</a> contains a good,
    simple, example of the use of Marpa's left-eideticism.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6"><b>6.</b>
    For best effect,
    left-eidetism and functional purity
    probably should be used in combination.
    For the moment at least,
    I am focusing on explaining the capabilities,
    and leaving it to others to find the monadic
    or other solutions that will allow programmers to leverage
    this power in functionally pure ways.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7"><b>7.</b>
    Specifically O(n^2) for unambiguous grammars,
    and O(n^3) for ambiguous grammars.
 <a href="#footnote-7-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 08:29 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/combinator.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sun, 17 Jun 2018</h3>
<br />
<center><a name="csg"> <h2>Marpa and procedural parsing</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>Procedural parsing</h2>
    <p>Marpa is an Earley-based parser,
      and Earley parsers are typically not good at procedural parsing.
      Many programmers are used to recursive descent (RD),
      which has been state-of-the-art in terms of
      its procedural programming capabilities --
      it was these capabilities which led to
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/fast_power.html">
      RD's
      triumph over the now-forgotten Irons algorithm.</a>
    </p>
    <p>
      Marpa, however, has a parse engine expressly redesigned<a id="footnote-1-ref" href="#footnote-1">[1]</a>
      to handle procedural logic well.
      In fact, Marpa is <b>better</b> at procedural logic
      than RD.
    </p>
    <h2>A context-sensitive grammar</h2>
    <p>Marpa parses all LR-regular grammars in linear time,
    so the first challenge is to find a grammar
    that illustrates a
    <b>need</b> for procedural logic, even when Marpa is used.
    The following is the canonical example of a grammar that is
    context-sensitive, but not context-free:
    </p>
    <pre>
          a^n . b^n . c^n : n >= 1
    </pre>
    I will call this the "ABC grammar".
    It is a sequence of
    <tt>a</tt>'s,
    <tt>b</tt>'s, and
    <tt>c</tt>'s,
    in alphabetical order,
    where the character counts are all
    equal to each other and greater
    than zero.
    </p>
    <p>The ABC "grammar" is really a counting problem more than
    a natural parsing problem,
    and parsing is not the fastest or easiest way to solve it.
    Three tight loops, with counters, would do the same job nicely,
    and would be much faster.
    But I chose the ABC grammar for exactly this reason.
    It <b>is</b> simple in itself,
    but it is tricky when treated as a parsing problem.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    </p>
    <p>
    In picking the strategy below,
    I opted for one that illustrates
    a nice subset of Marpa's procedural parsing capabilities.
    Full code is
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/gh-pages/code/csg">
    on-line<a>,
    and readers are encouraged to "peek ahead".
    </p>
    <h2>Step 1: the syntax</h2>
    <p>Our strategy will be to start with a context-free syntax,
    and then extend it with procedural logic.
    Here is the context-free grammar:
    </p>
    <pre><tt>
    lexeme default = latm => 1
    :default ::= action => [name,start,length,values]
    S ::= prefix ABC trailer
    ABC ::= ABs Cs
    ABs ::= A ABs B | A B
    prefix ::= A*
    trailer ::= C_extra*
    A ~ 'a'
    B ~ 'b'
    :lexeme ~ Cs pause => before event => 'before C'
    Cs ~ 'c' # dummy -- procedural logic reads <Cs>
    C_extra ~ 'c'
    </tt></pre>
    <p>The first line is boiler-plate:
    It turns off a default which was made pointless
    by a later enhancement to Marpa::R2.
    Marpa::R2 is stable, and backward-compatibility is
    a very high priority.
    <pre><tt>
    :default ::= action => [name,start,length,values]
    </tt></pre>
    <p>
    We will produce a parse tree.
    The second line defines its format --
    each node is an array whose elements are,
    in order,
    the node name, its start position,
    its length and its child nodes.
    </p>
    <pre><tt>
    S ::= prefix ABC trailer
    </tt></pre>
    <p>The symbol <tt>&lt;ABC&gt;</tt>
    is our "target" -- the counted
    <tt>a</tt>'s,
    <tt>b</tt>'s,
    and <tt>c</tt>'s.
    To make things a bit more interesting,
    and to make the problem more like a parsing problem instead of a counting problem,
    we allow a prefix of <tt>a</tt>'s
    and a trailer of <tt>c</tt>'s.
    </p>
    <pre><tt>
    ABC ::= ABs Cs
    </tt></pre>
    <p>We divide the
    <tt>&lt;ABC&gt;</tt> target into two parts:
    <tt>&lt;ABs&gt;</tt>, which contains the
    <tt>a</tt>'s,
    and <tt>b</tt>'s;
    and
    <tt>&lt;Cs&gt;</tt>, which contains
    the <tt>c</tt>'s.
    </p>
    <p>
    The string
    </p>
    <pre><tt>
    a^n . b^n
    </tt></pre>
    <p>
    is context free, so that we can handle it
    without procedural logic, as follows:
    </p>
    <pre><tt>
    ABs ::= A ABs B | A B
    </tt></pre>
    <p>
    The line above recognizes a non-empty string of
    <tt>a</tt>'s,
    followed by an equal number
    of <tt>b</tt>'s.
    </p>
    <pre><tt>
    prefix ::= A*
    trailer ::= C_extra*
    </tt></pre>
    <p>As stated above,
    <tt>&lt;prefix&gt;</tt>
    is a series of <tt>a</tt>'s and
    <tt>&lt;trailer&gt;</tt>
    is a series of <tt>c</tt>'s.
    </p>
    <pre><tt>
    A ~ 'a'
    B ~ 'b'
    </tt></pre>
    <p>Marpa::R2 has a separate lexical and syntactic phase.
    Here we define our lexemes.
    The first two are simple enough:
    <tt>&lt;A&gt;</tt> is the character "<tt>a</tt>"; and
    <tt>&lt;B&gt;</tt> is the character "<tt>b</tt>".
    </p>
    <pre><tt>
    :lexeme ~ Cs pause => before event => 'before C'
    Cs ~ 'c' # dummy -- procedural logic reads <Cs>
    C_extra ~ 'c'
    </tt></pre>
    <p>
    For the character "<tt>c</tt>",
    we need procedural logic.
    As hooks for procedural logic,
    Marpa allows a full range of events.
    Events can occur on prediction and completion of symbols;
    when symbols are nulled;
    before lexemes;
    and after lexemes.
    The first line in the above display
    declares a "before lexeme" event
    on the symbol
    <tt>&lt;Cs&gt;</tt>.
    The name of the event is "<tt>before C</tt>".
    </p>
    <p>The second line is a dummy entry,
    which is needed to allow the "<tt>before C</tt>"
    event to trigger.
    The entry says that
    <tt>&lt;Cs&gt;</tt> is a single character "<tt>c</tt>".
    This is false --
    <tt>&lt;Cs&gt;</tt> is a series of one or more
    <tt>c</tt>'s,
    which needs to be counted.
    But when
    the "<tt>before C</tt>" event triggers,
    the procedural
    logic will make things right.
    </p>
    <p>The third line defines
    <tt>&lt;C_extra&gt;</tt>, which
    is another lexeme for the character "<tt>c</tt>".
    We have two different lexemes for
    the character <tt>c</tt>, because we want some
    <tt>c</tt>'s (those in the target)
    to trigger events;
    and we want other
    <tt>c</tt>'s (those in the trailer)
    not to trigger events,
    but to be consumed by Marpa directly.
    </p>
    <h2>The procedural logic</h2>
    <p>
    At this point, we have solved part of the problem with context-free syntax,
    and set up a Marpa event named "<tt>before C</tt>",
    which will solve the rest of it.
    </p>
    <pre><tt>
    my $input_length = length ${$input};
    for (
        my $pos = $recce->read($input);
        $pos < $input_length;
        $pos = $recce->resume()
      )
    {
      </tt><b>... Process events ...</b><tt>
    }
    </tt></pre>
    <p>Processing of events takes place inside a Marpa read loop.
    This is initialized with a <tt>read()</tt> method,
    and is continued with a <tt>resume()</tt> method.
    The <tt>read()</tt> and <tt>resume()</tt> methods
    both return the current position
    in the input.
    If the current position is end-of-input, we are done.
    If not, we were interrupted by an event, which we
    must process.
    </p>
    <pre><tt>
    </tt><b>Process events</b><tt>

    EVENT:
      for (
	  my $event_ix = 0 ;
	  my $event    = $recce->event($event_ix) ;
	  $event_ix++
	)
      {
	  my $name = $event->[0];
	  if ( $name eq 'before C' ) {
	      </tt><b>... Process "before C" event ...</b><tt>
	  }
	  die qq{Unexpected event: name="$name"};
      }
    </tt></pre>
    <p>In this application, only one event can occur at any location,
    so the above loop is "overkill".
    It loops through the events, one by one.
    The <tt>event</tt> method returns a reference to an array
    of event data.
    The only element we care about is the event name.
    In fact, if we weren't being careful about error checking,
    we would not even care about the event name,
    since there can be only one.
    </p>
    <p>If, as expected, the event name is "<tt>before C</tt>",
    we process it.
    In any other case, we die with an error message.
    </p>
    <pre><tt>
    </tt><b>Process "before C" event</b><tt>

    my ( $start, $length ) = $recce->last_completed_span('ABs');
    my $c_length = ($length) / 2;
    my $c_seq = ( 'c' x $c_length );
    if ( substr( ${$input}, $pos, $c_length ) eq $c_seq ) {
	$recce->lexeme_read( 'Cs', $pos, $c_length, $c_seq );
	next EVENT;
    }
    die qq{Too few C's};
    </tt></pre>
    <p>This is the core part of our procedural logic,
    where we have a "<tt>before C</tt>" event.
    We must
    <ul>
    <li>determine the right number of <tt>c</tt> characters;</li>
    <li>check that the input has
      the right number of <tt>c</tt> characters;</li>
    <li>put together a lexeme to feed the Marpa parser; and</li>
    <li>return control to Marpa.</li>
    </ul>
    There is a lot going on,
    and some of Marpa's most powerful capabilities for assisting
    procedural logic are shown here.
    So we will go through the above display in detail.
    </p>
    <h3>Left-eidetic</h3>
    <pre><tt>
    my ( $start, $length ) = $recce->last_completed_span('ABs');
    my $c_length = ($length) / 2;
    </tt></pre>
    <p>Marpa claims to be "left-eidetic",
    that is, to have full knowledge of the parse so far,
    and to make this knowledge available to the programmer.
    How does a programmer cash in on this promise?
    <p>Of course, there is
    <a href="https://metacpan.org/pod/distribution/Marpa-R2/pod/Progress.pod">a fully general interface</a>,
    which allows you to go through the Earley tables and extract
    the information in any form necessary.
    But another, more convenient interface,
    fits our purposes here.
    Specifically,
    </p>
    <ul><li>we want to determine how many <tt>c</tt> characters we are looking for.</li>
    <li>How many <tt>c</tt> characters we are looking for depends
    on the number of
    <tt>a</tt> and <tt>b</tt> characters that we have already seen
    in the target.</li>
    <li>The <tt>a</tt> and <tt>b</tt> characters that we have already seen in the
    target are in the
    <tt>&lt;ABs&gt;</tt> symbol instance.</li>
    <li>So, what we want to know is the length of the
    most recent <tt>&lt;ABs&gt;</tt> symbol instance.</li>
    </ul>
    </p>
    <p>Marpa has a <tt>last_completed_span()</tt> method,
    and that is just what we need.
    This finds the most recent instance of a symbol.
    (If there had been more than one most recent instance,
    it would have found the longest.)
    The <tt>last_completed_span()</tt> method returns the start
    of the symbol instance (which we do not care about)
    and its length.
    The desired number of <tt>c</tt> characters,
    <tt>$c_length</tt>, is half the length of the
    <tt>&lt;ABs&gt;</tt> instance.
    </p>
    <h3>External parsing</h3>
    <pre><tt>
    my $c_seq = ( 'c' x $c_length );
    if ( substr( ${$input}, $pos, $c_length ) eq $c_seq ) { </tt><b>...</b><tt> }
    </tt></pre>
    <p>Marpa allows external parsing.
    You can pause Marpa, as we have done,
    and hand control over to another parser -- including
    another instance of Marpa.
    </p>
    <p>
    Here external parsing is necessary to make our parser
    context-sensitive,
    but the external parser does not have to be fancy.
    All it needs to do is
    some counting -- not hard,
    but something that a context-free grammar cannot do.
    </p>
    <p>
    <tt>$pos</tt> is the current position in the input,
    as returned by the <tt>read()</tt> or <tt>resume()</tt>
    method in the outer loop.
    Our input is the string referred to by <tt>$input</tt>.
    We just calculated <tt>$c_length</tt> as the number of
    <tt>c</tt> characters required.
    The above code checks to see that the required number of
    <tt>c</tt> characters is at <tt>$pos</tt> in the input.
    </p>
    <h3>Communicating with Marpa</h3>
    <pre><tt>
	$recce->lexeme_read( 'Cs', $pos, $c_length, $c_seq );
    </tt></pre>
    <p>
    Our external logic is doing the parsing,
    but we need to let Marpa know what we are finding.
    We do this with the <tt>lexeme_read()</tt> method.
    <tt>lexeme_read()</tt> needs to know what symbol we are reading
    (<tt>Cs</tt> in our case);
    and its value
    (<tt>$c_seq</tt> in our case).
    </p>
    <p>
    Marpa requires that
    every symbol be tied in some way to the input.
    The tie-in is only for error reporting,
    and it can be hack-ish or completely artificial,
    if necessary.
    In this application, our symbol instance is tied into
    the input in a very natural way --
    it is the stretch of the input that we compared
    to <tt>$c_seq</tt> in the display before last.
    We therefore tell Marpa
    that the symbol is at <tt>$pos</tt> in the input,
    and of length <tt>$c_length</tt>.
    </p>
    <h3>Passing control back to Marpa</h3>
    <pre><tt>
	next EVENT;
    </tt></pre>
    <p>
    External parsing can go on quite a long time.
    In fact, an external parser <b>never</b> has to hand
    control back to Marpa.
    But in this case, we are done very quickly.
    </p>
    <p>
    We ask for the next iteration of the <tt>EVENT</tt>
    loop.
    (In this code,
    there will not be a next iteration, unless there is an error.)
    Once done, the <tt>EVENT</tt> loop will hand control
    over to the outer loop.
    The outer loop will call the <tt>resume()</tt>
    method to return control back to Marpa.
    </p>
    <h2>The code, comments, etc.</h2>
    <p>The full code for this example is 
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/gh-pages/code/csg">
    on-line<a>.
      There is a lot more to Marpa, including
      more facilities for adding procedural logic to your Marpa parsers.
      To learn more about Marpa,
      a good first stop is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
      To handle procedural logic well,
      an Earley engine needs to complete its Earley sets
      in strict order --
      that is, Earley set <tt>N</tt>
      cannot change after work on Earley set <tt>N+1</tt>
      has begun.
      I have not looked at every Earley parse engine,
      and some may have had this strict-sequencing property.
      And many of the papers are agnostic about the order
      of operations.
      But Marpa is the first Earley parser to recognize
      and exploit strict-sequencing as a feature.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
    The ABC grammar, in fact,
    is not all that easy or natural to describe
    even with a context-sensitive phrase structure description.
    A solution is given on Wikipedia:
    <a href="https://en.wikipedia.org/wiki/Context-sensitive_grammar#Examples">
    https://en.wikipedia.org/wiki/Context-sensitive_grammar#Examples</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 20:02 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/csg.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 11 Jun 2018</h3>
<br />
<center><a name="pingali"> <h2>Parsing with pictures</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>Derivatives == Earley?</h2>
    <p>In a cordial Twitter exchange with Matt Might and
      and David Darais, Prof. Might asked if I was interested
      in looking at their derivatives-based approach.
      I answered that I was looking at it --
      Marpa is an optimization of the Might/Darais approach.
    </p>
    <p>This may sound strange.
      At first glance, our two algorithms seem about as different as
      parsing algorithms can get.
      My Marpa parser is an Earley parser, table-driven,
      and its parse engine is written in C language.<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    </p>
    <p>
      The MDS (Might/Darais/Spiewak) parser is
      an extension of regular expressions
      which constructs states on the fly.
      MDS uses combinators and has implementations
      in several functional programming languages.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    </p>
    <h2>Grammar Flow Graphs</h2>
    <p>Why then do I imagine that Marpa is an optimized version of the MDS
      approach?
      The reason is a paper sent to
      me by Prof. Keshav Pingali at Austin: "Parsing with Pictures".<a id="footnote-3-ref" href="#footnote-3">[3]</a>
      The title is a little misleading:
      their approach is not
      <b>that</b>
      easy,
      and the paper requires a considerable amount of math.
      But it is a lot easier than the traditional way
      of learning the various approaches to parsing.
    </p>
    <p>The basis of the Pingali-Bilardi approach
      is the Grammar Flow Graph (GFG).
      These GFGs are the "pictures" of their title.
      GFGs are NFAs with recursion added.
      As has long been known,
      adding recursion to NFAs
      allows them to represent any context-free language.
    </p>
    <p>Pingali and Bilardi's next step is new.
      A GFG can be "simulated" using the same algorithm
      used to simulate an NFA.
      However, the result is not immediately impressive.
      The simulation does produce a recognizer, but not a good recognizer:
      Some of the strings recognized by
      the GFG simulator
      are not in the context-free language.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </p>
    <p>To repair their "simulation",
      Pingali and Bilardi add a tag to each state.
      This tag tracks where the recursion began.<a id="footnote-5-ref" href="#footnote-5">[5]</a>.
      This not only fixes the recognizer,
      but the added information is enough to allow the set
      of parse trees to be efficiently recovered from the
      sets of GFG states.
      In other words, with tags, the GFG recognizer now is a parser.
    </p>
    <p>
      It turns out that this recognizer-turned-parser is not new.
      In fact, it is
      <b>exactly</b>
      Earley's algorithm.
    </p>
    <p>Pingali and Bilardi do not stop there.
      Using their new framework,
      they go on to show that all LL-based and LR-based algorithms
      are simplifications of their Earley parser.
      From this point of view,
      Earley parsing is the foundation of all context-free parsing,
      and LL- and LR-based algorithms are Earley optimizations.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </p><h2>Step 1: The MDS algorithm</h2>
    <p>
      To show that Marpa is an optimization of the MDS approach,
      I will start with the MDS algorithm, and attempt to optimize it.
      For its functional programming language,
      the MDS paper uses Racket.
      The MDS parser is described directly,
      in the usual functional language manner,
      as a matching
      operation.
    </p>
    <p>
      In the MDS paper,
      the MDS parser is optimized with laziness and memoization.
      Nulls are dealt with by computing their fixed points on the fly.
      Even with these three optimizations,
      the result is still highly inefficient.
      So,
      as an additional step, MDS also
      implements "deep recursive simplication" -- in effect,
      strategically replacing laziness with eagerness.
      With this the MDS paper conjectures that the algorithm's time
      is linear for a large class of practical grammars.
    </p>
    <h2>Step 2: Extended regular expressions</h2>
    <p>
      Next, we notice that the context-free grammars
      of the MDS algorithm
      are regular expressions extended to allow recursion.
      Essentially, they are
      GFG's translated into Racket match expressions.
      The equivalence is close enough that
      you could imagine the MDS paper using GFG's
      for its illustrations.
    </p>
    <h2>Step 3: Simulating an NFA</h2>
    <p>
      Unsurprisingly, then,
      the MDS and GFG approaches are similar in their first step.
      Each consumes a single character to produce a
      "partial parse".
      A partial parse, for both of these algorithms,
      can be represented as a duple.
      One element of the duple is a string representing the
      unconsumed input.
      The other is a representation of a set of parse trees.
      In the case of MDS,
      in keeping with its functional approach,
      the set of parse trees is represented directly.
      In the case of the GFG-simulator,
      the set of parse trees is compressed into a sequence of
      GFG-state-sets.
      There is one GFG-state-set for the start of parsing,
      and one for each consumed character.
    </p>
    <h2>Step 4: Earley's Algorithm</h2>
    <p>At this point,
      with the introduction of the GFG state-sets to represent parse-trees,
      the MDS algorithm and its optimized GFG equivalent have
      "forked".
      Recall from above,
      that this GFG simulator has a bug --
      it is over-liberal.
    </p>
    <p>
      The bug is the one already described,
      and our fix is the one already described:
      Each GFG state either starts a recursion or is part of one.
      We fix the bug by tagging each GFG state with
      the index of the GFG state-set that starts its recursion.
      Once these tags are added,
      the GFG state-sets are exactly Earley sets.
    </p>
    <h2>Step 5: The Leo optimization</h2>
    <p>Next we incorporate an optimization by Joop Leo,
      which makes Earley parsing linear for all LR-regular grammars,
      without using lookahead.
      Since LR-regular is a superset of LR(k) and LL(k),
      including LL(*),
      we do not bother with lookahead.
    </p>
    <h2>Step 6: Marpa</h2>
    <p>To get from an Earley/Leo parser to a Marpa parser,
      we need to address one more major point.
      In modern parsing practice,
      programmers expect the ability to introduce procedural logic,
      even to the point of switching parsers.
      By ensuring that processing each Earley set is complete
      before processing on the next Earley set begins,
      we accomplish this.
    </p>
    <p>
      This means that Marpa has available full information
      about the parse so far -- it is left-eidetic.
      Error reporting is unexcelled,
      and procedural logic can use this information as well.
      For example,
      full parse information implies full knowledge of which
      tokens are expected next.
    </p>
    <p>
      This means that you can write an liberal HTML parser,
      starting with a very illiberal HTML grammar.
      When the illiberal parser encounters a point where it cannot
      continue because of a missing token,
      procedural logic can ask
      what the expected token is;
      concoct that token on the fly;
      supply that token to the illiberal parser;
      and then ask the illiberal parser to continue.
      This is called the "Ruby Slippers" technique,
      and an HTML parser based on it has been implemented.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </p>
    <h2>The way forward</h2>
    <p>
      As mentioned,
      the MDS algorithm has its own approach to optmization,
      one which takes maximum advantage of functional programming.
      In contrast, Marpa relies on C level coding.
    </p>
    <p>One example of the contrast in optimization techniques
      is null handling.
      Recall from above that, to deal with null processing,
      the MDS algorithm uses a fixed-point
      algorithm on the fly.
      Marpa, on the other hand,
      before parsing begins.
      precomputes a list of nullable
      symbols using bit vectors.
      In this particular case, Marpa will usually be the winner.
    </p>
    <p>A second case in which hand optimization wins is the
      all-important Leo optimization.
      Simon Peyton-Jones is a very smart man,
      but nonetheless I believe that
      the day that GHC will look at a functional specification
      of an Earley parser and rediscover the Leo optimization
      at compile time is far off.
    </p>
    <p>
      On the other hand,
      I do not imagine that hand optimization and/or C language is the
      winner in all cases.
      A programmer is deceiving himself if he imagines that he can spot all the cases where
      lazy evaluation or memoization will be effective in the general case.
      And of course,
      even an omniscient programmer is not going to be there at run-time
      to do "just in time" optimization.
      Perhaps the optimal parser of the future will combine important hand optimizations
      with functional programming.
    </p>
    <h2>Comments, etc.</h2>
    <p>
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
        Kegler, Jeffrey.
        "Marpa, A Practical General Parser: The Recognizer.", 2013.
        <a href="http://dinhe.net/~aredridel/.notmine/PDFs/Parsing/KEGLER,%20Jeffrey%20-%20Marpa,%20a%20practical%20general%20parser:%20the%20recognizer.pdf">
          PDF accessed 24 April 2018</a>.
        <br><br>
        Marpa has a stable implementation.
        For it, and for more information on Marpa, there are these resources:
        <a href="http://savage.net.au/Marpa.html">
          Marpa website, accessed 25 April 2018</a>.
        <a href="https://jeffreykegler.github.io/Marpa-web-site/">
          Kegler's website, accessed 25 April 2018</a>.
        <a href="https://github.com/jeffreykegler/Marpa--R2">
          Github repo, accessed 25 April 2018.</a>
        <a href="https://metacpan.org/pod/Marpa::R2">
          MetaCPAN, accessed 30 April 2018.</a>.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
        Matthew Might, David Darais and Daniel Spiewak. "Functional Pearl: Parsing with Derivatives." International Conference on Functional Programming 2011 (ICFP 2011). Tokyo, Japan. September, 2011. pages 189--195.
        <a href="http://matt.might.net/papers/might2011derivatives.pdf">
          PDF accessed 9 Jun 2018</a>.
        <a href="http://matt.might.net/papers/might2011derivatives-icfp-talk.pdf">
          Slides accessed 9 June 2018</a>.
        <a href="http://matt.might.net/media/mattmight-icfp2011-derivatives.mp4">
          Video accessed 9 June 2018</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3"><b>3.</b>
        Keshav Pingali and Gianfranco Bilardi, UTCS tech report TR-2012.
        2012.
        <a href="https://apps.cs.utexas.edu/tech_reports/reports/tr/TR-2102.pdf">
          PDF accessed 9 Junk 2018</a>.
        <a href="https://www.youtube.com/watch?v=eeZ3URxd8Wc">
          Video accessed 9 June 2018</a>.
        <br><br>
        Less accessible,
        but with more details about GFGs and GFG-based
        Earley parsing is
        Keshav Pingali and Gianfranco Bilardi,
        "A graphical model for context-free grammar parsing."
        Compiler Construction - 24th International Conference, CC 2015.
        Lecture Notes in Computer Science,
        Vol. 9031, pp. 3-27, Springer Verlag, 2015.
        <a href="https://www.researchgate.net/publication/286479583_A_Graphical_Model_for_Context-Free_Grammar_Parsing">
          PDF accessed 9 June 2018</a>.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4"><b>4.</b>
        Pingali and Bilardi 2012, section 3.1.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5"><b>5.</b>
        Pingali and Bilardi 2015, p. 11.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6"><b>6.</b>
        Pingali and Bilardi 2012, Sections 4-7.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7"><b>7.</b>
        I have based an
        liberal HTML pretty-printer on that parser,
        one which I use quite frequently.
        I used it, for example, when writing this blog post.
        To find out more about Ruby Slippers parsing see the Marpa FAQ,
        <a href="http://savage.net.au/Perl-modules/html/marpa.faq/faq.html#q122">
          questions 122</a>
        and
        <a href="http://savage.net.au/Perl-modules/html/marpa.faq/faq.html#q123">
          123</a>;
        my
        <a href="file:///mnt2/new/projects/Ocean-of-Awareness-blog/metapages/annotated.html#PARSE_HTML">
          blog series on parsing html</a>; and
        my blog post
        <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2011/11/marpa-and-the-ruby-slippers.html">
          "Marpa and the Ruby Slippers"</a>.
 <a href="#footnote-7-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 03:06 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/pingali.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 04 Jun 2018</h3>
<br />
<center><a name="knuth_1965"> <h2>Why is parsing considered solved?</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <p>It is often said that parsing is a "solved problem".
      Given the level of frustration with the state of the art,
      the underuse of the very powerful technique of
      Language-Oriented Programming due to problematic tools<a id="footnote-1-ref" href="#footnote-1">[1]</a>,
      and the vast superiority of human parsing ability
      over computers,
      this requires explanation.
    </p>
    <p>
      On what grounds would someone say that parsing is "solved"?
      To understand this,
      we need to look at the history of Parsing Theory.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
      In fact, we'll have to start decades before computer Parsing Theory
      exists,
      with a now nearly-extinct school of linguistics,
      and its desire to put the field on strictly
      scientific basis.
    </p>
    <h2>1929: Bloomfield redefines "language"</h2>
    <p>In 1929 Leonard Bloomfield,
      as part of his effort to create a linguistics that
      would be taken seriously as a science,
      published his "Postulates".<a id="footnote-3-ref" href="#footnote-3">[3]</a>
      The "Postulates" include his definition of language:
    </p><blockquote>
      The totality of utterances that can be made in a speech
      community is the
      <b>language</b>
      of that speech-community.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </blockquote><p>
      There is no reference in this definition to the usual view,
      that the utterances of a language "mean" something.
      This omission is not accidental:
    </p><blockquote>
      The statement of meanings is therefore the weak point in
      language-study, and will remain so until human knowledge
      advances very far beyond its present state. In practice, we define the
      meaning of a linguistic form, wherever we can, in terms of some
      other science.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </blockquote><p>
      Bloomfield is passing the buck,
      because the behaviorist science of his time rejects
      any claims about mental states as
      unverifiable statements -- essentially,
      as claims to be able to read minds.
      "Hard" sciences like physics, chemistry and even
      biology avoid dealing with unverifiable mental states.
      Bloomfield and the behaviorists want to make the methods of linguistics
      as close to hard science as possible.
    </p>
    <p>
      Draconian as Bloomfield's exclusion of meaning is,
      it is a big success.
      Known as structural linguistics,
      Bloomfield's approach dominates lingustics for
      the next couple of decades.
    </p>
    <h2>1955: Noam Chomsky graduates</h2>
    <p>
      Noam Chomsky earns his PhD at the University of Pennsylvania.
      His teacher, Zelig Harris, is a prominent Bloomfieldian,
      and Chomsky's early work is thought to be in the Bloomfield school.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
      Chomsky becomes a professor at MIT.
      MIT does not have a linguistics department,
      and Chomsky is free to teach his own approach to the subject.
    </p>
    <h2>The term "language" as of 1956</h2>
    <p>Chomsky publishes his "Three models" paper,
      one of the most important papers of all time.
      His definition of language uses the terminology
      of set theory:
    </p><blockquote>
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of symbols.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </blockquote>
    <p>
      This definition is pure Bloomfield in substance,
      but signs of departure from the behaviorist orthodoxy are
      apparent in "Three Models" --
      Chomsky is quite willing to talk about what sentences mean,
      when it serves his purposes.
      For a utterance with multiple meanings,
      Chomsky's new model produces multiple syntactic derivations.
      Each of these syntactic derivations
      "looks" like the natural representation
      of one of the meanings.
      Chomsky points out that the insight into semantics
      that his new model provides is a very
      desirable property to have.<a id="footnote-8-ref" href="#footnote-8">[8]</a>
    </p>
    <h2>1959: Chomsky reviews Skinner</h2>
    <p>In 1959, Chomsky reviews a book by B.F. Skinner's on linguistics.<a id="footnote-9-ref" href="#footnote-9">[9]</a>
      Skinner is the most prominent behaviorist of the time.
    </p>
    <p>
      Chomsky's review removes all doubt about where he stands
      on behaviorism
      or on the relevance of linguistics to the study of meaning.<a id="footnote-10-ref" href="#footnote-10">[10]</a>
      His review galvanizes the opposition to behaviorism, and
      Chomsky establishes himself as behavorism's most
      prominent and effective critic.
    </p>
    <p>
      In later years,
      Chomsky will make it clear that he had had no intention
      of avoiding semantics:
    </p><blockquote>
      [...] it would be absurd to develop
      a general syntactic theory
      without assigning an absolutely
      crucial role to semantic considerations,
      since obviously the necessity to support
      semantic interpretation is one of the primary
      requirements
      that the structures
      generated by the syntactic component of a grammar
      must meet.<a id="footnote-11-ref" href="#footnote-11">[11]</a>
    </blockquote>
    <h2>1961: Oettinger discovers pushdown automata</h2>
    <p>
      While the stack itself goes back to Turing<a id="footnote-12-ref" href="#footnote-12">[12]</a>,
      its significance for parsing becomes an object
      of interest in itself with
      Samuelson and Bauer's 1959 paper<a id="footnote-13-ref" href="#footnote-13">[13]</a>.
      Mathematical study of stacks as models of computing begins with Anthony Oettinger's 1961 paper.<a id="footnote-14-ref" href="#footnote-14">[14]</a></p>
    <p>Oettinger 1961 is full of evidence that stacks
      (which he calls "pushdown stores") are still very new.
      For example,
      Oettinger does not use the terms "push" or "pop",
      but instead describes operations on his pushdown stores using
      a set of vector operations which will later form the basis
      of the APL language.
    </p>
    <p>Oettinger defines 4 languages.
      Oettinger's definitions all follow the behavorist model --
      they are sets of strings.<a id="footnote-15-ref" href="#footnote-15">[15]</a>
      Oettinger's pushdown stores
      will eventually be called
      deterministic pushdown automata (DPDA's) and
      become the basis of a model of language
      and the subject of a substantial literature,
      all of which will use the behaviorist definition
      of "language".
    </p>
    <p>
      Oettinger hopes that DPDA's
      will be an adequate basis for
      the study of both computer and
      natural language translation.
      (Oettinger's own field is Russian translation.)
      DPDA's soon prove totally inadequate
      for natural languages.
    </p>
    <p>
      But for dealing with computing languages,
      DPDA's will have a much longer life.
      As of 1961, all algorithms with acceptable speed are using
      stacks with various modifications.
    </p>
    <blockquote>
      The development of a theory of pushdown algorithms should
      hopefully lead to systematic techniques for generating
      algorithms satisfying given requirements to replace
      the ad hoc invention of each new algorithm.<a id="footnote-16-ref" href="#footnote-16">[16]</a>
    </blockquote>
    <p>
      The search for a comprehensive theory of
      stack-based parsing
      quickly becomes identified
      with the search for a theoretical basis for practical parsing.
    </p>
    <h2>1965: Knuth discovers LR(k)</h2>
    <p>Donald Knuth
      reports his new results on stack-based parsing.
      In a pivotal paper<a id="footnote-17-ref" href="#footnote-17">[17]</a>,
      Knuth sets out a theory that
      encompasses all the "tricks"<a id="footnote-18-ref" href="#footnote-18">[18]</a>
      used for efficient parsing up to that time.
      With this Oettinger's hope for a theory
      to replace "ad hoc invention" is fulfilled.
      In an exhilarating (and exhausting) 39-page
      demonstration of mathematical virtuousity,
      Knuth shows that stack-based parsing is
      equivalent to a new and unexpected class of grammars.
      Knuth calls these LR(k), and provides a parsing algorithm for them.
    </p>
    <p>
      Knuth's new algorithm might be expected to be "the one to rule
      them all".
      Unfortunately, while deterministic and linear,
      it is not practical -- it requires huge tables well beyond
      the memory capabilities of the time.
    </p>
    <p>
      The impracticality of his LR(k) algorithm
      does not suggest to Knuth that the stack-based model
      is inappropriate as a model of practical parsing.
      Instead it suggests to him, and to the field,
      that the boundary of practical parsing lies in a subclass of the
      LR(k) grammars.
    </p>
    <p>
      To be sure,
      Knuth, in his program for further research<a id="footnote-19-ref" href="#footnote-19">[19]</a>,
      does suggests investigation of parsers for superclasses
      of LR(k).
      He even describes a new superclass of his own:
      LR(k,t), which is LR(k) with more aggressive lookahead.
      But he is clearly unenthusiastic about LR(k,t).<a id="footnote-20-ref" href="#footnote-20">[20]</a>
      It is reasonable to suppose
      that Knuth is even more negative about
      the more general approaches that
      he does not bother to mention.<a id="footnote-21-ref" href="#footnote-21">[21]</a>
    </p>
    <p>
      In any case, those reading Knuth's LR(k) paper focused almost
      exclusively on his suggestions for research within the stack-based
      model.
      These included grammar rewrites;
      streamlining of the LR(k) tables;
      and research into LR(k) subclasses.
      It is LR(k) subclassing that will receive the most attention.<a id="footnote-22-ref" href="#footnote-22">[22]</a>
    </p>
    <p>
      The idea that the solution to the parsing problem must be
      stack-based is not without foundation.
      In 1965, the limits of computer technology are severe.
      For practitioners,
      any parsing technique that required much more
      than a reasonably-sized state
      machine and a stack,
      is not likely to happen.
      After all,
      only four years earlier,
      stacks were bleeding edge.
    </p>
    <p>The practitioners of 1965
      are inclined to believe that,
      like it or not,
      they are stuck with stack-based parsing.
      But why do the theoreticians feel compelled to follow them?
      The answer is that theoreticians talk themselves into
      it, using a misleading equivalence based
      on the behaviorist definition of language.
    </p>
    <h2>"Language" as of 1965</h2>
    <p>
      Knuth defines language as follows:
    </p>
    <blockquote>
      The language defined by G is<br>
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      { &alpha; | S => &alpha; and &alpha; is a string over T }<br>
      namely, the set of all terminal strings derivable from S by using
      the productions of G as substitution rules.<a id="footnote-23-ref" href="#footnote-23">[23]</a>
    </blockquote><p>
      Here G is a grammar whose start symbol is S and whose set
      of terminals is T.
      This is the behavorist definition of language
      translated into set-theoretic terms.
    </p>
    <p>Knuth proves, to the satisfaction of the profession,
      the "equivalence" of LR(k) and DPDA's.
      LR(k) is a class of grammars and the DPDA model is of
      languages -- sets of strings.
      At first glance, this is an "apples and oranges" comparison --
      how do you prove the equivalence of a class of languages
      and a class of grammars?
    </p>
    <p>
      Knuth does this by reducing the class of DPDA languages and the class
      of grammars to their lowest common denominator, which is the language.
      And, of course, the "language" in the usage of Parsing Theory
      is a set of strings, without consideration of their syntax.
    </p>
    <p>
      Every grammar, when stripped of its syntax, defines a language.
      So Knuth compares the language which results from stripping down
      the LR(k) grammars,
      to the language of DPDA's.
      After some very impressive mathematics,
      Knuth is able to show that the two languages are equivalent.
    </p>
    <p>
      In theoretical mathematics, of course,
      you can define "equivalent" however you like.
      But if the purpose is to suggest limits in practice,
      you have to be much more careful.
      And in fact, as Knuth's paper shows,
      if you equate languages and grammars,
      you get into a very serious degree of magical thinking.
      Using the Knuth algorithm,
    </p>
    <ul>
      <li>parsing LR(k) grammars for arbitrary k is hopelessly impractical;
      </li>
      <li>parsing LR(1) grammars is impractical, but close to the boundary<a id="footnote-24-ref" href="#footnote-24">[24]</a>;
        and
      </li>
      <li>parsing LR(0) grammars is very practical.
      </li>
    </ul>
    <p>A problem for the relevance
      of Knuth's proof of equivalence is that,
      if you just look at sets of strings
      without regard to syntax,
      LR(1) and LR(k) are equivalent.
      That means that from the sets-of-strings point of view,
      hopelessly impractical and
      borderline impractical are the same thing.
    </p>
    <p>
      Worse, both LR(1) and LR(k) are equivalent to LR(0)
      for most applications.
      If you add
      an explicit end marker to an LR(1) language,
      which in most applications is easy to do<a id="footnote-25-ref" href="#footnote-25">[25]</a>,
      your LR(1) language becomes LR(0).
      Therefore, for most applications,
    </p>
    <center>
      LR(k) = LR(1) = LR(0)
    </center>
    <p>
      This means that, in the world of sets-of-strings,
      extremely impractical and very practical are usually the same thing.
    </p>
    <p>
      Clearly the world of sets of strings
      is a magical one,
      in which we can easily transport ourselves across the
      boundary between practical and impractical.
      We can take visions of a magical world back into the world of practice,
      but we cannot assume they will be helpful.
      In that light,
      it is no surprise that
      Joop Leo will show how to extend practical
      parsing well beyond LR(k).<a id="footnote-26-ref" href="#footnote-26">[26]</a>
    </p>
    <h2>Comments, etc.</h2>
    <p>
      I encourage
      those who want to know more about the story of Parsing Theory
      to look at my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3">
        Parsing: a timeline 3.0</a>.
      In particular,
      "Timeline 3.0" tells the story of the search for a good
      LR(k) subclass,
      and what happened afterwards.
    </p>
    <p>
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
        The well-known
        <a href="https://en.wikipedia.org/wiki/Design_Patterns">
          <cite>Design Patterns</cite>
          book</a>
        (aka "the Gang of 4 book")
        has a section on this.
        The Gang of 4
        call Language-Oriented Programming
        their "Interpreter pattern".
        That section amply illustrates the main obstacle to use
        of the pattern -- lack of adequate parsing tools.
        I talk more about this in my two blog posts on
        the Interpreter pattern:
        <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/bnf_to_ast.html">
          BNF to AST</a>
        and
        <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/interpreter.html">
          The Interpreter Design Pattern</a>.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
        This post takes the form of a timeline, and
        is intended to be incorporated in my
        <a href="https://jeffreykegler.github.io/personal/timeline_v3">
          Parsing: a timeline</a>.
        The earlier entires in this post borrow heavily from
        <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html">
          a previous blog post</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
        Bloomfield, Leonard,
        "A set of Postulates
        for the Science of Language",
        <cite>Language</cite>, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
        Bloomfield 1926, definition 4 on p. 154.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
        Bloomfield, Leonard.
        <cite>Language</cite>.
        Holt, Rinehart and Winston, 1933, p. 140.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
        Harris, Randy Allen,
        <cite>The Linguistics Wars</cite>,
        Oxford University Press, 1993,
        pp 31-34, p. 37.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
        The quote is on p. 114 of
        Chomsky, Noam.
        "Three models for the description of language."
        <cite>IRE Transactions on information theory</cite>,
        vol. 2, issue 3, September 1956, pp. 113-124.
        In case there is any doubt Chomsky's "strings"
        are Bloomfield's utterances,
        Chomsky also calls his strings,
        "utterances".
        For example in Chomsky, Noam,
        <cite>Syntactic Structures</cite>,
        2nd ed.,
        Mouton de Gruyter, 2002, on p. 15:
        "Any grammar of a language will project the finite and somewhat accidental
        corpus of observed utterances to a set (presumably infinite)
        of grammatical utterances."
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
        Chomsky 1956, p. 118, p. 123.
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9">9.
        Chomsky, Noam.
        A Review of B. F. Skinners Verbal Behavior.
        <cite>Language</cite>,
        Volume 35, No. 1, 1959, 26-58.
        <a href="https://chomsky.info/1967____/">
          https://chomsky.info/1967____/</a>
        accessed on 3 June 2018.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10">10.
        See in particular, Section IX of Chomsky 1959.
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11">11.
        Chomsky, Noam.
        <cite>Topics in the Theory of Generative Grammar</cite>.
        De Gruyter, 1978, p. 20.
        (The quote occurs in footnote 7 starting on p. 19.)
 <a href="#footnote-11-ref">&#8617;</a></p>
<p id="footnote-12">12.
        Carpenter, Brian E., and Robert W. Doran.
        "The other Turing machine."
        <cite>The Computer Journal</cite>, vol. 20, issue 3, 1 January 1977, pp. 269-279.
 <a href="#footnote-12-ref">&#8617;</a></p>
<p id="footnote-13">13.
        Samelson, Klaus, and Friedrich L. Bauer. "Sequentielle formelbersetzung." it-Information Technology 1.1-4 (1959): 176-182.
 <a href="#footnote-13-ref">&#8617;</a></p>
<p id="footnote-14">14.
        Oettinger, Anthony.
        "Automatic Syntactic Analysis and the Pushdown Store"
        <cite>Proceedings of Symposia in Applied Mathematics</cite>,
        Volume 12,
        American Mathematical Society, 1961.
 <a href="#footnote-14-ref">&#8617;</a></p>
<p id="footnote-15">15.
        Oettinger 1961, p. 106.
 <a href="#footnote-15-ref">&#8617;</a></p>
<p id="footnote-16">16.
        Oettinger 1961, p. 127.
 <a href="#footnote-16-ref">&#8617;</a></p>
<p id="footnote-17">17.
        Knuth, Donald E.
        "On the translation of languages from left to right."
        <cite>Information and Control</cite>, Volume 8, Issue 6, December 1965, pp. 607-639.
        <a href="https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d">
          https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d</a>, accessed 24 April 2018.
 <a href="#footnote-17-ref">&#8617;</a></p>
<p id="footnote-18">18.
        Knuth 1965, p. 607, in the abstract.
 <a href="#footnote-18-ref">&#8617;</a></p>
<p id="footnote-19">19.
        Knuth 1961, pp. 637-639.
 <a href="#footnote-19-ref">&#8617;</a></p>
<p id="footnote-20">20.
        "Finally, we might mention another generalization of LR(k)"
        (Knuth 1965, p. 638); and
        "One might choose to call this left-to-right translation,
        although we had to back up a finite amount."
        (p. 639).
 <a href="#footnote-20-ref">&#8617;</a></p>
<p id="footnote-21">21.
        Knuth's skepticism of more general Chomskyan approaches
        is suggested by his own plans for his (not yet released) Chapter
        12 of the
        <cite>Art of Computer Programming</cite>,
        in which he planned to use pre-Chomskyan bottom-up methods. (See
        Knuth, Donald E., "The genesis of attribute grammars",
        <cite>Attribute Grammars and Their Applications</cite>,
        Springer, September 1990, p. 3.)
 <a href="#footnote-21-ref">&#8617;</a></p>
<p id="footnote-22">22.
        The story of the research followup to Knuth's LR(k) paper is told
        in my
        <a href="https://jeffreykegler.github.io/personal/timeline_v3">
          Parsing: a timeline 3.0</a>.
 <a href="#footnote-22-ref">&#8617;</a></p>
<p id="footnote-23">23.
        Knuth 1965, p. 608.
 <a href="#footnote-23-ref">&#8617;</a></p>
<p id="footnote-24">24.
          Given the capacity of computer memories in 1965,
          LR(1) was clearly impractical.
          With the huge computer memories of 2018,
          that could be reconsidered, but LR(1) is still restrictive
          and has poor error-handling,
          and few have looked at the possibility.
 <a href="#footnote-24-ref">&#8617;</a></p>
<p id="footnote-25">25.
        Some parsing applications, such as those which receive their input "on-line",
        can not determine the size of their input in advance.
        For these applications adding an end marker to their input is
        inconvenient or impossible.
 <a href="#footnote-25-ref">&#8617;</a></p>
<p id="footnote-26">26.
        Joop M. I. M.
        "A general context-free parsing algorithm running in linear time on every LR (k) grammar without using lookahead."
        <cite>Theoretical computer science</cite>, Volume 82, Issue 1, 22 May 1991, pp. 165-176.
        <a href="https://www.sciencedirect.com/science/article/pii/030439759190180A">
          https://www.sciencedirect.com/science/article/pii/030439759190180A</a>, accessed 24 April 2018.
 <a href="#footnote-26-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 06:58 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
