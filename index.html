<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Sat, 01 Nov 2014</h3>
<br />
<center><a name="delimiter"> <h2>Reporting mismatched delimiters</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>In many contexts, programs need to treat a text
      as divided into into pieces.
      One very direct way to indicate the pieces of a text
      is to use pairs of delimiters.
      One delimiter of the pair marks the start
      and the other marks the end.
    </p>
    <p>
      Delimiters can take many forms.
      Quote marks, parentheses, curly braces and square brackets,
      and XML and HTML tags
      are all delimiters.
    </p>
    <p>
      Mismatching delimiters is easy to do and
      traditional parsers are often poor at reporting these errors:
      hopeless after the first mismatch,
      and for that matter none too precise about the first one.
      This post outlines a scaleable method for the accurate
      reporting of mismatched delimiters.
      I will illustrate the method with an example of a simple
      but useable tool --
      a utility which reports mismatched brackets.
    </p>
    <h3>The example script</h3>
    <p>The example script,
      <tt>bracket.pl</tt>,
      is a utility that reports mismatched brackets in the set:
    </p><blockquote><pre>
      () {} []
      </pre></blockquote><p>
      They are expected to nest in the usual way.
      Other text is treated as filler, so that
      <tt>bracket.pl</tt>
      will not be smart about things
      like strings or comments.
      This does have the advantage of making
      <tt>bracket.pl</tt>
      mostly language-agnostic.
    </p>
    <p>
      Because it's intended primarily to be read
      as an illustration of the technique,
      <tt>bracket.pl</tt>
      is relatively simple.
      It's simple enough that a recursive descent parser, for
      example, could emulate it for this specific grammar.
      I hope the reader who goes on to look at the details
      will see that this technique scales to more
      complex situations,
      in a way in which a solution based on a traditional parser
      will not.
    </p>
    <h3>Error reports</h3>
    <p>The description of how the method works will make more
      sense after we've looked at some examples of the diagnostics
      <tt>bracket.pl</tt>
      produces.
      To be truly useful,
      <tt>bracket.pl</tt>
      must report mismatches that span
      many lines,
      and it can do this.
      But single-line examples are easier to follow.
      All the examples in this post will be contained in a one line.
      Consider the string '((([))':
      <tt>bracket.pl</tt>'s diagnostics are:
    </p><blockquote><pre>
* Line 1, column 1: Opening '(' never closed, problem detected at end of string
((([))
^
====================
* Line 1, column 4: Missing close ], problem detected at line 1, column 5
((([))
   ^^
</pre></blockquote>
    <p>
      In the next example
      <tt>bracket.pl</tt>
      realizes that it
      cannot accept the ')' at column 16, without first closing the set of curly braces started at column 5.
      It identifies the problem, along with both of the locations involved.
    </p>
    <blockquote><pre>
* Line 1, column 5: Missing close }, problem detected at line 1, column 16
[({({x[]x{}x()x)})]
    ^          ^
</pre></blockquote>
    <p>
      So far, so good.
      But an important advantage of
      <tt>bracket.pl</tt>
      has yet to be seen.
      The error messages of
      most compilers, after they report a mismatched delimiter,
      are so unreliable that they are in practice useless.
      <tt>bracket.pl</tt>
      attempts to repair the damage fixing the unmatched brackets,
      and it usually succeeds well enough to accurately report
      the subsequent issues.
      Consider the text
      <tt>'({]-[(}-[{)'</tt>.
      The output of
      <tt>bracket.pl</tt>
      is
    </p><blockquote><pre>
* Line 1, column 1: Missing close ), problem detected at line 1, column 3
({]-[(}-[{)
^ ^
====================
* Line 1, column 2: Missing close }, problem detected at line 1, column 3
({]-[(}-[{)
 ^^
====================
* Line 1, column 3: Missing open [
({]-[(}-[{)
  ^
====================
* Line 1, column 5: Missing close ], problem detected at line 1, column 7
({]-[(}-[{)
    ^ ^
====================
* Line 1, column 6: Missing close ), problem detected at line 1, column 7
({]-[(}-[{)
     ^^
====================
* Line 1, column 7: Missing open {
({]-[(}-[{)
      ^
====================
* Line 1, column 9: Missing close ], problem detected at line 1, column 11
({]-[(}-[{)
        ^ ^
====================
* Line 1, column 10: Missing close }, problem detected at line 1, column 11
({]-[(}-[{)
         ^^
====================
* Line 1, column 11: Missing open (
({]-[(}-[{)
          ^
</pre></blockquote>
    <p>Each time,
      <tt>bracket.pl</tt>
      corrects itself,
      and accurately reports the next set of problems.
    </p><h3>A difficult error report</h3>
    <p>
      To be 100% accurate,
      <tt>bracket.pl</tt>
      would have to guess the programmer's intent.
      This is, of course, not possible.
      Let's look at a text where
      <tt>bracket.pl</tt>'s guesses are not so good:
      <tt>{{]}</tt>.
      Here we will assume the closing square bracket is a typo for a closing parenthesis.
      Here's the result:
    </p><blockquote><pre>
* Line 1, column 1: Missing close }, problem detected at line 1, column 3
{{]}
^ ^
====================
* Line 1, column 2: Missing close }, problem detected at line 1, column 3
{{]}
 ^^
====================
* Line 1, column 3: Missing open [
{{]}
  ^
====================
* Line 1, column 4: Missing open {
{{]}
   ^
</pre></blockquote><p>
      Instead of one error,
      <tt>bracket.pl</tt>
      finds four.
    </p><p>
      But even in this case, the method is fairly good, especially when
      compared with current practice.
      The problem is at line 1, column 3,
      and the first three messages all identify this as one of their
      potential problem locations.
      It is reasonable to believe that a programmer, especially once
      he becomes used to this kind of mismatch reporting,
      will quickly find the first mismatch and fix it.
      This may not be much better than the state of the art,
      but it is certainly no worse.
    </p>
    <h3>How it works</h3>
    <p>
      For full details of the workings of
      <tt>bracket.pl</tt>
      there is
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">the code</a>,
      which is heavily commented.
      This section provides a conceptual overview.
    </p><p>
      <tt>bracket.pl</tt>
      uses two features of Marpa:
      left-eideticism and the Ruby Slippers.
      By left-eidetic, I mean that Marpa knows everything there is to know
      about the parse at, and to left of, the current position.
      As a consequence,
      Marpa
      also knows exactly which of its input symbols
      can lead to a successful parse,
      and is able to stop as soon as it knows that the parse cannot succeed.
    </p>
    <p>
      In the Ruby Slippers technique, we arrange for parsing to stop
      whenever we encounter an input which
      would cause parsing to fail.
      The application then
      asks Marpa, "OK.  What input would allow the
      parse to continue?"
      The application takes Marpa's answer to this
      question, and uses it to concoct
      an input that Marpa will accept.
    </p>
    <p>
      In this case,
      <tt>bracket.pl</tt>
      creates a virtual token which fixes the mismatch
      of brackets.
      Whatever the missing bracket may be,
      <tt>bracket.pl</tt>
      invents a bracket of that kind,
      and adds it to the virtual input.
      This done,
      parsing and error detection
      can proceed as if there was no problem.
      Of course,
      the error which made the Ruby Slippers token necessary
      is recorded, and those records are the source of the
      error reports we saw above.
    </p>
    <p>
      To make its error messages as informative as possible
      in the case of missing closing brackets,
      <tt>bracket.pl</tt>
      needs to report the exact location of
      the opening bracket.
      Left-eideticism again comes in handy here.
      Once the virtual closing bracket is supplied to Marpa,
      <tt>bracket.pl</tt>
      asks, "That bracketed text that I just closed -- where did it begin?"
      The Marpa parser tracks the start location
      of all symbol and rule instances,
      so it is able to provide the application
      with the exact location of
      the starting bracket.
    </p><p>
      Finally, but importantly,
      when
      <tt>bracket.pl</tt>
      encounters a problem at a point where there are unclosed opening
      brackets, it has two choices.
      It can be optimistic or it can be pessimistic.
      "Optimistic" means it can hope that something later in the input will close
      the opening bracket.
      "Pessimistic" means it can decide that "all bets are off" and use
      Ruby Slippers tokens to close all the currently active open brackets.
    </p>
    <p>
      <tt>bracket.pl</tt>
      uses the pessimistic strategy.
      While the optimistic strategy sounds better, in practice
      the pessimistic one seems to provide better diagnostics.
      The pessimistic strategy does report some fixable problems
      as errors.
      But the optmistic one can introduce spurious fixes.
      These hide the real errors,
      and missing errors is worse than overreporting them.
      Even when the pessimistic strategy overreports,
      its first error messages will always accurately identify a
      a problem location.
    </p>
    <h3>Implications</h3>
    <p>
      While
      <tt>bracket.pl</tt>
      is already useable,
      I think of it as a prototype.
      And beyond that,
      the problem of matching delimiters
      is in fact very general, and I believe these techniques have very wide application.
    </p>
    <h3>For more</h3>
    <p>
      The example script of this post is
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">a Github gist</a>.
      For more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 11:11 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/delimiter.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sun, 07 Sep 2014</h3>
<br />
<center><a name="chron"> <h2>Parsing: a timeline</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>[ Revised 22 Oct 2014 ]
    </p>
    <p><b>1960</b>:
      The ALGOL 60 spec comes out.
      It specifies, for the first time, a block structured
      language.
      The ALGOL committee is well aware
      that
      nobody knows how to parse such a language.
      But they believe that,
      if they specify a block-structured
      language, a parser for it will be invented.
      Risky as this approach is, it pays off ...
    </p>
    <p><b>1961</b>: Ned Irons publishes his ALGOL parser.
      In fact, the Irons parser
      is the first parser of any kind to be described
      in print.
      Ned's algorithm is a left parser --
      a form of recursive descent.
      Unlike modern
      recursive descent,
      the Irons algorithm
      is general and syntax-driven.
      "General" means it can parse anything written in BNF.
      "Syntax-driven" (aka declarative) means that parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    </p>
    <p><b>1961</b>:
      Almost simultaneously, hand-coded approaches to left parsing
      appear.
      These we would now recognize as recursive descent.
      Over the following years, hand-coding approaches
      will become more popular for left parsers
      than syntax-driven algorithms.
      Three factors are at work:
      <ul>
      <li>
      In 1960's, memory and CPU are both extremely limited.
      Hand-coding pays off, even when the gains are small.
      <li>
      Pure left parsing is a very weak parsing technique.
      Hand-coding is often necessary
      to overcome its limits.
      This is
      as true today as it is in 1961.
      <li>
      Left parsing works well in combination with hand-coding --
      they are a very good fit.
      </ul>
    </p>
    <p><b>1965</b>:
    Don Knuth invents LR parsing.
      Knuth is primarily interested
      in the mathematics.
      Knuth describes a parsing algorithm,
      but it is not thought practical.
    </p>
    <p><b>1968</b>: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's core idea is to
      track everything about the parse in tables.
      Earley's algorithm is enticing, but it has three major issues:
      <ul>
      <li>First, there is a bug in the handling of zero-length rules.
      <li>Second, it is quadratic for right recursions.
      <li>Third, the bookkeeping required to set up the tables is,
      by the standards of 1968 hardware, daunting.
      </ul>
    <p><b>1969</b>:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
    </p>
    <p><b>1972</b>:
      Aho and Ullmann describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    <p><b>1975</b>:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    </p>
    <p><b>1977</b>:
      The first "Dragon book" comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters "LALR".
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    </p>
    <p><b>1979</b>: Bell Laboratories releases Version 7 UNIX.
	V7 includes what is, by far,
		the most comprehensive, useable and easily available
		compiler writing toolkit yet developed.
	 Central to the toolkit is
	 yacc, an LALR based parser generator.
	  With a bit of hackery,
	  yacc parses its own input language,
	  as well as the language of V7's main compiler,
	  the portable C compiler.
	  After two decades of research,
	  it seems that the parsing problem is solved.
    </p>
    <p><b>1987</b>:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    </p>
    <p><b>1991</b>:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of Earley algorithm.
      But Earley parsing is almost forgotten.
      It will be 20 years before anyone writes a practical
      implementation of Leo's algorithm.
    </p>
    <p><b>1990's</b>:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is "fast but stupid".
    </p><b>2000</b>:
    Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    </p>
    <p><b>2002</b>:
      Aycock&Horspool publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    </p>
    <p><b>2006</b>:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    </p>
    <p><b>2000 to today</b>:
    With the retreat from LALR comes a collapse in the
      prestige of parsing theory.
      After a half century,
      we seem to be back
      where we started.
      If you took Ned Iron's original 1961 algorithm,
      changed the names and dates,
      and translated the code from the mix of assembler and
      ALGOL into Haskell,
      you would easily republish it today,
      and bill it as 
      as revolutionary and new.
    </p>
    <p>
    <h3>Marpa</h3>
      Over the years,
      I had come back to Earley's algorithm again and again.
      Around 2010, I realized
      that the original, long-abandoned vision --
      an efficient, practical, general and syntax-driven parser --
      was now, in fact, quite possible.
      The necessary pieces had fallen into place.
    </p>
    <p>
      Aycock&Hospool has solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and probably no longer relevant in any case --
      caches misses are now the bottleneck.
    </p>
    <p>But while the original issues with Earley's disappeared,
      a new issue emerged.
      With a parsing algorithm as powerful as Earley's behind it,
      a syntax-driven approach can do much more than it can with
      a left parser.
      But with the experience with LALR in their collective consciousness,
      few modern programmers are prepared
      to trust a purely declarative parser.
      As Lincoln said, "Once a cat's been burned,
      he won't even sit on a cold stove."
    </p>
    <p>
      To be accepted, Marpa needed to allow
      procedure parsing,
      not just declarative parsing.
      So Marpa allows the user to specify events --
      occurrences of symbols and rules --
      at which declarative parsing pauses.
      While paused,
      the application can call procedural logic
      and single-step forward token by token.
      The procedural logic can hand control back
      over to syntax-driven parsing at any point it likes.
      The Earley tables can provide the procedural logic with
      full knowledge of the state of the
      parse so far:
      all rules recognized
      in all possible parses so far,
      and all symbols expected.
      Earley's algorithm is now a even better companion
      for hand-written procedural logic than recursive descent.
    </p>
    <h3>For more</h3>
    <p>
      For more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
      official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 16:50 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/09/chron.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 01 Sep 2014</h3>
<br />
<center><a name="website"> <h2>Marpa has a new web page</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Marpa has
      <a href="http://savage.net.au/Marpa.html">a
      new official public website</a>,
      which Ron Savage has generously agreed to manage.
      For those who have not heard of it,
      Marpa is a parsing algorithm.
      It is new, but very much based
      on earlier work by Jay Earley, Joop Leo, John Aycock and R. Nigel Horspool.
      Marpa is intended to replace, and to go well beyond,
      recursive descent and the yacc family of parsers.
    </p><ul>
      <li>
        Marpa is fast. It parses in linear time:
        <ul>
          <li>all the grammar classes that recursive descent parses;</li>
          <li>the grammar class that the yacc family parses;</li>
          <li>in fact, all unambiguous grammars, as long as they are free of unmarked middle recursions;
	  and</li>
	  <li>all
	  ambiguous grammars that are unions of a finite set of any of the above grammars.</li>
        </ul>
      </li>
      <li>
        Marpa is powerful. Marpa will parse anything that can be
	written in BNF.
	This includes any mixture of left, right and middle recursions.
      </li>
      <li>Marpa is convenient.
      Unlike recursive descent, you do not have to write a parser --
      Marpa generates one from BNF.
      Unlike PEG or yacc, parser generation is unrestricted and exact.
      Marpa converts any grammar which can be written as BNF
      into a parser which recognizes everything
      in the language described by that BNF, and which rejects everything that is
      not in that language.
      The programmer is not forced to make arbitrary choices while parsing.
      If a rule has several alternatives,
      all of the alternatives are considered for as long as they might yield a valid parse.
      </li>
      <li>
        Marpa is flexible. Like recursive descent, Marpa allows you to stop and
        do your own custom processing. Unlike recursive descent, Marpa makes available
        to you detailed information about the parse so far --
        which rules and symbols have been recognized, with their locations,
        and which rules and symbols are expected next.
      </li>
      </ul>
    <h3>Comments</h3>
    <p>
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 20:17 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/09/website.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 18 Aug 2014</h3>
<br />
<center><a name="ambig"> <h2>Language design: Exploiting ambiguity</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Currently, in designing languages,
      we don't allow ambiguities --
      not even potential ones.
      We insist that it must not be
      even
      <b>possible</b>
      to write an ambiguous program.
      This is unnecessarily restrictive.
    </p>
    <p>
      This post is written in English, which is full of ambiguities.
      Natural languages are always ambiguous,
      because human beings find that that's best way for versatile,
      rapid, easy communication.
      Human beings arrange things so that every
      sentence is unambiguous in context.
      Mistakes happen, and ambiguous sentences occur,
      but in practice, the problem is manageable.
      In a conversation, for example,
      we would just ask for clarification.
    </p>
    <p>
      If we allow our computer languages to take their most natural forms,
      they will often have the
      <b>potential</b>
      for ambiguity.
      This is even less of a problem on a computer than it is in
      conversation -- a computer can always spot an actual ambiguity
      immediately.
      When
      <b>actual</b>
      ambiguities occur, we can deal with them
      in exactly the same way that we deal with
      any other syntax problem:
      The computer catches it and reports it,
      and we fix it.
    </p>
    <h3>An example</h3>
    <p>
      To illustrate,
      I'll use a DSL-writing DSL language.
      It'll be tiny -- just lexeme declarations and BNF rules.
      Newlines will
      <b>not</b>
      be significant.
      Statements can end with a semicolon, but that's optional.
      (The code for this post is in
      <a href="https://gist.github.com/jeffreykegler/ed64bf00983f7be666bc">
        a Github gist</a>.)
    </p>
    <p>Here is a toy calculator written in our tiny DSL-writing language:
    </p><blockquote>
      <pre>
  Number matches '\d+'
  E ::= T '*' F
  E ::= T
  T ::= F '+' Number
  T ::= Number
</pre>
    </blockquote>
    <h3>Trying an improvement</h3>
    <p>With a grammar this small, just about
      <b>anything</b>
      is readable.
      But let's assume we want to improve it, and that we decide
      that the lexeme declaration of
      <tt>Number</tt>
      really belongs
      after the rules which use it.
      (If our grammar was longer, this could make a real difference.)
      So we move the lexeme declaration to the end:
    </p><blockquote>
      <pre>
  E ::= T '*' F
  E ::= T
  T ::= F '+' Number
  T ::= Number
  Number matches '\d+'
</pre>
    </blockquote>
    <h3>But there's an issue</h3>
    <p>
      It turns out the grammar for our toy DSL-writer is ambiguous.
      When a lexeme declaration follows a BNF rule,
      there's no way to tell whether or not it is actually a
      lexeme declaration, or part of the BNF rule.
      Our parser catches that:
    </p><blockquote>
      <pre>
Parse of BNF/Scanless source is ambiguous
Length of symbol "Statement" at line 4, column 1 is ambiguous
  Choices start with: T ::= Number
  Choice 1, length=12, ends at line 4, column 12
  Choice 1: T ::= Number
  Choice 2, length=33, ends at line 5, column 20
  Choice 2: T ::= Number\nNumber matches '\\d
</pre></blockquote>
    <p>
      Here Marpa tells you why it thinks your script is ambiguous.
      Two different statements can start at line 4.
      Both of them are BNF rules, but one is longer than the other.
    </p>
    <h3>Just another syntax error</h3>
    <p>Instead of having to design a language where ambiguity was not
      even possible, we designed one where ambiguities can happen.
      This allows us to design a much more flexible language,
      like the ones we choose when we humans communicate with each other.
      The downside is that actual ambiguities will occur,
      but they can be reported, and fixed,
      just like any other syntax error.
    </p><p>In this case, we
      recall we allowed semi-colons to terminate a rule,
      and our fix is easy:
    </p>
    <p>
    </p><blockquote>
      <pre>
  E ::= T '*' F
  E ::= T
  T ::= F '+' Number
  T ::= Number ;
  Number matches '\d+'
</pre>
    </blockquote>
    <h3>To learn more</h3>
    <p>
      The code for this post is
      <a href="https://gist.github.com/jeffreykegler/ed64bf00983f7be666bc">
        a gist on Github</a>.
      It was written using
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2,
        which is available on CPAN</a>.
      A list of my Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
        here</a>.
      There are
      new tutorials by
      <a href="http://marpa-guide.github.io/chapter1.html">Peter Stuifzand</a>
      and
      <a href="http://longanswers.blogspot.de/2013/06/transforming-syntax.html">amon</a>.
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        The Ocean of Awareness blog</a>
      focuses on Marpa,
      and it has
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>.
      Marpa has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page that I maintain</a>
      and Ron Savage maintains
      <a href="http://savage.net.au/Perl-modules/html/marpa.papers/index.html">
        another</a>.
      For questions, support and discussion, there is
      <a href="http://groups.google.com/group/marpa-parser">
        a "marpa parser"
        Google Group</a>
      and an IRC channel:
      <tt>#marpa</tt>
      at
      <tt>irc.freenode.net</tt>.
    </p>
    <h3>Comments</h3>
    <p>
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 14:12 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/08/ambig.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 10 Mar 2014</h3>
<br />
<center><a name="kv"> <h2>Evolvable languages</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Ideally, if a syntax is useful and clear,
      and a programmer can easily read it at a glance,
      you should be able to add it to an existing language.
      In this post, I will describe
      a modest incremental change to the Perl syntax.
    </p>
    <p>
      It's one I like, because that's beside the point, for two
      reasons.
      First, it's simply intended as an example of language evolution.
      Second, regardless of its merits, it is unlikely to happen,
      because of the way that Perl 5 is parsed.
      In this post I will demonstrate a way of writing a parser,
      so that this change,
      or others, can be made in a straightforward way,
      and without designing your language into a corner.
    </p>
    <p>
      When initializing a hash, Perl 5 allows you to use not just commas,
      but also the so-called "wide comma" (<tt>=&gt;</tt>).
      The wide comma is suggestive visually, and it also has some smarts
      about what a hash key is:
      The hash key is always converted into a string, so that wide comma
      knows that in a key-value pair like this:
    </p><blockquote>
      <pre>
    key1 => 711,
</pre>
    </blockquote><p>
      that
      <tt>key1</tt>
      is intended as a string.
    </p>
    <p>
      But what about something like this?
    </p>
    <blockquote>
      <pre>
  {
   company name => 'Kamamaya Technology',
   employee 1 => first name => 'Jane',
   employee 1 => last name => 'Doe',
   employee 1 => title => 'President',
   employee 2 => first name => 'John',
   employee 2 => last name => 'Smith',
   employee 3 => first name => 'Clarence',
   employee 3 => last name => 'Darrow',
  }
</pre>
    </blockquote><p>
      Here I think the intent is obvious -- to create an employee database in the form
      of a hash of hashes, allowing spaces in the keys.
      In Data::Dumper format, the result would look like:
    </p><blockquote>
      <pre>
{
              'employee 2' => {
                                'last name' => '\'Smith\'',
                                'first name' => '\'John\''
                              },
              'company name' => '\'Kamamaya Technology\'',
              'employee 3' => {
                                'last name' => '\'Darrow\'',
                                'first name' => '\'Clarence\''
                              },
              'employee 1' => {
                                'title' => '\'President\'',
                                'last name' => '\'Doe\'',
                                'first name' => '\'Jane\''
                              }
            }
</pre>
    </blockquote>
    <p>And in fact, that is the output of the script in
      <a href="https://gist.github.com/jeffreykegler/9478391">
    this Github gist</a>,
        which parses the previous "extended Perl 5" snippet using a Marpa
        grammar before passing it on to Perl.
      </p>
      <p>Perl 5 does not allow a syntax like this,
      and looking at its parsing code will tell you why -- it's already
      a maintenance nightmare.
      The extension I've described above could, in theory, be added to Perl
      5, but doing so would aggravate an already desperate maintenance situation.
    <p>
      Now, depending on taste,
      you may be just as happy that you'll never
      see the extensions I have just outlined in Perl 5.
      But I don't think it is as easy to
      be happy about a parsing technology that
      quickly paints the languages which use it into a corner.
    </p>
    <h3>How it works</h3>
    <p>
      The code is in
      <a href="https://gist.github.com/jeffreykegler/9478391">
        a Github gist</a>.
	For the purposes of the example, I've implemented
	a toy subset of Perl.
	But this approach has been shown to scale.
	There are full Marpa-powered parsers of
	<a href="https://metacpan.org/release/MarpaX-Languages-C-AST">C</a>,
	<a href="https://metacpan.org/release/MarpaX-Languages-ECMAScript-AST">ECMAScript</a>,
	<a href="https://metacpan.org/release/MarpaX-xPathLike">XPath</a>, and
	<a href="https://metacpan.org/pod/distribution/Marpa-R2/html/pod/HTML.pod">liberal HTML</a>.
    </p>
    <p>Marpa is a general BNF parser, which means that anything you can write in BNF, Marpa can parse.
    For practical parsing, what matters are those grammars that can be parsed in linear time,
    and with Marpa that class is vast, including all the classes of grammar currently in practical use.
    To describe the class of grammars that Marpa parses in linear time,
    assume that you have either a left or right parser,
    with infinite lookahead,
    that uses regular expressions.
    (A parser like this is called LR-regular.)
    Assume that this LR-regular parser parses your grammar.
    In that case,
    you can be sure that
    Marpa will parse that grammar in linear time, and without doing the lookahead.
    (Instead Marpa tracks possibilities in a highly-optimized table.)
    Marpa also parses many grammars that are not LR-regular in linear time,
    but just LR-regular is very likely to include any class of grammar that you will be
    interested in parsing.
    The LR-regular grammars easily include all those that can be
    parsed using yacc, recursive descent or regular expressions.
    </p>
    <p>
    Marpa excels at those special hacks so necessary in recursive descent and other techniques.
    Marpa allows you to define events that will stop it at symbols or rules, both before and after.
    While stopped,
    you can hand processing over to your own custom code.
    Your custom code can feed your own tokens to the parse for as long as you like.
    In doing so, it can
    consult Marpa to determine exactly what symbols and rules have been recognized and
    which ones are expected.
    Once finished with custom processing,
    you can then ask Marpa to pick up again at any point you wish.
    </p>
    <h3>The craps game is over</h3>
    <p>The bottom line is that if you can describe your language extension in BNF,
    or in BNF plus some hacks,
    you can rely on Marpa parsing it in reasonable time.
    Language design has been like shooting crap in a casino
    that sets you up to
    win a lot of the first rolls before
    the laws of probability grind you down.
    Marpa changes the game.
    </p>
    <h3>To learn more</h3>
    <p>
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2
        is available on CPAN</a>.
      A list of my Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
        here</a>.
      There are
      new tutorials by
      <a href="http://marpa-guide.github.io/chapter1.html">Peter Stuifzand</a>
      and
      <a href="http://longanswers.blogspot.de/2013/06/transforming-syntax.html">amon</a>.
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        The Ocean of Awareness blog</a>
      focuses on Marpa,
      and it has
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>.
      Marpa has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page that I maintain</a>
      and Ron Savage maintains
      <a href="http://savage.net.au/Perl-modules/html/marpa.papers/index.html">
        another</a>.
      For questions, support and discussion, there is
      <a href="http://groups.google.com/group/marpa-parser">
        the "marpa parser"
        Google Group.</a>
    </p>
    <h3>Comments</h3>
    <p>
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
      Marpa's Google group</a>.
      </p>
  </body>
</html>
<br />
<p>posted at: 19:48 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/03/kv.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
