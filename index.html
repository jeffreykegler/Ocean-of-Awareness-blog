<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Tue, 21 Aug 2018</h3>
<br />
<center><a name="combinator2"> <h2>Marpa and combinator parsing 2</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <p>
    In
    <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/combinator.html">
    a previous post</a>,
    I outlined a method for using the Marpa algorithm as the basis for
    better combinator parsing.
    This post follows up
    with a trial implementation.
    </p>
    <p>For this trial,
    I choose the most complex example from the classic 1996 tutorial
    on combinator parsing by 
    Hutton and Meijer<a id="footnote-1-ref" href="#footnote-1">[1]</a>.
    Their example implements the offside-rule parsing of a functional language --
    parsing where whitespace is part of the syntax.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    The Hutton and Meijer example is for Gofer,
    a now obsolete implementation of Haskell.
    To make the example more relevant,
    I wrote a parser for Haskell layout
    according to the Haskell 2010 Language Report<a id="footnote-3-ref" href="#footnote-3">[3]</a>.
    </p>
    <p>For tests,
    I used the five examples (2 long, 3 short) provided
    in the 2010 Report<a id="footnote-4-ref" href="#footnote-4">[4]</a>,
    and the four examples given in the "Gentle Introduction" to Haskell<a id="footnote-5-ref" href="#footnote-5">[5]</a>.
    I implemented only enough of the Haskell syntax to run
    these examples,
    but this was enough to include a substantial subset of Haskell's
    syntax.
    </p>
    <p>This description of the implementation includes many extracts from
    the code.
    For those looking for more detail,
    the full code and test suite for this example are
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/gh-pages/code/haskell">
    on Github</a>.
    While the comments in the code do not amount to a tutorial, they are
    extensive.
    Readers who like to "peek ahead" are encouraged to do so.
    </p>
    <h2>Layout parsing and the off-side rule</h2>
    <p>It won't be necessary to know Haskell to follow this post.
    This section will describe Haskell's layout informally.
    Briefly, these two code snippets should have the same effect:
    <pre><tt>
       let y   = a*b
	   f x = (x+y)/y
       in f c + f d </tt><a id="footnote-6-ref" href="#footnote-6">[6]</a>
     </pre>
    <pre><tt>
       let { y   = a*b
	   ; f x = (x+y)/y
	   } </tt><a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </pre>
    <p>
    In my test suite, both code snippets produce the same AST.
    The first code display uses Haskell's implicit layout parsing,
    and the second code display uses explicit layout.
    In each, the "<tt>let</tt>" is followed by a block
    of declarations
    (symbol <tt>&lt;decls&gt;</tt>).
    Each decls contains one or more 
    declarations
    (symbol <tt>&lt;decl&gt;</tt>).
    For the purposes of determining layout,
    Haskell regards
    <tt>&lt;decls&gt;</tt> as a "block",
    and each
    <tt>&lt;decl&gt;</tt> as a block "item".
    In both displays, there are two items in
    the block.
    The first item is
    <tt>y = a*b</tt>,
    and the second
    <tt>&lt;decl&gt;</tt> item
    is <tt>f x = (x+y)/y</tt>.
    </p>
    <p>
    In explicit layout, curly braces surround the block,
    and semicolons separate each
    item.
    Implicit layout follows the "offside rule":
    The first element of the laid out block
    determines the "block indent".
    The first non-whitespace character of every subsequent non-empty line
    determines the line indent.
    The line indent is compared to the block indent.
    <ul>
    <li>If the line indent is deeper than the block indent,
    then the line continues the current block item.
    </li>
    <li>If the line indent is equal to the block indent,
    then the line starts a new block item.
    </li>
    <li>If the line indent is less than the block indent
    (an "outdent"),
    then the line ends the block.
    An end of file also ends the block.
    </li>
    </ul>
    Lines containing only whitespace are ignored.
    Comments count as whitespace.
    </p>
    <p>
    Explicit semicolons can be used
    in implicit layout:
    If a semicolon occurs in implicit layout,
    it separates block items.
    In our test suite,
    the example
    <pre><tt>
       let y   = a*b;  z = a/b
	   f x = (x+y)/z
       in f c + f d </tt><a id="footnote-8-ref" href="#footnote-8">[8]</a>
    </pre>
    contains three 
    <tt>&lt;decl&gt;</tt> items.
    </p>
    <p>The examples in the displays above are simple.
    The two long examples from the 2010 Report are
    more complicated:
    6 blocks of 4 different kinds,
    with nesting twice reaching
    a depth of 4.
    The two long examples in the 2010 Report are the same,
    except that one uses implicit layout and the other uses
    explicit layout.
    In the test of my Haskell subset parser,
    both examples produce identical ASTs.
    </p>
    <p>There are additional rules,
    including for tabs, Unicode characters and
    multi-line comments.
    These rules are not relevant in the examples I took from the Haskell literature;
    they present no theoretical challenge to this parsing method;
    and they are not implemented in this prototype Haskell parser.
    </p>
    <h2>The strategy</h2>
    <p>To tackle Haskell layout parsing, I use a separate
    combinator for each layout block.
    Every combinator, therefore, has its own block and item symbols,
    and its own block indent;
    and each combinator implements exactly one method of layout -- explicit or implicit.
    </p>
    <p>From the point of view of its parent combinator,
    a child combinator is a lexeme,
    and the parse tree it produces is the
    value of the lexeme.
    Marpa can automatically produce an AST,
    and it adds lexeme values to the AST as leaves.
    The effect is that Marpa automatically assembles
    a parse tree for us from the tree segments returned by the
    combinators.
    </p>
    <h2>Ruby Slippers semicolons</h2>
    <p>In outlining this algorithm, I will start by explaining
    where the "missing" semicolons come from in the implicit layout.
    Marpa allows various kinds of "events",
    including on discarded tokens.
    ("Discards" are tokens thrown away, and not used in the parse.
    The typical use of token discarding in Marpa is for the handling of whitespace
    and comments.)
    </p>
    The following code sets an event named 'indent', which
    happens when Marpa finds a newline followed by zero or more
    whitespace characters.<a id="footnote-9-ref" href="#footnote-9">[9]</a>
    This does not capture the indent of the first line of a file,
    but that is not an issue --
    the 2010 Report requires that the first indent be treated as a
    special case anyway.
    <pre><tt>
      :discard ~ indent event => indent=off
      indent ~ newline whitechars </tt><a id="footnote-10-ref" href="#footnote-10">[10]</a>
      </pre>
    <p>
    Indent events, like others, occur in the main read loop
    of each combinator.  Outdents and EOFs are dealt with by terminating
    the read loop.<a id="footnote-11-ref" href="#footnote-11">[11]</a>
    Line indents deeper than the current block indent are dealt with by
    resuming the read loop.  <a id="footnote-12-ref" href="#footnote-12">[12]</a>
    Line indents equal to the block indent trigger the reading of a
    Ruby Slippers semicolon as shown in the following:
    <pre><tt>
	$recce->lexeme_read( 'ruby_semicolon', $indent_start,
	    $indent_length, ';' ) </tt><a id="footnote-13-ref" href="#footnote-13">[13]</a>
    </footnote>
    </pre>
    </p>
    <h2>Ruby Slippers</h2>
    <p>
    In Marpa, a "Ruby Slippers" symbol is one which does not actually occur
    in the input.
    Ruby Slippers parsing is new with Marpa,
    and made possible because Marpa is left-eidetic.
    By left-eidetic, I mean that Marpa knows, in full
    detail, about the parse to the left of its current position,
    and can provide that information to the parsing app.
    This implies that Marpa also knows which tokens are acceptable
    to the parser at the current location,
    and which are not.
    </p>
    Ruby Slippers parsing enables a very important trick which
    is useful in "liberal"
    parsing -- parsing where certain elements might be in some sense
    "missing".
    With the Ruby Slippers you can design a "liberal" parser with
    a "fascist" grammar.
    This is, in fact, how the Haskell 2010 Report's
    context-free grammar is designed --
    the official syntax requires explicit layout,
    but Haskell programmers are encouraged to omit most of the explicit
    layout symbols,
    and Haskell implementations are required to "dummy up" those
    symbols in some way.
    Marpa's method for doing this is left-eideticism and Ruby Slippers
    parsing.
    <p>The term "Ruby Slippers" refers to a widely-known scene in the "Wizard of Oz" movie.
    Dorothy is in the fantasy world of Oz, desperate to return to Kansas.
    But, particularly after a shocking incident in which orthodox Oz wizardry
    is exposed as an affable fakery,
    she is completely at a loss as to how to escape.
    The "good witch" Glenda appears and tells Dorothy that in fact she's always
    had what she's been wishing for.
    The Ruby Slippers, which she had been wearing all through the movie,
    can return her to Kansas.
    All Dorothy needs to do is wish.
    </p>
    <p>In Ruby Slippers parsing,
    the "fascist" grammar "wishes" for lots of things that may not be in
    the actual input.
    Procedural logic here plays the part of a "good witch" -- it tells
    the "fascist" grammar that what it wants has been there all along,
    and supplies it.
    To do this,
    the procedural logic has to have a reliable way of knowing what the parser
    wants.
    Marpa's left-eideticism provides this.
    </p>
    <h2>Ruby Slippers combinators</h2>
    <p>This brings us to a question
    I've postponed -- how do we know which combinator
    to call when?
    The answer is Ruby Slippers parsing.
    First, here are some lexer rules for "unicorn" symbols.
    We use unicorns when symbols need to appear in Marpa's lexer,
    but must never be found in actual input.
    </p>
    <pre><tt>
      :lexeme ~ L0_unicorn
      L0_unicorn ~ unicorn
      unicorn ~ [^\d\D]
      ruby_i_decls ~ unicorn
      ruby_x_decls ~ unicorn </tt><a id="footnote-14-ref" href="#footnote-14">[14]</a>
    </footnote>
    </pre>
    <p>
    <tt>&lt;unicorn&gt;</tt> is defined to match 
    <tt>[^\d\D]</tt>.
    This pattern is all the symbols which are not digits
    and not non-digits -- in other words, it's impossible that this
    pattern will ever match any character.
    The rest of the statements declare other unicorn lexemes
    that we will need.
    <tt>&lt;unicorn&gt;</tt> and
    <tt>&lt;L0_unicorn&gt;</tt> are separate,
    because we need to use
    <tt>&lt;unicorn&gt;</tt> on the RHS of some lexer rules,
    and a Marpa lexeme can never occur
    on the RHS of a lexer rule.<a id="footnote-15-ref" href="#footnote-15">[15]</a>
    </p>
    <p>In the above Marpa rule,
    <ul>
    <li>
    <tt>&lt;decls&gt;</tt> is the symbol from the 2010 Report;
    </li>
    <li>
    <tt>&lt;ruby_i_decls&gt;</tt> is a Ruby Slippers symbol for
    a block of declarations with implicit layout.
    </li>
    <li>
    <tt>&lt;ruby_x_decls&gt;</tt> is a Ruby Slippers symbol for
    a block of declarations with explicit layout.
    </li>
    <li>
    <tt>&lt;laidout_decls&gt;</tt> is a symbol (not in the 2010 Report)
    for a block of declarations covering all the possibilities for
    a block of declarations.
    </li>
    </ul>
    <pre><tt>
      laidout_decls ::= ('{') ruby_x_decls ('}')
	       | ruby_i_decls
	       | L0_unicorn decls L0_unicorn </tt><a id="footnote-16-ref" href="#footnote-16">[16]</a>
    </pre>
    </p>
    <p>It is the expectation of a 
    <tt>&lt;laidout_decls&gt;</tt> symbol that causes child
    combinators to be invoked.
    Because <tt>&lt;L0_unicorn&gt;</tt> will never be found
    in the input,
    the 
    <tt>&lt;decls&gt;</tt> alternative will never match --
    it is there for documentation and debugging reasons.<a id="footnote-17-ref" href="#footnote-17">[17]</a>
    Therefore Marpa, when it wants a
    <tt>&lt;laidout_decls&gt;</tt>,
    will look for a
    <tt>&lt;ruby_x_decls&gt;</tt> 
    if a open curly brace is read;
    and a
    <tt>&lt;ruby_i_decls&gt;</tt> otherwise.
    Neither <tt>&lt;ruby_x_decls&gt;</tt> 
    or
    <tt>&lt;ruby_i_decls&gt;</tt> will ever be found in the
    input,
    and Marpa will reject the input,
    causing a "rejected" event.
    <h2>Rejected events</h2>
    <p>In this code, as often,
    the "good witch" of Ruby Slippers does her work through
    "rejected" events.
    These events can be set up to happen when, at some parse
    location, none of the tokens that Marpa's internal lexer
    finds are acceptable.
    </p>
    <p>
    In the "rejected" event handler,
    we can use Marpa's left eideticism to find out what
    lexemes Marpa <b>would</b> consider acceptable.
    Specifically, there is a <tt>terminals_expected()</tt>
    method which returns a list of the symbols acceptable
    at the current location.
    </p>
    <pre><tt>
            my @expected =
              grep { /^ruby_/xms; } @{ $recce->terminals_expected() }; </tt><a id="footnote-18-ref" href="#footnote-18">[18]</a>
    </footnote></pre>
    <p>Once we "grep" out all but the symbols with the "<tt>ruby_</tt>" prefix,
    there are only 4 non-overlapping possibilities:
    </p>
    <ul>
    <li>Marpa expects a 
    <tt>&lt;ruby_i_decls&gt;</tt>
    lexeme;
    </li>
    <li>Marpa expects a 
    <tt>&lt;ruby_x_decls&gt;</tt>
    lexeme;
    </li>
    <li>Marpa expects a 
    <tt>&lt;ruby_semicolon&gt;</tt>
    lexeme;
    </li>
    <li>Marpa does not expect
    any of the Ruby Slippers lexemes;
    </li>
    </ul>
    <p>If Marpa does not expect any of the Ruby Slippers
    lexemes, there was a syntax error in the Haskell code.<a id="footnote-19-ref" href="#footnote-19">[19]</a>
    <p>If a <tt>&lt;ruby_i_decls&gt;</tt>
    or a <tt>&lt;ruby_x_decls&gt;</tt>
    lexeme is expected, a child combinator is invoked.
    The Ruby Slippers symbol determines
    whether the child combinator looks for implicit
    or explicit layout.
    In the case of implicit layout, the location of
    the rejection determines the block indent.<a id="footnote-20-ref" href="#footnote-20">[20]</a>
    </footnote>
    </p>
    <p>If a 
    <tt>&lt;ruby_semicolon&gt;</tt>
    is expected, then the parser is at the point where a
    new block item could start,
    but none was found.
    Whether the block was implicit or explicit,
    this indicates we have reached the end of the block,
    and should return control to the parent combinator.<a id="footnote-21-ref" href="#footnote-21">[21]</a>
    </footnote>
    </p>
    <p>
    To explain why
    <tt>&lt;ruby_semicolon&gt;</tt>
    indicates end-of-block,
    we look at both cases.
    In the case of an explicit layout combinator,
    the rejection should have been caused by a closing
    curly brace, and
    we return to the parent combinator
    and retry it.
    In the parent combinator, the closing curly brace
    will be acceptable.
    </p>
    <p>If we experience a "rejected" event while
    expecting a
    <tt>&lt;ruby_semicolon&gt;</tt> in an implicit layout
    combinator,
    it means we did not find an explicit semicolon;
    and we also never found the right indent for creating a Ruby semicolon.
    In other words, the indentation is telling us that we are at the end
    of the block.
    We therefore return control to the parent combinator.
    </p>
    <h2>Conclusion</h2>
    <p>
    With this, we've covered the major points of this Haskell prototype
    parser.
    It produces an AST whose structure and node names are those of
    the 2010 Report.
    (The Marpa grammar introduces non-standard node names and rules,
    but these are pruned from the AST in post-processing.)
    </p>
    <p>
    In the code, the grammars from the 2010 Report are included for
    comparison, so a reader can easily determine what syntax we left out.
    It might be tedious to add the rest,
    but I believe it would be unproblematic, with one interesting exception:
    fixity.
    To deal with fixity, we may haul out the Ruby Slippers again.
    </p>
    <h2>The code, comments, etc.</h2>
    <p>A permalink to the
    full code and a test suite for this prototype,
    as described in this blog post,
    is
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell">
    on Github</a>.
    I expect to update this code,
    and the latest commit can be found
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/gh-pages/code/haskell">
    here</a>.
    Links for specific lines of code in this post are usually
    static permalinks to earlier commits.
    </p>
    <p>
      To learn more about Marpa,
      a good first stop is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
    Graham Hutton and Erik Meijer,
    <cite>Monadic parser combinators</cite>, Technical Report NOTTCS-TR-96-4.
    Department of Computer Science, University of Nottingham, 1996,
    pp 30-35.
    <a href="http://eprints.nottingham.ac.uk/237/1/monparsing.pdf">
    http://eprints.nottingham.ac.uk/237/1/monparsing.pdf</a>.
    Accessed 19 August 2018.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
    I use
    whitespace-significant parsing as a convenient example
    for this post,
    for historical reasons and
    for reasons of level of complexity.
    This should not be taken to indicate that I recommend it
    as a language feature.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3"><b>3.</b>
    Simon Marlow,
    <cite>Haskell 2010 Language Report</cite>,
    2010.
    <a href="https://www.haskell.org/onlinereport/haskell2010/haskell.html#haskellpa1.html">
    Online version accessed
    21 August 2018.</a>
    For layout, see in particular
    section 2.7 (pp. 12-14)
    and section 10.3 (pp. 131-134).
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4"><b>4.</b>
    2010 Report.
    The short examples are on p. 13 and p. 134.
    The long examples are on p. 14.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5"><b>5.</b>
    Paul Hudak, John Peterson and Joseph Fasel
    <cite>Gentle Introduction To Haskell</cite>, version 98.
    Revised June, 2000 by Reuben Thomas.
    <a href="https://www.haskell.org/tutorial/index.html">
    Online version accessed
    21 August 2018.</a>
    The examples are in section 4.6,
    which is on pp. 20-21 of
    <a href="https://www.haskell.org/tutorial/haskell-98-tutorial.pdf">
    the October 1999 PDF</a>.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6"><b>6.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/short.t#L21">
      Github Permalink.</a>
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7"><b>7.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/short.t#L28">
      Github Permalink.</a>
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8"><b>8.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/short.t#L43">
      Github Permalink.</a>
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9"><b>9.</b>
    Single-line comments are dealt with properly by lexing them
    as a different token and discarding them separately.
    Handling multi-line comments is not yet implemented --
    it is easy in principle but
    tedious in practice and the examples drawn from the
    Haskell literature did not provide any test cases.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10"><b>10.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L608">
      Github Permalink.</a>
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11"><b>11.</b>
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L1007">
    Github Permalink.</a>
 <a href="#footnote-11-ref">&#8617;</a></p>
<p id="footnote-12"><b>12.</b>
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L1028">
    Github Permalink.</a>
 <a href="#footnote-12-ref">&#8617;</a></p>
<p id="footnote-13"><b>13.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L1101">
      Github Permalink.</a>
 <a href="#footnote-13-ref">&#8617;</a></p>
<p id="footnote-14"><b>14.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L550">
      Github Permalink.</a>
 <a href="#footnote-14-ref">&#8617;</a></p>
<p id="footnote-15"><b>15.</b>
    The reason for this is that by default a Marpa grammar determines
    which of its symbols are lexemes using the presence of those
    symbol on the LHS and RHS
    of the rules
    in its lexical and context-free grammars.
    A typical Marpa grammar
    requires a minimum of explicit lexeme declarations.
    (Lexeme declarations are statements with the <tt>:lexeme</tt>
    pseudo-symbol on their LHS.)
    As an aside,
    the Haskell 2010 Report is not always careful about the lexer/context-free
    boundary,
    and adopting its grammar
    required more use of Marpa's explicit lexeme declarations
    than usual.
 <a href="#footnote-15-ref">&#8617;</a></p>
<p id="footnote-16"><b>16.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L361">
      Github Permalink.</a>
 <a href="#footnote-16-ref">&#8617;</a></p>
<p id="footnote-17"><b>17.</b>
    Specifically, the presense of a 
    <tt>&lt;decls&gt;</tt> alternative silences the usual warnings about
    symbols inaccessible from the start symbol.
    These warnings can be silenced in other ways,
    but at the prototype stage it is convenient to check that
    all symbols supposed to be accessible through
    <tt>&lt;decls&gt;</tt> are in fact accessible.
    There is a small startup cost to allowing the extra symbols
    in the grammars,
    but the runtime cost is probably not measureable.
 <a href="#footnote-17-ref">&#8617;</a></p>
<p id="footnote-18"><b>18.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L1055">
      Github Permalink.</a>
 <a href="#footnote-18-ref">&#8617;</a></p>
<p id="footnote-19"><b>19.</b>
    Currently the handling of these is simplistic.
    A practical implementation of this method would want better reporting.
    In fact, Marpa's left eideticism allows some interesting things
    to be done in this respect.
 <a href="#footnote-19-ref">&#8617;</a></p>
<p id="footnote-20"><b>20.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L1081">
      Github Permalink.</a>
 <a href="#footnote-20-ref">&#8617;</a></p>
<p id="footnote-21"><b>21.</b>
      <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/blob/6c76ffc791d24f4515edea376ac31ad7264a420c/code/haskell/haskell.pm#L1072">
      Github Permalink.</a>
 <a href="#footnote-21-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 10:24 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/combinator2.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sun, 08 Jul 2018</h3>
<br />
<center><a name="knuth_1965_2"> <h2>Undershoot: Parsing theory in 1965</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <blockquote>The difference between theory and practice is
      that in theory there is no difference between
      theory and practice,
      but in practice, there is.<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    </blockquote>
    <p>
      Once it was taken seriously that humans might have the power to, for
      example, "read" a chessboard in a way that computers could not beat.
      This kind of "computational mysticism" has taken a beating.
      But it survives in one last stronghold -- parsing theory.
    </p><p>
      In
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html">
        a previous post</a>,
      I asked "Why is parsing considered solved?"
      If the state of the art of computer parsing is taken as anything close to its ultimate solution,
      then it is a case of "human exceptionalism" --
      the human brain has some
      power that makes it much better at parsing than computers can be.
      It is very unlikely resorting to human exceptionalism as an explanation
      would be accepted
      for any other problem in computer science.
      Why is it accepted for parsing theory?<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    </p>
    <p>
      The question really requires two separate answers:
    </p><ul>
      <li>"Why do practitioners accept the current state of the art as the solution?" and
      </li><li>"Why do the theoreticians accept the current state of the art as the solution?"
      </li></ul>
    <p>
    </p><p>In one sense, the answer to both questions is the same --
      because of the consensus created by Knuth's 1965 paper
      "On the translation of languages from left to right".
      In
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html">a previous post</a>,
      I looked at Knuth 1965
      and I answered the practitioner question in detail.
      But, for the sake of brevity,
      I answered the question about the theoreticians in outline.
      This post expands on that outline.
    </p>
    <h2>The Practitioners</h2>
    <p>
      To summarize, in 1965,
      <b>practitioners</b>
      accepted the parsing problem as solved
      for the following reasons.
    </p>
    <ul>
      <li>In 1965, every practical parser was stack-driven.</li>
      <li>As of 1965, stacks themselves were quite leading edge.
        As recently as 1961,
        a leading edge article<a id="footnote-3-ref" href="#footnote-3">[3]</a>
        could not assume that its readers knew what "pop" and "push" operations
        were.
      </li>
      <li>An algorithm that combined state transitions and stack operations was
        already a challenge to existing machines.
        In 1965, any more complicated algorithm was likely to be unuseable
        in practice.
      </li>
      <li>Last, but not least, the theoreticians assured the
        practitioners that
        <tt>LR</tt>-parsing was either state-of-the-art
        or beyond,
        so making more agressive use of hardware
        would be futile.
      </li>
    </ul>
    <h2>What about the theorists?</h2>
    <p>The practitioners of 1965, then,
      were quite reasonable in feeling that
      <tt>LR</tt>-parsing was as good as anything they were likely to be able
      to implement any time soon.
      And they were being told by the theorists that,
      in fact,
      it never would get any better --
      there were theoretical limits on parsers that faster
      hardware could not overcome.
    </p>
    <p>We now know that the theorists were wrong --
      there are non-<tt>LR</tt>
      parsers which are better than the
      <tt>LR</tt>
      parsers are at
      <tt>LR</tt>
      grammars.
      What made the theorists go astray?
    </p>
    <h2>How theorists work</h2>
    <p>As the epigraph for this post reminds us,
      theorists who hope to guide practitioners have to confront a big problem --
      theory is practice only in theory.
      Theoreticians
      (or at least the better ones, like Knuth)
      know this,
      but they try to make theory as reliable a guide to
      practice as possible.
    </p>
    <p>One of the most important examples of the theoretician's successes
      is asymptotic notation, which we owe to Knuth<a id="footnote-4-ref" href="#footnote-4">[4]</a>.
      Asymptotic notation is
      more commonly referred to as big-O notation.
      The term "asymptotic notation"
      emphasizes its most dangerous aspect
      from a practical point of view:
      Asymptotic notation assumes
      that the behavior of most interest
      is the behavior for arbitrarily large inputs.
    </p>
    <p>
      Practical inputs can be very large but,
      by definition,
      they are never arbitrarily large.
      Results in asymptotic terms
      might be what is called "galactic" --
      they might have
      relevance only in situations which cannot possibly occur in practice.
    </p>
    <p>
      Fortunately for computer science,
      asymptotic results usually are
      not "galactic".
      Most often asymptotic results are not only
      relevant to practice --
      they are extremely relevant.
      Wikipedia pages for algorithms put
      the asymptotic complexities in special displays,
      and these displays are one of the first
      things that some practitioners look at.
    </p>
    <h2>Bracketing</h2>
    <p>Since coming up with a theoretical model that is equivalent
      to "practical" is impossible,
      theoreticians often work like artillerists.
      Artillerists often deliberately overshoot and undershoot,
      before they "fire for effect".
      "Bracketing" their target in this way has disadvantages --
      it reduces the element of surprise,
      and can even allow the enemy to get their counter-fire in first.
      But, nasty as these consequences could be,
      the advantage in accuracy is usually held to outweigh them.
    </p>
    <p>The practice of theoretical computer science is
      less risky,
      which makes "bracketing" a very attractive approach to
      tricky problems.
      Theoreticians often
      try to "bracket" practice between an "undershoot"
      and an "overshoot".
      The undershoots are models simple and efficient enough to be practical,
      but too weak to capture all the needs of practice.
      The overshoots are models which capture everything
      a practitioner needs,
      but which are too complicated and/or too resource-intensive
      for practice.
    </p><p>The P vs. NP problem is an active example of a bracketing technique.
      You will sometimes read that
      the P/NP boundary is expected to be
      that between practical and impractical,
      but this is an extreme simplification.
      P includes complexities like
      <tt>O(n^1000000)</tt>,
      where the complexity for even
      <tt>n == 2</tt>
      is
      a nunber which, in decimal form,
      fills many pages.
      Modulo bold advances in quantum computing,
      I cannot imagine that
      <tt>O(n^1000000)</tt>
      will ever be
      practical.
      And you can make the complexities much harder
      than
      <tt>O(n^1000000)</tt>
      without ever reaching P-hard.
    </p>
    <p>
      So P-hard is beyond any reasonable definition of "practical" --
      it is an "overshoot".
      But the P vs. NP question is almost certainly very relevant to what is "practical".
      Resolving the P vs. NP question is likely
      to be an important or even necessary step.
      It is a mystery that such a seemingly obvious
      question has resisted the best efforts of the theoreticians
      for so long,
      and the solution of P vs. NP is likely
      to bring
      new insights
      into asymptotic complexity.
    </p>
    <h2>Bracketing practical parsing</h2>
    <p>When Knuth published his 1965,
      "practical parsing" was already bracketed.
      On the overshoot side, Irons had already published a parser for
      context-free grammars.
      Worst case, this ran in exponential time,
      and it was, and remains, expected that general context-free parsing
      was not going to be practical.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </p>
    <p>On the undershoot side,
      there were regular expressions and recursive descent.
      Regular expressions are fast and very practical,
      but parse a very limited set of grammars.
      Recursive descent is also fast and,
      since it parses a larger set of grammars,
      was the closest undershoot.
    </p>
    <h2>Mistake 1: The misdefinition of "language"</h2>
    <p>To curry respect from the behaviourists,
      American linguistics for many years banned any reference
      to meaning.
      Behaviorists looked down on
      hypothesized mental states as not worthy of "science",
      and it's hard to have a theory of meaning
      without conjectures about mental states.
      Without mental states,
      language was just a set of utterances.
      So in 1926 the linguist Leonard Bloomfield
      dutifully
      defined a "language" as a set of "utterances"
      (for our purposes, "strings"),
      and through the 30s and 40s most American
      linguists followed him.
    </p>
    <p>After a brief nod to this tradition,
      Noam Chomsky restored sanity to linguistics.
      But it was too late for computer science.
      Automata theory adopted the semantics-free definition.
      In 1965, Knuth inherited a lot of prior work,
      almost all of which ignored,
      not just meaning or semantics,
      but even syntax and structure.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </p>
    <h2>Language extension versus language intension</h2>
    <p>Knuth, of course, wanted to make contact with prior art.
      The definition he had inherited seemed to work well enough
      and Knuth's 1965 defines a language as a set of strings.
      Most subsequent work has refused to breach this tradition.
    </p>
    <p>In most people's idea of what a language is,
      the utterances/strings mean something --
      you cannot take just
      any set of meaningless strings and call it a language.
      So the parsing theorists and everybody else had
      two different definitions of language.
    </p>
    <p>But parsing theory also hoped to produce results relevant
      to practice,
      and few people are interested in recognizing meaningless strings --
      almost everybody who parses is interested in (at a minimum)
      finding some kind of structure in what they parse,
      in order to do something with the result of the parse.
      Parsing theorists ended up using the word "language" in one
      sense, but implying that results they found worked
      for the word "language" in the usual sense.
    </p>
    <p>
      At this point both senses of the word "language"
      have gotten entrenched in parsing theory.
      Instead of making up a new terminology for this blog post,
      I will borrow a distinction from linguistics
      and speak of
      <b>the extension of a language</b>
      and
      <b>the intension of a language</b>.
      The extension of a language is the Bloomfieldian defintion --
      the set of utterances/strings in the language.
      The intension of a language, for our purposes here,
      can be regarded as its BNF grammar.
      Each language intension will have (if it is well-defined)
      exactly one extension.
      But multiple language intensions can have the same extension.
    </p>
    <h2>Red Herring 1: The stack machine model as a natural boundary</h2>
    <p>The temptation to use language extensions as
      a proxy for
      <tt>LR</tt>-grammars must have been overwhelming.
      It turns out that the language extension of
      deterministic stack machines
      is
      <b>exactly</b>
      that of the
      <tt>LR</tt>
      grammars.
      Further,
      the language extension of the context-free grammars is
      exactly that of the non-deterministic stack machines.
      (Non-deterministic stack machines are
      stack machines which can "fork" new instances of themselves on the fly.)
    </p>
    <p>
      If you take language extensions as the proxy for grammars,
      things fall into place very neatly:
      the
      <tt>LR</tt>-parsers are the deterministic subset of the
      context-free parsers.
      And "deterministic" seemed like a very good approximation
      of practical.
      Certainly non-deterministic parsing is probably not practical.
      And the best practical parsers in 1965 were
      deterministic stack parsers.
    </p><p>
      Viewed this way,
      <tt>LR</tt>-parsing looked like the equivalent
      of practical parsing.
      It was a "direct hit",
      or as close to a exact equivalent of practical parsing
      as theory was going to get.
    </p>
    <p>As we shall see,
      with this red herring,
      the reasoning went astray.
      But disaster was not inevitable.
      The whole point of bracketing, after all,
      is that it allows you to correct errors.
      Another red herring, however, resulted in
      parsing theory going on a decades-long
      wrong turn.
    </p>
    <h2>Red Herring 2:
      <tt>LR</tt>
      parsers are not good at
      <tt>LR</tt>
      grammars</h2>
    <p>The second red herring led to the mis-bracketing of practical
      parsing.
      Having seemingly established that
      <tt>LR</tt>-parsing is a natural boundary
      in the hierarchy of languages,
      Knuth discovered that general
      <tt>LR</tt>-parsers were very far from practical.
      <tt>LR</tt>
      parsing goes out to
      <tt>LR(k)</tt>
      for arbitrary
      <tt>k</tt>,
      but even
      <tt>LR(1)</tt>
      parsing was impractical in 1965 --
      in fact, it is rare in practical use today.
      As the
      <tt>k</tt>
      in
      <tt>LR(k)</tt>
      grows, the size of the tables grows exponentially,
      while the value of the additional lookahead rapidly diminishes.
      It is not likely that
      <tt>LR(2)</tt>
      parsing will ever see much practical use,
      never mind
      <tt>LR(k)</tt> for any <tt>k</tt>
      greater than 2.
    </p>
    <p>
      From this it was concluded that
      <tt>LR</tt>-parsing is an overshoot.
      In reality,
      as Joop Leo was to show,
      it is an
      <b>undershoot</b>,
      and in practical terms a very large one.
      If you mistake an undershoot for an overshoot,
      bracketing no longer works,
      and you are not likely to hit your target.
    </p>
    <h2>The Wrong Turn</h2>
    <p>
      Summing up,
      parsing theorists concluded,
      based on the results of Knuth 1965,
      that
    </p><ul>
      <li>LR-parsing is a good approximation to practical parsing -- it brackets
        it closely.</li>
      <li>LR-parsing is an overshoot.</li>
      <li>A subset of
        <tt>LR</tt>-parsing will be the solution to the parsing problem.</li>
    </ul>
    <h2>Signs of trouble ignored</h2>
    <p>There were, in hindsight, clear signs
      that
      <tt>LR</tt>
      language extensions were not a good proxy for
      <tt>LR</tt>
      grammars.
      <tt>LR</tt>
      grammars form a hierarchy --
      for every
      <tt>k&#8805;0</tt>,
      there is an
      <tt>LR</tt>
      grammar which
      is
      <tt>LR(k+1)</tt>, but which is not
      <tt>LR(k)</tt>.
    </p>
    <p>
      But if you look at extensions
      instead of grammars,
      the hierarchy immediately
      collapses --
      every
      <tt>LR(k)</tt>
      language extension is also
      an
      <tt>LR(1)</tt>
      language extension,
      as long as
      <tt>k&#8805;1</tt>.
      Only
      <tt>LR(0)</tt>
      remains distinct.
    </p>
    <p>It gets worse.
      In most practical applications,
      you can add an end-of-input marker to a grammar.
      If you do this the
      <tt>LR</tt>
      extension hierarchy collapses totally --
      every
      <tt>LR(k)</tt>
      language extension is also an
      <tt>LR(0)</tt>
      language extension.
    </p>
    <p>In short, it seems that,
      as a proxy for
      <tt>LR</tt>
      grammars,
      <tt>LR</tt>
      language extensions are likely to be completely worthless.
    </p>
    <h2>Why didn't Knuth see the problem?</h2>
    <p>Why didn't Knuth see the problem?
      Knuth certainly noted the strange behavior of the
      <tt>LR</tt>
      hierarchy
      in extensional terms -- he discovered it,
      and devoted several dense pages of his 1965 to laying
      out the complicated mathematics involved.
    </p>
    <p>
      So why did
      Knuth expect to get away with punning
      intension and extension,
      even in the face of some very unsettling results?
      Here, the answer is very simple --
      "punning" had always worked before.
    </p>
    <p>
      Regular expressions are easily turned into parsers<a id="footnote-7-ref" href="#footnote-7">[7]</a>,
      so the language extension of a regular grammar is an adequate approximation
      to its intension.
      Context-free recognition has the same complexity,
      and in practice uses the same algorithms,
      as context-free parsing,
      so here again,
      language extension is a good approximation
      of language intension.
    </p>
    <p>
      And the
      <tt>LL</tt>
      language extensions follow a strict hierarchy --
      for every
      <tt>k&#8805;0</tt>,
      <tt>LL(k+1)</tt>
      is a proper superset of
      <tt>LL(k)</tt>.
      This fact forces
      <tt>LL</tt>
      grammars to follow the same
      hierarchy<a id="footnote-8-ref" href="#footnote-8">[8]</a>.
      So, when studying complexity,
      <tt>LL</tt>
      language extensions are an excellent proxy for
      <tt>LL</tt>
      grammars.
    </p>
    <p>
      Based on past experience,
      Knuth had reason to believe
      he could use language extensions as a proxy
      for grammars,
      and that the result would be
      a theory that was a reliable
      guide to practice.
    </p>
    <h2>Aftermath</h2>
    <p>In
      <a href="https://jeffreykegler.github.io/personal/timeline_v3">
        my timeline of parsing</a>,
      I describe what happened next.
      Briefly,
      theory focused on finding a useful subset of
      <tt>LR(1)</tt>.
      One,
      <tt>LALR</tt>, became the favorite and
      the basis of the
      <tt>yacc</tt>
      and
      <tt>bison</tt>
      tools.
    </p>
    <p>
      Research into parsing of supersets of
      <tt>LR</tt>
      became rare.
      The theorists were convinced the
      <tt>LR</tt>
      parsing
      was the solution.
      These were so convinced that when,
      in 1991, Joop Leo discovered a practical way to
      parse an
      <tt>LR</tt> superset,
      the result went unimplemented for decades.
    </p>
    <p>In 1965, the theoreticians gave a lot of weight
      to the evidence from the world of practice,
      but probably not undue weight.
      Going forward, it was a different story.
    </p>
    <p>
      Leo had,
      in essence,
      disproved the implied conjecture of Knuth 1965.
      But the question is
      not an explicit mathematical question,
      like that of P vs. NP.
      It is a slipprier one -- capturing practice.
      Practitioners left it to the theoreticians to keep up with
      the literature.
      But theoreticians, as long as
      <tt>LR</tt>-superset methods did not
      come into use in the world of practice,
      felt no need to revisit their conclusions.
    </p>
    <h2>Comments, etc.</h2>
    <p>
      I encourage
      those who want to know more about the story of Parsing Theory
      to look at my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3">
        Parsing: a timeline 3.0</a>.
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel:
      <tt>#marpa</tt> at <tt>freenode.net</tt>.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
        Attributed to Jan L. A. van de Snepscheut and Yogi Berra.
        See
        <a href="https://en.wikiquote.org/wiki/Jan_L._A._van_de_Snepscheut">
          https://en.wikiquote.org/wiki/Jan_L._A._van_de_Snepscheut</a>,
        accessed 1 July 2018.
        I quote my preferred form  of this --
        the one it takes in
        Doug Rosenberg and Matt Stephens,
        <cite>Use Case Driven Object Modeling with UML: Theory and Practice</cite>,
        2007,
        p. xxvii.
        Rosenberg and Stephens is also the accepted authority for its attribution.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
        As an aside, I am open to the idea that
        the human mind has abilities that Turing machines cannot improve on
        or even duplicate.
        When it comes to
        survival heuristics tied to the needs of human bodies, for example,
        it seems very reasonable to at least entertain the conjecture
        that the human mind might be near-optimal,
        particularly in big-O terms.
        But when it comes to ability to solve problems which can be formalized
        as "puzzles" -- and syntactic analysis is one of these --
        I think that resort to human exceptionalism
        is a sign of desperation.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
          Oettinger, Anthony. "Automatic Syntactic Analysis and the Pushdown
          Store",
          <cite>Proceedings of Symposia in Applied Mathematics</cite>,
          Volume 12,
          American Mathematical Society, 1961.
          Oettinger describes "push" and "pop"
          stack operations in "Iversion notation" -- what
          later became APL.
          See the discussion of Oettinger in
          my
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html">
            "Why is parsing considered solved?" post</a>.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
        Knuth did not invent asymptotic notation --
        it comes from calculus --
        but he introduced it to computer science
        and motivated its use.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
        The best lower bound for context-free parsing is still
        <tt>O(n)</tt>.
        So it is even possible that there is a practical
        linear-time general context-free
        parser.
        But its discovery would be a big surprise.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
        In
        <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html">
          another blog post</a>,
        I talk about the use of the word
        "language" in parsing theory
        in much more detail.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
        For example,
        regular expressions can be extended with "captures".
        Captures cannot handle recursion, but neither can regular expressions,
        so captures are usually sufficient to provide all the structure
        an application wants.
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
        The discussion of the
        <tt>LL(k)</tt>
        hierarchy is in a sense anachronistic --
        the
        <tt>LL(k)</tt>
        hierachy was not studied until after 1965.
        But Knuth certainly was aware of recursive descent,
        and it seems reasonable to suppose that,
        even in 1965,
        he had a sense of what
        the
        <tt>LL</tt>
        hierarchy would look like.
 <a href="#footnote-8-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 21:02 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/07/knuth_1965_2.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Wed, 20 Jun 2018</h3>
<br />
<center><a name="lrecursion"> <h2>Parsing left recursions</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>Left recursion</h2>
    <p>A lot has been written about parsing left recursion.
    Unfortunately, much of it simply adds to the mystery.
    In this post, I hope to frame the subject clearly and briefly.
    <p>
    </p>I expect the reader has some idea of what left recursion is,
    and perhaps some experience of it as an issue.
    Informally, left recursion occurs when a symbol expands to something
    with itself on the left.
    This can happen directly, for example, if
    production <tt>(1)</tt> is in a grammar.
    </p>
    <pre><tt>
    (1) A ::= A B
    </tt></pre>
    Indirect left recursion happens when,
    for example,
    <tt>(2)</tt> and
    <tt>(3)</tt>
    are productions in a grammar.
    <pre><tt>
    (2) A ::= B C
    (3) B ::= A D
    </tt></pre>
    A grammar with productions
    <tt>(4)</tt> and
    <tt>(5)</tt>
    has a "hidden" left recursion.
    <pre><tt>
    (4) A ::= B A C
    (5) B ::= # empty
    </tt></pre>
    This is because
    <tt>&lt;A&gt;</tt>
    ends up leftmost in derivations like:
    <pre><tt>
    (6) A  &#10230; B A C &#10230 A C
    </tt></pre>
    In derivation <tt>(6)</tt>,
    production <tt>(4)</tt> was applied,
    then production <tt>(5)</tt>.
    <p>For those into notation,
    a grammar is left recursive if and only if it allows a derivation of the
    form
    <pre><tt>
    (7) A  &#10230;<sup>+</sup> &beta; A &gamma; </tt> where <tt> &beta; = &epsilon;
    </tt></pre>
    In <tt>(7)</tt> <tt>&epsilon;</tt> is the empty string,
    and 
    <tt> &alpha; &#10230;<sup>+</sup> &beta;</tt>
    indicates that <tt>&alpha;</tt> derives <tt>&beta;</tt>
    in one or more rule applications.
    <h2>So, OK, what is the problem?</h2>
    <p>The problem with parsing left recursions is that if you are parsing
    using a derivation like
    <pre><tt>
    (8) A  &#10230; A B </tt>
    </tt></pre>
    then you have defined
    <tt>&lt;A&gt;</tt>
    in terms of 
    <tt>&lt;A&gt;</tt>.
    All recursions can be a problem,
    but left recursions are a particular problem because almost all practical
    parsing methods<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    proceed left to right,
    and derivations like <tt>(8)</tt> will lead many of
    the most popular algorithms straight into
    an infinite regress.
    <h2>Why do some algorithms not have a problem?</h2>
    <p>In a sense,
    all algorithms which solve the left recursion problem do
    it in the same way.
    It is just that in some,
    the solution appears in a much simpler form.
    </p>
    <p>
    The solution is at most simple in Earley's algorithm.
    That is no coincidence -- as Pingali and Bilardi<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    show,
    Earley's, despite its daunting reputation,
    is actually the most basic Chomskyan context-free parsing algorithm,
    the one from which all others derive.
    </p>
    <p>Earley's builds a table.
    The Earley table contains an initial Earley set
    and an Earley set for each token.
    The Earley set for each token
    describes the state of the parse after consuming that token.
    The basic idea is not dissimilar
    to that of the Might/Darais/Spiewak (MDS) idea of parsing by derivatives,
    and the logic for building the Earley sets resembles
    that of MDS.<a id="footnote-3-ref" href="#footnote-3">[3]</a>
    <p>
    For the purpose of studying left recursion,
    what matters is that
    each Earley set contains Earley "items".
    Some of the items are called predictions
    because they predict the occurrence of a symbol
    at that location in the input.
    </p>
    To record a left recursion in an Earley set,
    the program adds
    a prediction item for the left recursive symbol.
    It is that simple.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </p>
    Multiple occurrences of a prediction item would be identical,
    and therefore useless.
    Therefore subsequent attempts
    to add the same prediction item are ignored,
    and recursion does not occur.
    </p>
    <h2>If some have no problem, why do others?</h2>
    <p>Besides Earley's,
    a number of other algorithms handle left recursion without
    any issue -- notably LALR 
    (aka <tt>yacc</tt> or <tt>bison</tt>) and LR.
    This re-raises the original question:
    why do some algorithms have a left recursion problem?
    </p>
    <p>The worst afflicted algorithms are the "top-down"
    parsers.
    The best known of these is recursive descent --
    a parsing methodology which, essentially, does parsing
    by calling a subroutine to handle each symbol.
    In the traditional implementation of recursive descent,
    left recursion is very problematic.
    Suppose that, as part of a recursive descent implementation,
    you are writing the function to parse the
    symbol <tt>&lt;A&gt;</tt>,
    which you are calling
    <tt>parse_A()</tt>.
    If your grammar has a rule
    <pre><tt>
    (9) A ::= A B
    </tt></pre>
    the first thing you need to do in a naive
    implementation of
    <tt>parse_A()</tt>
    is to call <tt>parse_A()</tt>.
    And <tt>parse_A()</tt> will
    then call <tt>parse_A()</tt>.
    And so, in the naive implementation, on and on forever.
    <h2>The fixed-point solution to left recursion</h2>
    <p>Over the years,
    many ways to solve the top-down left recursion issue have been
    announced.
    The MDS solution is one of the more interesting --
    interesting because it actually works<a id="footnote-5-ref" href="#footnote-5">[5]</a>,
    and because it describes all the others,
    including the Earley algorithm solution.
    MDS reduces the problem to
    the more general one of finding a "fixed point" of the recursion.
    </p>
    <p>In math, the "fixed point" of a function is an argument of
    the function which is equal to its value for that argument --
    that is, an <tt>x</tt> such that <tt>f(x)&nbsp;=&nbsp;x</tt>.
    MDS describes an algorithm which "solves" the left recursion
    for its fixed point.
    That "fixed point" can then be memoized.
    For example the value of <tt>parse_A</tt>
    can be the memoized "fixed point" value of
    <tt>&lt;A&gt;</tt>.
    </p>
    <p>The Earley solution of left recursion was, in fact, an optimized
    "fixed point".
    The computation of an Earley is the application of a set
    of rules for adding Earley items.
    This continues until no more Earley items can be added.
    In other words, the rules for building an Earley set
    are applied until they find
    their "fixed point".<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </p>
    <h2>Other top-down solutions</h2>
    <p>The MDS fixed point solution <b>does</b>
    the job,
    but as described in their paper it requires a functional
    programming language to implement,
    and it is expensive.
    In the worst case, the MDS approach is exponential,
    although they conjecture that it is linear for a large
    class of practical grammars.
    </p>
    <p>Top-down algorithms can take an "in-between strategy" --
    they can tackle those left recursions that are cheap to
    find, without computing the full "fixed point".
    Here a well-defined boundary is crucial:
    A programmer wants to know if their particular grammar will
    work,
    and whether small tweaks to their grammar will break it.
    </p>
    <p>
    Top-down can be seen as
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/12/topdown.html">
    a "guessing" strategy with hacks</a>.
    Hacks are always needed in top-down, because the input is at the bottom,
    not the top, and a useful top-down algorithm needs to look at the input.
    But the hacks can be as simple as lookahead,
    and lookahead can be implemented without seriously compromising
    the simplicity and flexibility of the original top-down approach.
    </p>
    <p>With detection and fixing of left-recursion,
    the "hack" part of the top-down strategy becomes very complicated.
    The attraction of top-down is its simplicity,
    and its resulting adapability to procedural logic.
    The point can be reached where the original strategy
    comes into question.
    </p>
    <p>
    After all,
    a recursive descent parser can straightforwardly take care of left recursion
    issues by calling an Earley parser.
    But in that case,
    why not simply use Earley's?
    </p>
    <h2>Comments, etc.</h2>
      Marpa is my own implementation of an Earley parser.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on its IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
    I probably could have said "all practical parsing methods"
    instead of "almost all".
    Right-to-left parsing methods exist,
    but they see little use.
    In any case, they only reverse the problem.
    Parsing in both directions is certainly possible but,
    as I will show,
    we do not have to go to quite that much trouble.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
        Keshav Pingali and Gianfranco Bilardi, UTCS tech report TR-2012.
        2012.
        <a href="https://apps.cs.utexas.edu/tech_reports/reports/tr/TR-2102.pdf">
          PDF accessed 9 Junk 2018</a>.
        <a href="https://www.youtube.com/watch?v=eeZ3URxd8Wc">
          Video accessed 9 June 2018</a>.
        Less accessible is
        Keshav Pingali and Gianfranco Bilardi,
        "A graphical model for context-free grammar parsing."
        Compiler Construction - 24th International Conference, CC 2015.
        Lecture Notes in Computer Science,
        Vol. 9031, pp. 3-27, Springer Verlag, 2015.
        <a href="https://www.researchgate.net/publication/286479583_A_Graphical_Model_for_Context-Free_Grammar_Parsing">
          PDF accessed 9 June 2018</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3"><b>3.</b>
        Matthew Might, David Darais and Daniel Spiewak.
	"Functional Pearl: Parsing with Derivatives."
	International Conference on Functional Programming 2011 (ICFP 2011).
	Tokyo, Japan. September, 2011. pages 189-195.
        <a href="http://matt.might.net/papers/might2011derivatives.pdf">
          PDF accessed 9 Jun 2018</a>.
        <a href="http://matt.might.net/papers/might2011derivatives-icfp-talk.pdf">
          Slides accessed 9 June 2018</a>.
        <a href="http://matt.might.net/media/mattmight-icfp2011-derivatives.mp4">
          Video accessed 9 June 2018</a>.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4"><b>4.</b>
    Those familiar with Earley's algorithm may note that
    Earley items are traditionally in terms of productions,
    not symbols.
    Symbol predictions are therefore recorded indirectly.
    <br><br>
    Specifically, Earley items are traditionally duples
    of <tt>(dr, origin)</tt>,
    where <tt>dr</tt> is a dotted production -- a production
    with a "dot location" marked;
    and <tt>origin</tt> is a location in the input.
    In all predictions <tt>origin</tt> is the current location,
    and the dot location is at the start of the production,
    so there can be at most one prediction per rule.
    A prediction of a symbol 
    <tt>&lt;A&gt;</tt>
    is recorded as a prediction of every
    production which has 
    <tt>&lt;A&gt;</tt> on its LHS.
    <br><br>
    The argument in the main text is made the way it is
    because it is simpler to speak
    of "predicted symbols" than
    to repeatedly refer to "sets of predictions
    of productions sharing a common LHS".
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5"><b>5.</b>
    There have been many more attempts than implementations
    over the years,
    and even some of the most-widely used
    implementations <a href="https://www.youtube.com/watch?v=lFBEf0o-4sY&feature=youtu.be&t=6m29s">
    have
    their issues.</a>
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6"><b>6.</b>
    Recall that potential left recursions are 
    recorded as "predictions" in Earley's algorithm.
    Predictions recurse,
    but since they do not depend on the input,
    they can be precomputed.
    This means that Earley implementations can
    bring each Earley set to its fixed point
    very quickly.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7"><b>7.</b>
        Marpa has a stable implementation.
        For it, and for more information on Marpa, there are these resources:<br>
        <a href="http://savage.net.au/Marpa.html">
          Marpa website, accessed 25 April 2018</a>.</br>
        <a href="https://jeffreykegler.github.io/Marpa-web-site/">
          Kegler's website, accessed 25 April 2018</a>.</br>
        <a href="https://github.com/jeffreykegler/Marpa--R2">
          Github repo, accessed 25 April 2018.</a></br>
        <a href="https://metacpan.org/pod/Marpa::R2">
          MetaCPAN, accessed 30 April 2018.</a>.</br>
	  There is also a theory paper for Marpa:
        Kegler, Jeffrey.
        "Marpa, A Practical General Parser: The Recognizer.", 2013.
        <a href="http://dinhe.net/~aredridel/.notmine/PDFs/Parsing/KEGLER,%20Jeffrey%20-%20Marpa,%20a%20practical%20general%20parser:%20the%20recognizer.pdf">
          PDF accessed 24 April 2018</a>.
 <a href="#footnote-7-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 09:15 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/lrecursion.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 18 Jun 2018</h3>
<br />
<center><a name="combinator"> <h2>Marpa and combinator parsing</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>The missing part</h2>
    <p>
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/csg.html">
    A previous post</a>
    described how to use the current stable Marpa
    implementation as a better procedural parser.
    This post describes how the Marpa algorithm can be used as the basis
    of better combinator parsers.
    </p>
    <p>In the post on procedural parsing,
    the subparsers<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    were like combinators,
    in that they could be called recursively,
    so that a parse could be built up from components.
    Like combinators,
    each child could return,
    not just a parse,
    but a set of parses.
    And, as in combinators, once a child combinator
    returned its value,
    the parent parser could resume parsing
    at a location specified by the child combinator.
    So what was missing?
    <p>A combinator,
    in order to handle ambiguity,
    returns not a subparse, but a set of subparses.
    In the full combinator model,
    each subparse can have its own "resume location".<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    The procedural parsing post did not provide for multiple
    resume locations.
    We will now proceed to make up for that.
    </p>
    <h2>How it works</h2>
    <p>The Marpa parser has the ability to accept
    multiple subparses,
    each with its own length.
    This allows child subparses to overlap in any fashion,
    forming a mosaic as complex as the application needs.
    </p>
    </p>An Earley parser is table-driven --
    its parse tables consists of Earley sets,
    with an initial Earley set
    and one Earley set per token.
    This makes for a very simple idea of location.
    Location 0 is the location of the initial Earley set.
    Location <tt>N</tt> is the location of the Earley set after the <tt>N</tt>'th
    token has been consumed.
    </p>
    <p>Simplicity is great, but unfortunately
    this won't work for variable-length
    tokens.
    To handle those, Marpa introduces another idea of location:
    the <b>earleme</b>.
    Like Earley set locations,
    the earlemes begin at 0,
    and advance in integer sequence.
    Earley set 0 is always at earleme 0.
    Every Earley set has an earleme location.
    On the other hand,
    not every earleme has a corresponding Earley set --
    there can be "empty" earlemes.
    </p>
    <p>The lower-level interface for Marpa is Libmarpa.
    Every time Libmarpa adds a token,
    a length in earlemes must be specified.
    In the most-used higher level Marpa interfaces,
    this "earleme length" is always 1,
    which makes the Libmarpa location model collapse into the traditional one.
    </p>
    <p>
    The Libmarpa recognizer advances earleme-by-earleme.
    In the most-used higher level Marpa interfaces,
    a token ends at every earleme
    (unless of course that earleme is after end-of-input).
    This means that the most-used Marpa interfaces
    create a new Earley set every time they advance one earleme.
    Again, in this case, the Libmarpa model collapses into
    the traditional one.
    </p>
    <p>In Libmarpa and other lower-level interfaces,
    there may be cases where
    <ul>
    <li>one or more tokens
    end after the current earleme, but</li>
    <li>no tokens end <b>at</b> the current earleme.</li>
    </ul>
    In such cases the current earleme will be empty.
    </p>
    <p>This is only an outline of the basic concepts behind the
    Marpa input model.
    The formalisms are in the Marpa theory paper.<a id="footnote-3-ref" href="#footnote-3">[3]</a>
    The documentation for Libmarpa and Marpa's other low-level interfaces contains
    more accessible,
    but detailed, descriptions.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </p>
    <h2>Value added</h2>
    <h3>Left-eidetic information</h3>
    <p>As readers of my previous posts<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    will know,
    Marpa is "left-eidetic" -- the application has access to everything to its left.
    This is an advantage over the traditional implementation of combinator parsing,
    where parse information about the left context may be difficult
    or impossible to access.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </p>
    <h3>More powerful linear-time combinators</h3>
    <p>Marpa parses a superset of LR-regular grammars in linear time,
    which makes it a more powerful "building block"
    than traditionally available for combinator parsing.
    This gives the programmer of a combinator parser more options.
    </p>
    <h3>State of the art worse-than-linear combinators</h3>
    <p>In special circumstances, programmers may want to use subparsers 
    which are worse than linear -- for example, they may know that
    the string is very short.
    Marpa parses context-free grammars in state of the art time.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </p>
    <h2>The code, comments, etc.</h2>
      To learn more about Marpa,
      a good first stop is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
    In some of the descriptions of Marpa's procedural
    parsing, these subparsers are called "lexers".
    This emphasizes the usual case in current practice,
    where the subparsers are the bottom layer of the
    parsing application,
    and do not invoke their own child subparsers.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
    In notational terms, a full combinator is a function of the form
    <br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <tt>A* &#8594; &#8473;( P &#215; A* )</tt>,
    <br>
    where <tt>A</tt> is the alphabet of the grammar;
    <tt>P</tt> is a representation of a single parser
    (for example, a parse tree);
    <tt>&#8473;(X)</tt> is the power set of a set <tt>X</tt>:
    and
    <tt>X &#215; Y</tt> is the Cartesian product
    of sets <tt>X</tt> and <tt>Y</tt>.
    The subparsers
    of the procedural parsing post
    were of the form
    <br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    <tt>A* &#8594; &#8473;( P ) &#215; A*</tt>.
    <br>
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3"><b>3.</b>
    Kegler, Jeffrey.<a
    href="http://dinhe.net/~aredridel/.notmine/PDFs/Parsing/KEGLER,%20Jeffrey%20-%20Marpa,%20a%20practical%20general%20parser:%20the%20recognizer.pdf">
    "Marpa, a Practical General Parser: The Recognizer".</a>
    2013.
    Section 12, "The Marpa input model", pp. 39-40.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4"><b>4.</b>
    Libmarpa API document,
    <a href="http://jeffreykegler.github.io/Marpa-web-site/libmarpa_api/stable/api_one_page.html#Input">
    the "Input" section</a>.
    Marpa::R2's NAIF interface allows
    access to the full Libmarpa input model
    and its documentation contains
    <a href="https://metacpan.org/pod/distribution/Marpa-R2/pod/Advanced/Models.pod">
    a higher-level description of Marpa's alternative input models.</a>
    There is also a thin Perl interface to Libmarpa,
    <a href="http://jeffreykegler.github.io/Marpa-web-site/libmarpa_api/stable/api_one_page.html#Input">the THIF interface</a>,
    which allows full access to the alternative input models.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5"><b>5.</b>
    For example, the
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2018/05/csg.html">
    post on procedural parsing</a> contains a good,
    simple, example of the use of Marpa's left-eideticism.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6"><b>6.</b>
    For best effect,
    left-eidetism and functional purity
    probably should be used in combination.
    For the moment at least,
    I am focusing on explaining the capabilities,
    and leaving it to others to find the monadic
    or other solutions that will allow programmers to leverage
    this power in functionally pure ways.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7"><b>7.</b>
    Specifically O(n^2) for unambiguous grammars,
    and O(n^3) for ambiguous grammars.
 <a href="#footnote-7-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 08:29 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/combinator.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sun, 17 Jun 2018</h3>
<br />
<center><a name="csg"> <h2>Marpa and procedural parsing</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <h2>Procedural parsing</h2>
    <p>Marpa is an Earley-based parser,
      and Earley parsers are typically not good at procedural parsing.
      Many programmers are used to recursive descent (RD),
      which has been state-of-the-art in terms of
      its procedural programming capabilities --
      it was these capabilities which led to
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/fast_power.html">
      RD's
      triumph over the now-forgotten Irons algorithm.</a>
    </p>
    <p>
      Marpa, however, has a parse engine expressly redesigned<a id="footnote-1-ref" href="#footnote-1">[1]</a>
      to handle procedural logic well.
      In fact, Marpa is <b>better</b> at procedural logic
      than RD.
    </p>
    <h2>A context-sensitive grammar</h2>
    <p>Marpa parses all LR-regular grammars in linear time,
    so the first challenge is to find a grammar
    that illustrates a
    <b>need</b> for procedural logic, even when Marpa is used.
    The following is the canonical example of a grammar that is
    context-sensitive, but not context-free:
    </p>
    <pre>
          a^n . b^n . c^n : n >= 1
    </pre>
    I will call this the "ABC grammar".
    It is a sequence of
    <tt>a</tt>'s,
    <tt>b</tt>'s, and
    <tt>c</tt>'s,
    in alphabetical order,
    where the character counts are all
    equal to each other and greater
    than zero.
    </p>
    <p>The ABC "grammar" is really a counting problem more than
    a natural parsing problem,
    and parsing is not the fastest or easiest way to solve it.
    Three tight loops, with counters, would do the same job nicely,
    and would be much faster.
    But I chose the ABC grammar for exactly this reason.
    It <b>is</b> simple in itself,
    but it is tricky when treated as a parsing problem.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    </p>
    <p>
    In picking the strategy below,
    I opted for one that illustrates
    a nice subset of Marpa's procedural parsing capabilities.
    Full code is
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/gh-pages/code/csg">
    on-line<a>,
    and readers are encouraged to "peek ahead".
    </p>
    <h2>Step 1: the syntax</h2>
    <p>Our strategy will be to start with a context-free syntax,
    and then extend it with procedural logic.
    Here is the context-free grammar:
    </p>
    <pre><tt>
    lexeme default = latm => 1
    :default ::= action => [name,start,length,values]
    S ::= prefix ABC trailer
    ABC ::= ABs Cs
    ABs ::= A ABs B | A B
    prefix ::= A*
    trailer ::= C_extra*
    A ~ 'a'
    B ~ 'b'
    :lexeme ~ Cs pause => before event => 'before C'
    Cs ~ 'c' # dummy -- procedural logic reads <Cs>
    C_extra ~ 'c'
    </tt></pre>
    <p>The first line is boiler-plate:
    It turns off a default which was made pointless
    by a later enhancement to Marpa::R2.
    Marpa::R2 is stable, and backward-compatibility is
    a very high priority.
    <pre><tt>
    :default ::= action => [name,start,length,values]
    </tt></pre>
    <p>
    We will produce a parse tree.
    The second line defines its format --
    each node is an array whose elements are,
    in order,
    the node name, its start position,
    its length and its child nodes.
    </p>
    <pre><tt>
    S ::= prefix ABC trailer
    </tt></pre>
    <p>The symbol <tt>&lt;ABC&gt;</tt>
    is our "target" -- the counted
    <tt>a</tt>'s,
    <tt>b</tt>'s,
    and <tt>c</tt>'s.
    To make things a bit more interesting,
    and to make the problem more like a parsing problem instead of a counting problem,
    we allow a prefix of <tt>a</tt>'s
    and a trailer of <tt>c</tt>'s.
    </p>
    <pre><tt>
    ABC ::= ABs Cs
    </tt></pre>
    <p>We divide the
    <tt>&lt;ABC&gt;</tt> target into two parts:
    <tt>&lt;ABs&gt;</tt>, which contains the
    <tt>a</tt>'s,
    and <tt>b</tt>'s;
    and
    <tt>&lt;Cs&gt;</tt>, which contains
    the <tt>c</tt>'s.
    </p>
    <p>
    The string
    </p>
    <pre><tt>
    a^n . b^n
    </tt></pre>
    <p>
    is context free, so that we can handle it
    without procedural logic, as follows:
    </p>
    <pre><tt>
    ABs ::= A ABs B | A B
    </tt></pre>
    <p>
    The line above recognizes a non-empty string of
    <tt>a</tt>'s,
    followed by an equal number
    of <tt>b</tt>'s.
    </p>
    <pre><tt>
    prefix ::= A*
    trailer ::= C_extra*
    </tt></pre>
    <p>As stated above,
    <tt>&lt;prefix&gt;</tt>
    is a series of <tt>a</tt>'s and
    <tt>&lt;trailer&gt;</tt>
    is a series of <tt>c</tt>'s.
    </p>
    <pre><tt>
    A ~ 'a'
    B ~ 'b'
    </tt></pre>
    <p>Marpa::R2 has a separate lexical and syntactic phase.
    Here we define our lexemes.
    The first two are simple enough:
    <tt>&lt;A&gt;</tt> is the character "<tt>a</tt>"; and
    <tt>&lt;B&gt;</tt> is the character "<tt>b</tt>".
    </p>
    <pre><tt>
    :lexeme ~ Cs pause => before event => 'before C'
    Cs ~ 'c' # dummy -- procedural logic reads <Cs>
    C_extra ~ 'c'
    </tt></pre>
    <p>
    For the character "<tt>c</tt>",
    we need procedural logic.
    As hooks for procedural logic,
    Marpa allows a full range of events.
    Events can occur on prediction and completion of symbols;
    when symbols are nulled;
    before lexemes;
    and after lexemes.
    The first line in the above display
    declares a "before lexeme" event
    on the symbol
    <tt>&lt;Cs&gt;</tt>.
    The name of the event is "<tt>before C</tt>".
    </p>
    <p>The second line is a dummy entry,
    which is needed to allow the "<tt>before C</tt>"
    event to trigger.
    The entry says that
    <tt>&lt;Cs&gt;</tt> is a single character "<tt>c</tt>".
    This is false --
    <tt>&lt;Cs&gt;</tt> is a series of one or more
    <tt>c</tt>'s,
    which needs to be counted.
    But when
    the "<tt>before C</tt>" event triggers,
    the procedural
    logic will make things right.
    </p>
    <p>The third line defines
    <tt>&lt;C_extra&gt;</tt>, which
    is another lexeme for the character "<tt>c</tt>".
    We have two different lexemes for
    the character <tt>c</tt>, because we want some
    <tt>c</tt>'s (those in the target)
    to trigger events;
    and we want other
    <tt>c</tt>'s (those in the trailer)
    not to trigger events,
    but to be consumed by Marpa directly.
    </p>
    <h2>The procedural logic</h2>
    <p>
    At this point, we have solved part of the problem with context-free syntax,
    and set up a Marpa event named "<tt>before C</tt>",
    which will solve the rest of it.
    </p>
    <pre><tt>
    my $input_length = length ${$input};
    for (
        my $pos = $recce->read($input);
        $pos < $input_length;
        $pos = $recce->resume()
      )
    {
      </tt><b>... Process events ...</b><tt>
    }
    </tt></pre>
    <p>Processing of events takes place inside a Marpa read loop.
    This is initialized with a <tt>read()</tt> method,
    and is continued with a <tt>resume()</tt> method.
    The <tt>read()</tt> and <tt>resume()</tt> methods
    both return the current position
    in the input.
    If the current position is end-of-input, we are done.
    If not, we were interrupted by an event, which we
    must process.
    </p>
    <pre><tt>
    </tt><b>Process events</b><tt>

    EVENT:
      for (
	  my $event_ix = 0 ;
	  my $event    = $recce->event($event_ix) ;
	  $event_ix++
	)
      {
	  my $name = $event->[0];
	  if ( $name eq 'before C' ) {
	      </tt><b>... Process "before C" event ...</b><tt>
	  }
	  die qq{Unexpected event: name="$name"};
      }
    </tt></pre>
    <p>In this application, only one event can occur at any location,
    so the above loop is "overkill".
    It loops through the events, one by one.
    The <tt>event</tt> method returns a reference to an array
    of event data.
    The only element we care about is the event name.
    In fact, if we weren't being careful about error checking,
    we would not even care about the event name,
    since there can be only one.
    </p>
    <p>If, as expected, the event name is "<tt>before C</tt>",
    we process it.
    In any other case, we die with an error message.
    </p>
    <pre><tt>
    </tt><b>Process "before C" event</b><tt>

    my ( $start, $length ) = $recce->last_completed_span('ABs');
    my $c_length = ($length) / 2;
    my $c_seq = ( 'c' x $c_length );
    if ( substr( ${$input}, $pos, $c_length ) eq $c_seq ) {
	$recce->lexeme_read( 'Cs', $pos, $c_length, $c_seq );
	next EVENT;
    }
    die qq{Too few C's};
    </tt></pre>
    <p>This is the core part of our procedural logic,
    where we have a "<tt>before C</tt>" event.
    We must
    <ul>
    <li>determine the right number of <tt>c</tt> characters;</li>
    <li>check that the input has
      the right number of <tt>c</tt> characters;</li>
    <li>put together a lexeme to feed the Marpa parser; and</li>
    <li>return control to Marpa.</li>
    </ul>
    There is a lot going on,
    and some of Marpa's most powerful capabilities for assisting
    procedural logic are shown here.
    So we will go through the above display in detail.
    </p>
    <h3>Left-eidetic</h3>
    <pre><tt>
    my ( $start, $length ) = $recce->last_completed_span('ABs');
    my $c_length = ($length) / 2;
    </tt></pre>
    <p>Marpa claims to be "left-eidetic",
    that is, to have full knowledge of the parse so far,
    and to make this knowledge available to the programmer.
    How does a programmer cash in on this promise?
    <p>Of course, there is
    <a href="https://metacpan.org/pod/distribution/Marpa-R2/pod/Progress.pod">a fully general interface</a>,
    which allows you to go through the Earley tables and extract
    the information in any form necessary.
    But another, more convenient interface,
    fits our purposes here.
    Specifically,
    </p>
    <ul><li>we want to determine how many <tt>c</tt> characters we are looking for.</li>
    <li>How many <tt>c</tt> characters we are looking for depends
    on the number of
    <tt>a</tt> and <tt>b</tt> characters that we have already seen
    in the target.</li>
    <li>The <tt>a</tt> and <tt>b</tt> characters that we have already seen in the
    target are in the
    <tt>&lt;ABs&gt;</tt> symbol instance.</li>
    <li>So, what we want to know is the length of the
    most recent <tt>&lt;ABs&gt;</tt> symbol instance.</li>
    </ul>
    </p>
    <p>Marpa has a <tt>last_completed_span()</tt> method,
    and that is just what we need.
    This finds the most recent instance of a symbol.
    (If there had been more than one most recent instance,
    it would have found the longest.)
    The <tt>last_completed_span()</tt> method returns the start
    of the symbol instance (which we do not care about)
    and its length.
    The desired number of <tt>c</tt> characters,
    <tt>$c_length</tt>, is half the length of the
    <tt>&lt;ABs&gt;</tt> instance.
    </p>
    <h3>External parsing</h3>
    <pre><tt>
    my $c_seq = ( 'c' x $c_length );
    if ( substr( ${$input}, $pos, $c_length ) eq $c_seq ) { </tt><b>...</b><tt> }
    </tt></pre>
    <p>Marpa allows external parsing.
    You can pause Marpa, as we have done,
    and hand control over to another parser -- including
    another instance of Marpa.
    </p>
    <p>
    Here external parsing is necessary to make our parser
    context-sensitive,
    but the external parser does not have to be fancy.
    All it needs to do is
    some counting -- not hard,
    but something that a context-free grammar cannot do.
    </p>
    <p>
    <tt>$pos</tt> is the current position in the input,
    as returned by the <tt>read()</tt> or <tt>resume()</tt>
    method in the outer loop.
    Our input is the string referred to by <tt>$input</tt>.
    We just calculated <tt>$c_length</tt> as the number of
    <tt>c</tt> characters required.
    The above code checks to see that the required number of
    <tt>c</tt> characters is at <tt>$pos</tt> in the input.
    </p>
    <h3>Communicating with Marpa</h3>
    <pre><tt>
	$recce->lexeme_read( 'Cs', $pos, $c_length, $c_seq );
    </tt></pre>
    <p>
    Our external logic is doing the parsing,
    but we need to let Marpa know what we are finding.
    We do this with the <tt>lexeme_read()</tt> method.
    <tt>lexeme_read()</tt> needs to know what symbol we are reading
    (<tt>Cs</tt> in our case);
    and its value
    (<tt>$c_seq</tt> in our case).
    </p>
    <p>
    Marpa requires that
    every symbol be tied in some way to the input.
    The tie-in is only for error reporting,
    and it can be hack-ish or completely artificial,
    if necessary.
    In this application, our symbol instance is tied into
    the input in a very natural way --
    it is the stretch of the input that we compared
    to <tt>$c_seq</tt> in the display before last.
    We therefore tell Marpa
    that the symbol is at <tt>$pos</tt> in the input,
    and of length <tt>$c_length</tt>.
    </p>
    <h3>Passing control back to Marpa</h3>
    <pre><tt>
	next EVENT;
    </tt></pre>
    <p>
    External parsing can go on quite a long time.
    In fact, an external parser <b>never</b> has to hand
    control back to Marpa.
    But in this case, we are done very quickly.
    </p>
    <p>
    We ask for the next iteration of the <tt>EVENT</tt>
    loop.
    (In this code,
    there will not be a next iteration, unless there is an error.)
    Once done, the <tt>EVENT</tt> loop will hand control
    over to the outer loop.
    The outer loop will call the <tt>resume()</tt>
    method to return control back to Marpa.
    </p>
    <h2>The code, comments, etc.</h2>
    <p>The full code for this example is 
    <a href="https://github.com/jeffreykegler/Ocean-of-Awareness-blog/tree/gh-pages/code/csg">
    on-line<a>.
      There is a lot more to Marpa, including
      more facilities for adding procedural logic to your Marpa parsers.
      To learn more about Marpa,
      a good first stop is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1"><b>1.</b>
      To handle procedural logic well,
      an Earley engine needs to complete its Earley sets
      in strict order --
      that is, Earley set <tt>N</tt>
      cannot change after work on Earley set <tt>N+1</tt>
      has begun.
      I have not looked at every Earley parse engine,
      and some may have had this strict-sequencing property.
      And many of the papers are agnostic about the order
      of operations.
      But Marpa is the first Earley parser to recognize
      and exploit strict-sequencing as a feature.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2"><b>2.</b>
    The ABC grammar, in fact,
    is not all that easy or natural to describe
    even with a context-sensitive phrase structure description.
    A solution is given on Wikipedia:
    <a href="https://en.wikipedia.org/wiki/Context-sensitive_grammar#Examples">
    https://en.wikipedia.org/wiki/Context-sensitive_grammar#Examples</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 20:02 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/csg.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
