<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Mon, 02 Mar 2015</h3>
<br />
<center><a name="peg"> <h2>PEG: Ambiguity, precision and plain old confusion</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <h3>ambiguity Hoffa and precision</h3>
    <p><a href="http://bford.info/packrat/">>PEG parsing</a>
      is a new notation
      for a notorously tricky algorithm with a history that goes back
      the days of the earliest computers.
      In its PEG form,
      this algorithm acquired an seductive new interface,
      one that looks like the best of
      extended BNF combined with the best of regular expressions.
      Looking at a sample of it gives the impression
      that writing a parser has suddenly become a very straightforward
      matter.
    </p><p>For those not yet in the know on this,
      I'll explain with a pair of examples from
      <a href="http://www.romanredz.se/papers/FI2008.pdf">
        an excellent 2008 paper by Redziejowski</a>.
      Let's start with these two PEG grammars.
    </p><blockquote><pre>
    ("a"|"aa")"a"
    ("aa"|"a")"a"
    </pre></blockquote><p>
      One of these two PEG grammars accepts
      the string "<tt>aaa</tt>" and "<tt>aa</tt>".
      The other accepts the string
      the string "<tt>aa</tt>" and "<tt>aaa</tt>".
      Can you tell which one?
      Could you sort this out
      if this logic was buried in a large specification?
      (For the answer to the first question,
      see page 4 of
      <a href="http://www.romanredz.se/papers/FI2008.pdf">
        Redziejowski 2008</a>.
    </p><p>
      Here is another example:
    </p><blockquote><pre>
    A = "a"A"a"/"aa"
    </pre></blockquote><p>
      What language does this describe?
      It's obviously string of the letter "<tt>a</tt>",
      but which ones?
      Again the answer is on
      page 4 of
      <a href="http://www.romanredz.se/papers/FI2008.pdf">
        Redziejowski 2008</a>
      -- it's exactly those strings
      whose length is a power of 2.
    </p><p>With PEG, what you see is not what you get.
      PEG parsing is sometimes called "precise",
      on the idea that PEG parsing is in a sense unambiguous.
      In this case "precise" is taken as synonymous with
      "unique".
      That is, PEG parsing is precise in exactly the same
      sense as the location of Jimmy Hoffa's body
      is a precise location.
      We know there is exactly one such place, but
      without necessarily being able to specify it.
    </p>
    <h3>Syntax-driven?</h3>
    <p>It's important to emphasize that the point of
      a syntax-driven parser generator is that the syntax,
      in fact, describe, in some way the programmer can
      in practice predict, the parser which is generated.
      This is, for most practical grammars, not the case
      with PEG.
      Below I will describe the work of several
      PEG researchers who understand this this issue,
      and who are trying to deal with it.
    </p><p>Before moving on,
      I want to point out why these PEG researchers
      take knowing precisely what language you are parsing
      very seriously.
      This is much more at stake than bragging rights
      over which algorithm is really syntax-driven and
      which is not.
    </p><p>Let me identify two problems.
      The least serious of these is that your PEG grammar
      might not parse all the strings in your language.
      There is, in usual practice, no way to determine
      this from the PEG specification,
      so you have to rely on your test suite.
      But test suites are not adequate to catch all
      possible errors in most applications,
      and this is especially the case with languages.
    </p><p>The second, more serious problem, is often forgotten.
      Your PEG parser might accept strings that are
      <b>not</b>
      in your language.
      No test suite can catch all of these, and few try.
      But if a parser accepts unexpected inputs,
      and applications come to rely on this,
      it becomes a huge problem for maintaining compatibility.
      Failure to accept correct input can be coped with
      in straightforward fashion,
      -- you simply fix the parser to accept the correct input.
      But if you discover you have been accepting incorrect input,
      you have a more difficult choice -- break compatiblity,
      or leave the problem unfixed.
      Leaving the problem unfixed is problematic, because
      acceptance of unexpected inputs is also
      a potential security risk.
    </p><p>The burden of maintaining a PEG parser is worth
      considering carefully.
      Even if you do know the current behavior of your PEG parser,
      you cannot expect to know what it will be if you need to
      change it.
      Small changes in PEG specifications often have big
      implications,
      implications which are hard to predict,
      and hard even to discover when your revised
      PEG parser seems to be working.
      And the larger the PEG parser gets, the worse the
      maintenance problem gets.
    </p><h3>Is PEG unambiguous?</h3>
    <p>PEG is not unambiguous in any
      helpful sense of that word.
      BNF allows you to specify ambiguous grammars,
      and that feature is tied to its power and flexibility
      and often useful in itself.
      PEG will only deliver one of those parses, but without
      an easy way of specifying, or even knowing, which one
      the underlying problem of ambiguity has not really been addressed.
    </p><p>My Marpa parser,
      for example, which is a general BNF parser based on Earley's,
      can simply throw all but one of the parses in an ambiguous
      parse away.
      But I would not say that, if you have an issue with ambiguity,
      Marpa has solved it by arbitrarily throwing away results.
      Marpa, in fact, does allow you to control which parses
      are tossed, by ranking some rules over others,
      and in Marpa's case the ranking works in an intuitive
      way.
      But in all cases, whether you call them ambiguous or not,
      Marpa delivers precisely the parses the grammar and the rule
      rankings specify,
      and it is "precision" in this sense that a grammar writer needs.
    </p><h3>Is there a sensible way to use PEG?</h3><p>
      I've already mentioned my own parser, Marpa,
      and I'll return to that.
      But suppose that you refuse to use Marpa,
      and insist on making
      the best of PEG.
      There are
      several excellent programmers who have committed themselves
      to PEG, while being fully honest about its limitations.
      I've already mentioned one important paper
      Redziejowski -- his collected papers can be
      found here, and many of them treat PEG,
      and they are trustworthy.
      Redziejowski, in his effort to promote PEG,
      can be relied not to sugarcoat PEG's problems.
    </p><p>Roberto Ierusalimschy, author of Lua and one of the best
      programmers of our time,
      has written a PEG-based parser of his own.
      Roberto's is fully aware of PEG's limits,
      and makes a good case for choosing PEG for his parser.
      First, LPEG's is intended for use Lua,
      a ruthlessly minimal language,
      so there is an almost unique emphasis on the size of the code.
      And, indeed, Roberto's version of PEG (LPEG) is small.
      Second, Roberto's intention
      is not to create a syntax-driven competitor of recursive descent,
      but instead to extend regular expressions in a disciplined way.
    </p><h3>Matching the BNF to the PEG spec</h3><p>
      Redziejowski, Ierusalimschy and the other authors
      of
      <a href="x">X</a>
      recognize that inability to know the the language you are
      parsing is no minor issue -- in many applications,
      it is a show-stopper, or at least should be.
      The solution they are pursuing is finding the subset of
      BNF for which the PEG spec
      <b>does</b>
      accept exactly
      the language specified by the BNF.
      This subset is in fact, very restricted -- it is essentially
      LL(1) --
      those languages a top-down parser can parse based on at
      most one character of input.
      A syntax-driven parser for LL(1) was described in 1961(?),
      but there was no followup,
      because LL(1) is just not sufficient for
      most practical languages.
      Redziejowski believes this class of languages which are well-behaved
      for PEG can be extended somewhat --
      those interested can look at his paper.
    </p><h3>Marpa</h3><p>
      In this post,
      I have taken the point of view of programmers using PEG,
      or thinking of doing so.
      My own belief in this matter is that
      very few programmers
      will want to deal with the issues I've just described.
      My reason for this is the Marpa parser --
      a general BNF Earley-drived parser that
    </p>
    <ul>
      <li>has an implementation you can use today;</li>
      <li>allows the application to combine syntax-driven parsing
        with custom procedural logic;</li>
      <li>makes available full, left-eidetic knowledge of the parse to
        the procedural logic;</li>
      <li>and parses a vast class of grammars the linear time,
        including all the LR-regular grammars.</li>
    </ul>
    <p>
      The LR-regular grammars include the LR(k) and LL(k)
      grammars for all
      <i>k</i>, and which can be expected to include any
      grammar that will be found to be well-behaved under PEG.
    </p>
    <h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href="http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 19:56 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2015/03/peg.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sat, 15 Nov 2014</h3>
<br />
<center><a name="ll"> <h2>Parsing: Top-down versus bottom-up</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p><p>Comparisons between top-down and bottom-up parsing
      are often either too high-level or too low-level.
      Overly high-level treatments reduce the two approaches to buzzwords,
      and the comparison to a recitation of received wisdom.
      Overly low-level treatments get immersed in the minutiae of implementation,
      and the resulting comparison is as revealing as placing
      two abstractly related code listings side by side.
      In this post I hope to find the middle level;
      to shed light on why advocates of bottom-up
      and top-down parsing approaches take the positions
      they do;
      and to speculate about the way forward.
    </p>
    <h3>Top-down parsing</h3>
    <p>The basic idea of top-down parsing is
      as brutally simple as anything in programming:
      Starting at the top, we add pieces.
      We do this by looking at the next token and deciding then and there
      where it fits into the parse tree.
      Once we've looked at every token,
      we have our parse tree.
    </p><p>
      In its purest form,
      this idea is too simple for practical parsing,
      so top-down parsing is almost
      always combined with lookahead.
      Lookahead of one token helps a lot.
      Longer lookaheads
      are very sparsely used.
      They just aren't that helpful,
      and since
      the number of possible lookaheads grows exponentially,
      they get very expensive very fast.
    </p><p>Top-down parsing has an issue with left recursion.
      It's straightforward to see why.
      Take
      an open-ended expression like
    </p><blockquote><pre>
    a + b + c + d + e + f + [....]</pre></blockquote>
    <p>
      Here the plus signs continue off to the right,
      and adding any of them to the parse tree
      requires a dedicated node which
      must be above the node for the first plus sign.
      We cannot put that first plus sign into a top-down parse
      tree without having first dealt with all those plus signs that follow it.
      For a top-down strategy, this is a big, big problem.
    </p><p>
      Even in the simplest expression,
      there is no way of counting the plus signs
      without looking to the right,
      quite possibly a very long way to the right.
      When we are not dealing with simple expressions,
      this rightward-looking needs to get
      sophisticated.
      There are ways of dealing with this difficulty,
      but all of them share one thing in common --
      they are trying to make top-down parsing into
      something that it is not.
    </p><h3>Advantages of top-down parsing</h3>
    <p>Top-down parsing does not look at the right context in any systematic way,
      and in the 1970's it was hard to believe that
      top-down was as good as we can do.
      (It's not all that easy to believe today.)
      But its extreme simplicity
      is also top-down parsing's great strength.
      Because a top-down parser is extremely simple,
      it is very easy to figure out what it is doing.
      And easy to figure out means easy to customize.
    </p><p>
      Take another of the many constructs incomprehensible to
      a top-down parser:
    </p><blockquote><pre>
    2 * 3 * 4 + 5 * 6
    </pre></blockquote><p>
      How do top-down parsers typically handle this?
      Simple: as soon as they realize they are faced
      with an expression, they give up on top-down
      parsing and switch to a special-purpose algorithm.
    </p><p>These two properties -- easy to understand
      and easy to customize --
      have catapulted top-down parsing
      to the top of the heap.
      Behind their different presentations,
      combinator parsing, PEG, and recursive descent are
      all top-down parsers.
    </p><h3>Bottom-up parsing</h3>
    <p>Few theoreticians of the 1970's imagined that top-down parsing might
      be the end of the parsing story.
      Looking to the right in ad hoc ways clearly does help.
      It would be almost paradoxical if
      there was no systematic way to exploit the right context.
    </p><p>In 1965, Don Knuth found an algorithm to exploit
      right context.
      Knuth's LR algorithm was,
      like top-down parsing as I have described it,
      deterministic.
      Determinism was thought to be essential --
      allowing more than one choice easily leads to
      a combinatorial explosion in the
      number of possibilities that have to be considered at once.
      When parsers are restricted to dealing with a single choice,
      it is much easier to guarantee that
      they will run in linear time.
    </p>
    <p>Knuth's algorithm did <b>not</b>
      try to hang
      each token from a branch of a top-down parse tree
      as soon as it was encountered.
      Instead, Knuth suggested delaying that decision.
      Knuth's algorithm collected
      "subparses".
      <p>
      When I say "subparses" in this discussion,
      I mean pieces of the parse that 
      contain all the decisions necessary to construct
      the part of the parse tree that is below them.
      But subparses do not contain any decisions about what is above them
      in the parse tree.
      Put another way, subparses know who they are,
      but not where they belong.
    <p></p>
      Subparses may not know where they belong,
      but knowing who they are is enough for them
      to be assembled into larger subparses.
      And, if we keep assembling the subparses,
      eventually we will have a "subparse" that
      is the full parse tree.
      And at that point we will
      know both who everyone is 
      and where everyone belongs.
    </p><p>
      Knuth's algorithm stored subparses by shifting them onto a stack.
      The operation to do this was called a "shift".
      (Single tokens of the input are treated as subparses with a single node.)
      When there was enough context to build a larger subparse,
      the algorithm popped one or more subparses off the stack,
      assembled a larger subparse,
      and put the resulting subparse back on the stack.
      This operation was called a "reduce",
      based on the idea that its repeated application
      eventually "reduces" the parse tree to its root node.
    </p><p>
      In handling the stack, we will often be faced with
      choices.
      One kind of choice is between using what we already have
      on top of the stack to assemble a larger subparse;
      or pushing more subparses on top of the stack instead ("shift/reduce").
      When we decide to reduce,
      we may encounter the other kind of choice --
      we have to decide which rule to use ("reduce/reduce").
    </p>
    <p>Like top-down parsing, bottom-up parsing is usually combined with lookahead.
      For the same lookahead, a bottom-up parser parses everything that a
      top-down parser can handle,
      and more.
    </p><p>Formally, Knuth's approach is now called shift/reduce parsing.
      I want to demonstrate why theoreticians,
      and for a long time almost everybody else as well,
      was so taken with this method.
      I'll describe how it works on some examples,
      including two very important ones that
      stump top-down parsers: arithmetic expressions and left-recursion.
      My purpose here is bring to light the basic concepts,
      and not to guide an implementor.
      There are excellent implementation-oriented presentations in many other places.
      <a href=http://en.wikipedia.org/wiki/Shift-reduce_parser>The Wikipedia article</a>,
      for example, is excellent.
    </p>
    <p>
      Bottom-up parsing solved
      the problem of left recursion.
      In the example from above,
    </p><blockquote><pre>
    a + b + c + d + e + f + [....]</pre></blockquote>
    <p>
      we simply build one subparse after another,
      as rapidly as we can.
      In the terminology of shift/reduce,
      whenever we can reduce, we do.
      Eventually we will have run out of tokens,
      and will have reduced until there is only one element on the stack.
      That one remaining element is the subparse that is also,
      in fact, our full parse tree.
    </p><p>
      The top-down parser had a problem with left recursion
      precisely because it needed to build top-down.
      To build top-down, it needed to know about all the plus signs to come,
      because these needed to be fitted into the parse tree above the current plus
      sign.
      But when building bottom-up,
      we don't need to know anything about
      the plus signs that will be above the current one in the parse tree.
      We can afford to wait until we encounter them.
    </p>
    <p>But if working bottom-up solves the left recursion problem,
      doesn't it create a right recursion problem?
      In fact,
      for a bottom-up parser, right recursion is harder, but not much.
      That's because of the stack.
      For a right recursion like this:
    </p><blockquote><pre>
    a = b = c = d = e = f = [....]</pre></blockquote>
    <p>
      we use a strategy opposite to the one we used for the
      left recursion.
      For left recursion, we reduced whenever we could.
      For right recursion, when we have a choice, we always shift.
      This means we will immediately shift the entire input onto the stack.
      Once the entire input is on the stack,
      we have no choice but to start reducing.
      Eventually we will reduce the stack to a single element.
      At that point, we are done.
      Essentially, what we are doing is exactly what we did for left recursion,
      except that we use the stack to reverse the order.
    </p><p>
      Arithmetic expressions like
    </p><blockquote><pre>
    2 * 3 * 4 + 5 * 6</pre></blockquote>
    <p>
      require a mixed strategy.
      Whenever we have a shift/reduce choice,
      and one of the operators is on the stack,
      we check to see if the topmost operator is a multiply or an addition operator.
      If it is a multiply operator, we reduce.
      In all other cases, if there is a shift/reduce choice, we shift.
    </p>
    <p>
      In the discussion above,
      I have pulled the strategy for making stack decisions
      (shift/reduce and reduce/reduce)
      out of thin air.
      Clearly, if bottom-up parsing was going to be
      a practical parsing algorithm,
      the stack decisions
      would have to be
      made algorithmically.
      In fact, discovering a practical way to do this
      was a far from trivial task.
      The solution in Knuth's paper was considered (and apparently intended)
      to be mathematically provocative, rather than practical.
      But by 1979, it was thought a practical way to make stack decisions
      had been found
      and yacc, a parser generator based on bottom-up parsing, was released.
      (Readers today may be more familiar with yacc's successor, bison.)
    </p>
    <h3>The fate of bottom-up parsing</h3>
    <p>
      With yacc, it looked as if the limitations of top-down parsing were past us.
      We now had a parsing algorithm that could readily and directly
      parse left and right recursions, as well as arithmetic expressions.
      Theoreticians thought they'd found the Holy Grail.
    </p><p>But not all of the medieval romances had happy endings.
      And as I've
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html">described
        elsewhere</a>,
      this story ended badly.
      Bottom-up parsing was driven by tables which made the algorithm fast
      for correct inputs, but unable to accurately diagnose faulty ones.
      The subset of grammars parsed was still not quite large enough,
      even for conservative language designers.
      And bottom-up parsing was very unfriendly to custom hacks,
      which made every shortcoming loom large.
      It is much harder to work around a problem in a bottom-up
      parser than than it is to deal with a similar shortcoming
      in a top-down parser.
      After decades of experience with bottom-up parsing,
      top-down parsing has re-emerged as the
      algorithm of choice.
    </p>
    <h3>Non-determinism</h3>
    <p>For many, the return to top-down parsing
      answers the question that we posed earlier:
      "Is there any systematic way to exploit right context when parsing?"
      So far, the answer seems to be a rather startling "No".
      Can this really be the end of the story?
    </p><p>
      It would be very strange if the best basic parsing algorithm we know is top-down.
      Above, I described at some length some very important grammars that
      can be parsed bottom-up
      but not top-down, at least not directly.
      Progress like this seems like a lot to walk away from,
      and especially to walk back all the way to what is
      essentially a brute force algorithm.
      This perhaps explains why lectures
      and textbooks persist in teaching bottom-up parsing to
      students who are very unlikely to use it.
      Because the verdict from practitioners seems to be in,
      and likely to hold up on appeal.
    </p>
    <p>Fans of deterministic top-down parsing,
      and proponents of deterministic bottom-up parsing share
      an assumption:
      For a practical algorithm to be linear,
      it has to be deterministic.
      But is this actually the case?
    </p>
    <p>It's not, in fact.
      To keep bottom-up parsing deterministic, we restricted ourselves to a stack.
      But what if we track all possible subpieces of parses?
      For efficiency, we can link them and put them into tables,
      making the final decisions in a second pass,
      once the tables are complete.
      (The second pass replaces the stack-driven
      see-sawing back and forth of the deterministic bottom-up algorithm,
      so it's not an inefficiency.)
      Jay Earley in 1968 came up with an algorithm to do this,
      and in 1991 Joop Leo added a memoization to Earley's
      algorithm which made it linear for all deterministic grammars.
    </p><p>The "deterministic grammars"
      are exactly the bottom-up parseable grammars
      with lookahead -- the set of grammars parsed by Knuth's algorithm.
      So that means the Earley/Leo algorithm parses,
      in linear time,
      everything that a deterministic bottom-up parser can parse,
      and therefore every grammar that
      a deterministic top-down parser can parse.
      (In fact, the Earley/Leo algorithm is linear for a lot of
      ambiguous grammars as well.)
    </p><p>Top-down parsing had the advantage that it was easy to know where
      you are.  The Earley/Leo algorithm has an equivalent advantage -- its
      tables know where it is, and it is easy to query them programmatically.
      In 2010, this blogger modified the Earley/Leo algorithm
      to have the other big advantage of top-down parsing:
      The Marpa algorithm rearranges the Earley/Leo parse engine so that we can
      stop it, perform our own logic, and restart where we left off.
      <a href="http://savage.net.au/Marpa.html">A quite useable parser based on the Marpa algorithm</a>
      is available as open source.
    </p><h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href="http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 17:53 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/ll.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Tue, 04 Nov 2014</h3>
<br />
<center><a name="successful"> <h2>What makes a parsing algorithm successful?</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>What makes a parsing algorithm successful?
      Two factors, I think.
      First, does the algorithm parse a workably-defined set of grammars in linear time?
      Second, does it allow the application to intervene in the parse
      with custom code?
      When parsing algorithms are compared,
      typically neither of these gets much attention.
      But the successful algorithms do one or the other.
    </p>
    <h3>Does the algorithm parse a workably-defined set of grammars in linear time?</h3>
    <p>"Workably-defined" means more than well-defined
      in the mathematical sense --
      the definition has to be <b>workable</b>.
      That is, the
      definition must be something that,
      with reasonable effort,
      a programmer can use in practice.
    </p><p>
      The algorithms in regular expression engines are workably-defined.
      A regular expression, in the pure sense consists of a sequence of symbols,
      usually shown by concatenation:
    </p><blockquote><pre>a b c</pre></blockquote><p>
      or a choice among sequences, usually shown by a vertical bar:
    </p><blockquote><pre>a | b | c</pre></blockquote><p>
      or a repetition of any of the above, typically shown with a star:
    </p><blockquote><pre>a*</pre></blockquote><p>
      or any recursive combination of these.
      True, if this definition is new to you, it can take time to get
      used to.
      But vast numbers of working programming are very much "used to it",
      can think in terms of regular expressions,
      and can determine if a particular problem will yield to treatment
      as a regular expression, or not.
      <p>
      Parsers in the LALR family (yacc, bison, etc.)
      do <b>not</b>
      have a workably defined set of grammars.
      LALR is perfectly well-defined mathematically,
      but even experts in parsing theory are hard put to decide
      if a particular grammar is LALR.
    </p><p>
      Recursive descent also does not have a workably defined
      set of grammars.
      Recursive descent doesn't even have a precise mathematical description --
      you can say that recursive descent is LL,
      but in practice LL tables are rarely used.
      Also in practice, the LL logic is extended with every other trick
      imaginable, up to and including switching to other parsing algorithms.
    </p>
    <h3>Does it allow the user to intervene in the parse?</h3>
    <p>It is not easy for users to intervene in the processing
      of a regular expression, though some implementations attempt to
      allow such efforts.
      LALR parsers are notoriously opaque.
      Those who maintain the LALR-driven Perl parser have tried
      to supplement its abilities with
      custom code, with results that will not encourage
      others making the same attempt.
    </p><p>Recursive descent, on the other hand, has no parse engine --
      it is 100% custom code.
      You don't get much friendlier than that.
    </p><h3>Conclusions</h3><p>
      Regular expressions are a success,
      and will remain so,
      because the set of grammars
      they handle is very workably-defined.
      Applications using regular expressions have to take what the algorithm
      gives them, but what it gives them is very predictable.
      <p>
      For example, an application can write regular expressions on the fly, and
      the programmer can be confident they will run as long as they are well-formed.
      And it is easy to determine if the regular expression is well-formed.
      (Whether it actually does what you want is a separate issue.)
      <p>
      Recursive descent does not handle a workably-defined set of grammars,
      and it also has to be hand-written.
      But it makes up for this by allowing the user to step into the parsing process
      anywhere, and "get his hands dirty".
      Recursive descent does nothing for you, but it does allow you complete control.
      This is enough to make recursive descent the current algorithm of choice
      for major parsing projects.
      <p>
      As I have
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html">chronicled
      elsewhere</a>,
      LALR was once,
      on highly convincing theoretical grounds,
      seen as
      <b>the</b> solution to the parsing problem.
      But while mathematically well-defined,
      LALR was not workably defined.
      And it was very hostile to applications that tried to alter,
      or even examine, its syntax-driven workings.
      After decades of trying to make it work,
      the profession has abandoned LALR almost totally.
    </p>
    <h3>What about Marpa?</h3>
    <p>Marpa has both properties:
      its set of grammars is workably-defined.
      And, while Marpa is syntax-driven like LALR and regular expressions,
      it also allows the user to stop the parse engine,
      communicate with it about the state of the parse,
      do her own parsing for a while,
      and restart the parse engine at any point she wants.
    </p>
    <p>Marpa's workable definition has a nuance that the one
    for regular expressions does not.
    For regular expressions, linearity is a given --
    they parse in linear time or fail.
      Marpa parses a much larger class of grammars, the context-free grammars --
      anything that can be written in BNF.
      BNF is used to describe languages in standards,
      and is therefore itself a kind of "gold standard"
      for a workable definition of a
      set of grammars.
      However, Marpa does <b>not</b>
      parse everything that can be written in BNF in linear time.
    </p>
    <p>Marpa linearly-parsed set of grammars is smaller than the context-free
    grammars, but it is still very large, and it is still workably-defined.
      Marpa will parse any unambiguous language in linear time,
      unless it contains unmarked middle recursions.
      An example of a "marked" middle recursion is the language described
      by
    </p><blockquote><pre>S ::= a S a | x</pre></blockquote><p>
    a string of which is "<tt>aaaxaaa</tt>",
      where the "<tt>x</tt>" marks the middle.
      An example of an "unmarked" middle recursion is the language described
      by
    </p><blockquote><pre>S ::= a S a | a</pre></blockquote><p>
    a string of which is "<tt>aaaaaaa</tt>",
      where nothing marks the middle, so that you don't know until the end where the
      middle of the recursion is.
      If a human can reliably find the middle by eyeball, the middle recursion is marked.
      If a human can't, then the middle recursion might be unmarked.
    </p>
    <p>Marpa also parses a large set of unambiguous grammars linearly,
      and this set of grammars is also workably-defined.
      Marpa parses an ambiguous grammar in linear time if
    </p><ul>
      <li>It has no unmarked middle recursions.
      </li>
      <li>All right recursions are unambiguous.
      </li>
      <li>There are no cycles.
      A cycle occurs, for example, if there is a rule <tt>A ::= A</tt>
      in the grammar.
      </li>
      <li>Marpa's level of ambiguity at any location is bounded by a constant.
      </li>
      </ul>
      <p>The term "level of ambiguity" requires a bit of explanation.
	At any given location,
        there can be as many rules "in play" as you like,
	without affecting the level of ambiguity.
        The key question: What is the maximum number of different origins
	that a rule might have?
	(The "origin" of a rule is the location where it begins.)
        That is, can a rule currently in play
	have at most 20 different origins?
        Or could it have its origin at every location so far?
        If the maximum number of origins is 20 or any other fixed constant, the level of
        ambiguity is "bounded".
	But if the maximum number of origins keeps growing as the length of the input
	grows, the level of ambiguity is unbounded.
    <p>For the unambiguous case, Marpa's workable definition encompasses
    a much larger class of grammars, but is no more
      complex than that for regular expressions.
      If you want to extend even further,
      and work with ambiguous grammars,
      the definition remains quite workable.
      Of the four restrictions needed to ensure linearity,
      the one requiring a bounded level of ambiguity is the only one
      that might force you to exercise real vigliance --
      once you get into ambiguity, unboundedness is easy to slip into.
      <p>
      As for the other three,
      cycles never occur in a practical grammars,
      and Marpa reports them,
      so that you simply fix them when they happen.
      Most recursions will be left recursions,
      which are unrestricted.
      My experience has been that, in practical grammars,
      unmarked middle recursions
      and ambiguous right recursions are not especially
      tempting features.
      If you note
      whenever you use a right recursion,
      checking that it is not ambiguous,
      and if you note whenever you use a middle recursion,
      checking that it is marked,
      then you will stay linear.
    </p>
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p><h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
<br />
<p>posted at: 21:10 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/successful.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<br />
<center><a name="backpan"> <h2>Removing obsolete versions of Marpa from CPAN</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>Marpa::XS, Marpa::PP, and Marpa::HTML are obsolete versions of
      Marpa, which I have been keeping on CPAN for the convenience of legacy
      users.
      All new users should look only at
      <a href="https://metacpan.org/release/Marpa-R2">Marpa::R2</a>.
    </p><p>
      I plan to delete the obsolete releases from CPAN soon.
      For legacy users who need copies, they will still be available on backPAN.
    </p><p>
      I do this because their placement on CPAN placement makes them "attractive nuisances" --
      they show up in searches and generally make it harder to find
      <a href="https://metacpan.org/release/Marpa-R2">Marpa::R2</a>,
      which is the version that new users should be interested in.
      There is also some danger a new user could, by mistake, use the
      obsolete versions instead of Marpa::R2.
    </p><p>
      It's been some time since someone has reported a bug in their code,
      so they should be stable for legacy applications.
      I would usually promise to fix serious bugs that affect legacy users,
      but unfortunately, especially in the case of Marpa::XS,
      it is a promise I would have trouble keeping.
      Marpa::XS depends on Glib, and uses a complex build which I last performed
      on a machine I no longer use for development.
    </p><p>
      For this reason, a re-release to CPAN with deprecatory language is also not an option.
      I probably would not want to do so anyway -- the CPAN infrastructure by default
      pushes legacy
      users into upgrading, which always carries some risk.
      New deprecatory language would add no
      value for the legacy users,
      and they are the only audience these releases exist to serve.
    </p>
    <h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 10:53 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/backpan.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sat, 01 Nov 2014</h3>
<br />
<center><a name="delimiter"> <h2>Reporting mismatched delimiters</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>In many contexts, programs need to identify
      non-overlapping pieces of a text.
      One very direct way to do this
      is to use a pair of delimiters.
      One delimiter of the pair marks the start
      and the other marks the end.
      Delimiters can take many forms:
      Quote marks, parentheses, curly braces, square brackets,
      XML tags, and HTML tags
      are all delimiters in this sense.
    </p>
    <p>
      Mismatching delimiters is easy to do.
      Traditional parsers are often poor at reporting these errors:
      hopeless after the first mismatch,
      and for that matter none too precise about the first one.
      This post outlines a scaleable method for the accurate
      reporting of mismatched delimiters.
      I will illustrate the method with a simple
      but useable tool --
      a utility which reports mismatched brackets.
    </p>
    <h3>The example script</h3>
    <p>The
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">example
      script</a>,
      <tt>bracket.pl</tt>,
      reports mismatched brackets in the set:
    </p>
    <blockquote><pre>() {} []</pre></blockquote>
      <p>
      They are expected to nest without overlaps.
      Other text is treated as filler.
      <tt>bracket.pl</tt>
      is not smart about things
      like strings or comments.
      This does have the advantage of making
      <tt>bracket.pl</tt>
      mostly language-agnostic.
    </p>
    <p>
      Because it's intended primarily to be read
      as an illustration of the technique,
      <tt>bracket.pl</tt>'s grammar
      is a basic one.
      The grammar that
      <tt>bracket.pl</tt>
      uses is so simple that
      an emulator of <tt>bracket.pl</tt>
      could be written using recursive descent.
      I hope the reader who goes on to look into the details
      will see that this technique scales to more
      complex situations,
      in a way that a solution based on a traditional parser
      will not.
    </p>
    <h3>Error reports</h3>
    <p>The description of how the method works will make more
      sense after we've looked at some examples of the diagnostics
      <tt>bracket.pl</tt>
      produces.
      To be truly useful,
      <tt>bracket.pl</tt>
      must report mismatches that span
      many lines,
      and it can do this.
      But single-line examples are easier to follow.
      All the examples in this post will be contained in a one line.
      Consider the string '<tt>((([))</tt>'.
      <tt>bracket.pl</tt>'s diagnostics are:
    </p><blockquote><pre>
* Line 1, column 1: Opening '(' never closed, problem detected at end of string
((([))
^
====================
* Line 1, column 4: Missing close ], problem detected at line 1, column 5
((([))
   ^^
</pre></blockquote>
    <p>
      In the next example
      <tt>bracket.pl</tt>
      realizes that it
      cannot accept the ')' at column 16, without first closing the set of curly braces started at column 5.
      It identifies the problem, along with both of the locations involved.
    </p>
    <blockquote><pre>
* Line 1, column 5: Missing close }, problem detected at line 1, column 16
[({({x[]x{}x()x)})]
    ^          ^
</pre></blockquote>
    <p>
      So far, so good.
      But an important advantage of
      <tt>bracket.pl</tt>
      has yet to be seen.
      Most compilers,
      once they report a first mismatched delimiter,
      produce error messages that are
      unreliable --
      so unreliable that they are useless in practice.
      <tt>bracket.pl</tt>
      repairs a mismatched bracket before continuing,
      so that it can do a reasonable job of analyzing the text
      that follows.
      Consider the text
      '<tt>({]-[(}-[{)</tt>'.
      The output of
      <tt>bracket.pl</tt>
      is
    </p><blockquote><pre>
* Line 1, column 1: Missing close ), problem detected at line 1, column 3
({]-[(}-[{)
^ ^
====================
* Line 1, column 2: Missing close }, problem detected at line 1, column 3
({]-[(}-[{)
 ^^
====================
* Line 1, column 3: Missing open [
({]-[(}-[{)
  ^
====================
* Line 1, column 5: Missing close ], problem detected at line 1, column 7
({]-[(}-[{)
    ^ ^
====================
* Line 1, column 6: Missing close ), problem detected at line 1, column 7
({]-[(}-[{)
     ^^
====================
* Line 1, column 7: Missing open {
({]-[(}-[{)
      ^
====================
* Line 1, column 9: Missing close ], problem detected at line 1, column 11
({]-[(}-[{)
        ^ ^
====================
* Line 1, column 10: Missing close }, problem detected at line 1, column 11
({]-[(}-[{)
         ^^
====================
* Line 1, column 11: Missing open (
({]-[(}-[{)
          ^
</pre></blockquote>
    <p>Each time,
      <tt>bracket.pl</tt>
      corrects itself,
      and accurately reports the next set of problems.
    </p><h3>A difficult error report</h3>
    <p>
      To be 100% accurate,
      <tt>bracket.pl</tt>
      would have to guess the programmer's intent.
      This is, of course, not possible.
      Let's look at a text where
      <tt>bracket.pl</tt>'s guesses are not so good:
      <tt>{{]}</tt>.
      Here we will assume the closing square bracket is a typo for a closing parenthesis.
      Here's the result:
    </p><blockquote><pre>
* Line 1, column 1: Missing close }, problem detected at line 1, column 3
{{]}
^ ^
====================
* Line 1, column 2: Missing close }, problem detected at line 1, column 3
{{]}
 ^^
====================
* Line 1, column 3: Missing open [
{{]}
  ^
====================
* Line 1, column 4: Missing open {
{{]}
   ^
</pre></blockquote><p>
      Instead of one error,
      <tt>bracket.pl</tt>
      finds four.
    </p><p>
      But even in this case, the method is fairly good, especially when
      compared with current practice.
      The problem is at line 1, column 3,
      and the first three messages all identify this as one of their
      potential problem locations.
      It is reasonable to believe that a programmer, especially once
      he becomes used to this kind of mismatch reporting,
      will quickly find the first mismatch and fix it.
      For this difficult case,
      <tt>bracket.pl</tt> may not be much better than the state of the art,
      but it is certainly no worse.
    </p>
    <h3>How it works</h3>
    <p>
      For full details of the workings of
      <tt>bracket.pl</tt>
      there is
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">the code</a>,
      which is heavily commented.
      This section provides a conceptual overview.
    </p><p>
      <tt>bracket.pl</tt>
      uses two features of Marpa:
      left-eideticism and the Ruby Slippers.
      By left-eidetic, I mean that Marpa knows everything there is to know
      about the parse at, and to left of, the current position.
      As a consequence,
      Marpa
      also knows exactly which of its input symbols
      can lead to a successful parse,
      and is able to stop as soon as it knows that the parse cannot succeed.
    </p>
    <p>
      In the Ruby Slippers technique, we arrange for parsing to stop
      whenever we encounter an input which
      would cause parsing to fail.
      The application then
      asks Marpa, "OK.  What input would allow the
      parse to continue?"
      The application takes Marpa's answer to this
      question, and uses it to concoct
      an input that Marpa will accept.
    </p>
    <p>
      In this case,
      <tt>bracket.pl</tt>
      creates a virtual token which fixes the mismatch
      of brackets.
      Whatever the missing bracket may be,
      <tt>bracket.pl</tt>
      invents a bracket of that kind,
      and adds it to the virtual input.
      This done,
      parsing and error detection
      can proceed as if there was no problem.
      Of course,
      the error which made the Ruby Slippers token necessary
      is recorded, and those records are the source of the
      error reports we saw above.
    </p>
    <p>
      To make its error messages as informative as possible
      in the case of missing closing brackets,
      <tt>bracket.pl</tt>
      needs to report the exact location of
      the opening bracket.
      Left-eideticism again comes in handy here.
      Once the virtual closing bracket is supplied to Marpa,
      <tt>bracket.pl</tt>
      asks, "That bracketed text that I just closed -- where did it begin?"
      The Marpa parser tracks the start location
      of all symbol and rule instances,
      so it is able to provide the application
      with the exact location of
      the starting bracket.
    </p><p>
      When
      <tt>bracket.pl</tt>
      encounters a problem at a point where there are unclosed opening
      brackets, it has two choices.
      It can be optimistic or it can be pessimistic.
      "Optimistic" means it can hope that something later in the input will close
      the opening bracket.
      "Pessimistic" means it can decide that "all bets are off" and use
      Ruby Slippers tokens to close all the currently active open brackets.
    </p>
    <p>
      <tt>bracket.pl</tt>
      uses the pessimistic strategy.
      While the optimistic strategy sounds better, in practice
      the pessimistic one seems to provide better diagnostics.
      The pessimistic strategy does report some fixable problems
      as errors.
      But the optimistic one can introduce spurious fixes.
      These hide the real errors,
      and it is worse to miss errors
      than it is to overreport them.
      Even when the pessimistic strategy overreports,
      its first error message will always accurately identify
      the first problem location.
    </p>
    <p>
      While
      <tt>bracket.pl</tt>
      is already useable,
      I think of it as a prototype.
      Beyond that,
      the problem of matching delimiters
      is in fact very general, and I believe these techniques may have very wide application.
    </p>
    <h3>For more</h3>
    <p>
      The example script of this post is
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">a Github gist</a>.
      For more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 11:11 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/delimiter.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
