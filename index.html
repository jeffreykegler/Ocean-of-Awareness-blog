<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Tue, 04 Nov 2014</h3>
<br />
<center><a name="successful"> <h2>What makes a parsing algorithm successful?</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>What makes a parsing algorithm successful?
      Two factors, I think.
      First, does the algorithm parse a workably-defined set of grammars in linear time?
      Second, does it allow the application to intervene in the parse
      with custom code?
      When parsing algorithms are compared,
      typically neither of these gets much attention.
      But the successful algorithms do one or the other.
    </p>
    <h3>Does the algorithm parse a workably-defined set of grammars in linear time?</h3>
    <p>By "workably-defined" I do not just mean well-defined
      in the mathematical sense.
      I mean something that goes beyond that --
      the set of grammars has to have a definiton that is workable.
      Workable means that the
      definition is something that,
      with reasonable effort,
      a programmer can use in practice.
    </p><p>
      The algorithms in regular expression engines are workably-defined.
      A regular expression, in the pure sense consists of a sequence of symbols,
      usually shown by concatenation:
    </p><blockquote><pre>a b c</pre></blockquote><p>
      or a choice among sequences, usually shown by a vertical bar:
    </p><blockquote><pre>a | b | c</pre></blockquote><p>
      or a repetition of any of the, typically shown with a star:
    </p><blockquote><pre>a*</pre></blockquote><p>
      or any recursive combination of these.
      True, if this definition is new to you, it can take time to get
      used to.
      But vast numbers of working programming are very much "used to it",
      can think in terms of regular expressions,
      and can determine if a particular problem will yield to treatment
      as a regular expression, or not.
      <p>
      Parsers in the LALR family (yacc, bison, etc.)
      do <b>not</b>
      have a workably defined set of grammars.
      LALR is perfectly well-defined mathematically,
      but even experts in parsing theory are hard put to decide
      if a particular grammar is LALR.
    </p><p>
      Recursive descent also does not have a workably defined
      set of grammars.
      Recursive descent doesn't even have a precise mathematical description --
      you can say they are LL, but in practice LL tables are rarely used.
      Also in practice, the LL logic is extended with every other trick
      imaginable, up to and including switching to other parsing algorithms.
    </p>
    <h3>Does it allow the user to intervene in the parse?</h3>
    <p>It is not easy for users to intervene in the processing
      of a regular expression, though some implementations attempt to
      allow such efforts.
      LALR parsers are notoriously opaque.
      Those who maintain the Perl parser have tried
      to supplement its abilities with
      custom code, with results that will not encourage
      others making the same attempt.
    </p><p>Recursive descent, on the other hand, has no parse engine --
      it is 100% custom code.
      You don't get much friendlier than that.
    </p><h3>Conclusion</h3><p>
      Regular expressions are a success,
      and will remain so,
      because the set of grammars
      they handle is very workably-defined.
      Applications using regular expressions have to take what the algorithm
      gives them, but what it gives them is very predictable.
      <p>
      For example, an application can write regular expressions on the fly, and
      the programmer can be confident they will run as long as they are well-formed.
      And it is easy to determine if the regular expression is well-formed.
      (Whether it actually does what you want is a separate issue.)
      <p>
      Recursive descent does not handle a workably-defined set of grammars,
      and it also has to be hand-written.
      But it makes up for this by allowing the user to step into the parsing process
      anywhere, and "get his hands dirty".
      Recursive descent does nothing for you, but it does allow you complete control.
      This is enough to mkae recursive descent the current algorithm of choice
      for major parsing projects.
      <p>
      As I have
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2014/09/chron.html">chronicled
      elsewhere</a>,
      LALR was once,
      on highly convincing theoretical grounds,
      seen as
      <b>the</b> solution to the parsing problem.
      But while mathematically well-defined,
      LALR was not workably defined,
      it was very hostile to applications that tried to alter,
      or even examine, its syntax-driven workings.
      After decades of trying to make it work,
      the profession has abandoned LALR almost totally.
    </p>
    <h3>What about Marpa?</h3>
    <p>Marpa has both properties:
      its set of grammars is workably-defined.
      And, while Marpa is syntax-driven like LALR and regular expressions,
      it also allows the user to stop the parse engine,
      communicate with it about the state of the parse,
      do its own parsing for a while,
      and restart the parse engine at any point it wants.
    </p>
    <p>Marpa's workable definition has a nuance that the one
    for regular expressions does not.
    For regular expression linearity is a given --
    they parse in linear time or fail.
      Marpa parses a much larger class of grammars, the context-free grammars --
      anything that can be written in BNF.
      BNF is used to describe languages in standards,
      and is therefore the "gold standard" for a workable definition of a
      set of grammars.
      However, Marpa does <b>not</b>
      parse everything that can be written in BNF in linear time.
    </p>
    <p>Marpa linearly-parsed set of grammars is smaller than the context-free
    grammars, but it is still very large, and it is still workably-defined.
      Marpa will parse any unambiguous language in linear time,
      unless it contains unmarked middle recursions.
      An example of a "marked" middle recursion is the language described
      by
    </p><blockquote><pre>S ::= a S a | x</pre></blockquote><p>
    a string of which is "<tt>aaaxaaa</tt>",
      where the "<tt>x</tt>" marks the middle.
      An example of an "unmarked" middle recursion is the language described
      by
    </p><blockquote><pre>S ::= a S a | a</pre></blockquote><p>
    a string of which is "<tt>aaaaaaa</tt>",
      where nothing marks the middle, so that you don't know until the end where the
      middle of the recursion is.
      If a human can reliably find the middle by eyeball, the middle recursion is marked.
      If a human can't, then the middle recursion might be unmarked.
    </p>
    <p>Marpa also parses a large set of unambiguous grammars linearly,
      and this set of grammars is also workably-defined.
      Marpa parses an ambiguous grammar in linear time if
    </p><ul>
      <li>It has no unmarked middle recursions.
      </li>
      <li>All right recursions are unambiguous.
      </li>
      <li>There are no cycles.
      A cycle occurs, when example, if there is a rule <tt>A ::= A</tt>
      in the grammar.
      </li>
      <li>Marpa's level of ambiguity at any location is bounded by a constant.
        There can be as many rules "in play" as you like.
        The key question is at how many points,
        earlier in the parse, can these rules have begun?
        That is, can a rule currently in play
        have begun only at 20 previous locations,
        and could it have started at every location so far?
        If the answer is 20 or some other constant, the level of
        ambiguity is "bounded".
        If not, the level of ambiguity is unbounded.
      </li>
      </ul>
    <p>For the unambiguous case, Marpa's workable definition encompasses
    a much larger class of grammars, but is no more
      complex than that for regular expressions.
      If you want to extend even further,
      and work with ambiguous grammars,
      the definition remains quite workable.
      Of the four restrictions needed to ensure linearity,
      the one requiring a bounded level of ambiguity is the only one
      that might force you to exercise real vigliance --
      once you get into ambiguity, unboundedness is easy to slip into.
      <p>
      As for the other three,
      cycles never occur in a practical grammars,
      and Marpa reports them,
      so that you simply fix them when they happen.
      Most recursions will be left recursions,
      which are unrestricted.
      And my experience so far has been that, in practical grammars,
      unmarked middle recursions
      and ambiguous right recursions are not especially
      tempting features.
    </p>
    <h3>More about Marpa</h3><p>
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p><h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
<br />
<p>posted at: 21:10 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/successful.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<br />
<center><a name="backpan"> <h2>Removing obsolete versions of Marpa from CPAN</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>Marpa::XS, Marpa::PP, and Marpa::HTML are obsolete versions of
      Marpa, which I have been keeping on CPAN for the convenience of legacy
      users.
      All new users should look only at
      <a href="https://metacpan.org/release/Marpa-R2">Marpa::R2</a>.
    </p><p>
      I plan to delete the obsolete releases from CPAN soon.
      For legacy users who need copies, they will still be available on backPAN.
    </p><p>
      I do this because their placement on CPAN placement makes them "attractive nuisances" --
      they show up in searches and generally make it harder to find
      <a href="https://metacpan.org/release/Marpa-R2">Marpa::R2</a>,
      which is the version that new users should be interested in.
      There is also some danger a new user could, by mistake, use the
      obsolete versions instead of Marpa::R2.
    </p><p>
      It's been some time since someone has reported a bug in their code,
      so they should be stable for legacy applications.
      I would usually promise to fix serious bugs that affect legacy users,
      but unfortunately, especially in the case of Marpa::XS,
      it is a promise I would have trouble keeping.
      Marpa::XS depends on Glib, and uses a complex build which I last performed
      on a machine I no longer use for development.
    </p><p>
      For this reason, a re-release to CPAN with deprecatory language is also not an option.
      I probably would not want to do so anyway -- the CPAN infrastructure by default
      pushes legacy
      users into upgrading, which always carries some risk.
      New deprecatory language would add no
      value for the legacy users,
      and they are the only audience these releases exist to serve.
    </p>
    <h3>Comments</h3>
    <p>Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
      To learn more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 10:53 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/backpan.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sat, 01 Nov 2014</h3>
<br />
<center><a name="delimiter"> <h2>Reporting mismatched delimiters</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>In many contexts, programs need to identify
      non-overlapping pieces of a text.
      One very direct way to do this
      is to use a pair of delimiters.
      One delimiter of the pair marks the start
      and the other marks the end.
      Delimiters can take many forms:
      Quote marks, parentheses, curly braces, square brackets,
      XML tags, and HTML tags
      are all delimiters in this sense.
    </p>
    <p>
      Mismatching delimiters is easy to do.
      Traditional parsers are often poor at reporting these errors:
      hopeless after the first mismatch,
      and for that matter none too precise about the first one.
      This post outlines a scaleable method for the accurate
      reporting of mismatched delimiters.
      I will illustrate the method with a simple
      but useable tool --
      a utility which reports mismatched brackets.
    </p>
    <h3>The example script</h3>
    <p>The
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">example
      script</a>,
      <tt>bracket.pl</tt>,
      reports mismatched brackets in the set:
    </p>
    <blockquote><pre>() {} []</pre></blockquote>
      <p>
      They are expected to nest without overlaps.
      Other text is treated as filler.
      <tt>bracket.pl</tt>
      is not smart about things
      like strings or comments.
      This does have the advantage of making
      <tt>bracket.pl</tt>
      mostly language-agnostic.
    </p>
    <p>
      Because it's intended primarily to be read
      as an illustration of the technique,
      <tt>bracket.pl</tt>'s grammar
      is a basic one.
      The grammar that
      <tt>bracket.pl</tt>
      uses is so simple that
      an emulator of <tt>bracket.pl</tt>
      could be written using recursive descent.
      I hope the reader who goes on to look into the details
      will see that this technique scales to more
      complex situations,
      in a way that a solution based on a traditional parser
      will not.
    </p>
    <h3>Error reports</h3>
    <p>The description of how the method works will make more
      sense after we've looked at some examples of the diagnostics
      <tt>bracket.pl</tt>
      produces.
      To be truly useful,
      <tt>bracket.pl</tt>
      must report mismatches that span
      many lines,
      and it can do this.
      But single-line examples are easier to follow.
      All the examples in this post will be contained in a one line.
      Consider the string '<tt>((([))</tt>'.
      <tt>bracket.pl</tt>'s diagnostics are:
    </p><blockquote><pre>
* Line 1, column 1: Opening '(' never closed, problem detected at end of string
((([))
^
====================
* Line 1, column 4: Missing close ], problem detected at line 1, column 5
((([))
   ^^
</pre></blockquote>
    <p>
      In the next example
      <tt>bracket.pl</tt>
      realizes that it
      cannot accept the ')' at column 16, without first closing the set of curly braces started at column 5.
      It identifies the problem, along with both of the locations involved.
    </p>
    <blockquote><pre>
* Line 1, column 5: Missing close }, problem detected at line 1, column 16
[({({x[]x{}x()x)})]
    ^          ^
</pre></blockquote>
    <p>
      So far, so good.
      But an important advantage of
      <tt>bracket.pl</tt>
      has yet to be seen.
      Most compilers,
      once they report a first mismatched delimiter,
      produce error messages that are
      unreliable --
      so unreliable that they are useless in practice.
      <tt>bracket.pl</tt>
      repairs a mismatched bracket before continuing,
      so that it can do a reasonable job of analyzing the text
      that follows.
      Consider the text
      '<tt>({]-[(}-[{)</tt>'.
      The output of
      <tt>bracket.pl</tt>
      is
    </p><blockquote><pre>
* Line 1, column 1: Missing close ), problem detected at line 1, column 3
({]-[(}-[{)
^ ^
====================
* Line 1, column 2: Missing close }, problem detected at line 1, column 3
({]-[(}-[{)
 ^^
====================
* Line 1, column 3: Missing open [
({]-[(}-[{)
  ^
====================
* Line 1, column 5: Missing close ], problem detected at line 1, column 7
({]-[(}-[{)
    ^ ^
====================
* Line 1, column 6: Missing close ), problem detected at line 1, column 7
({]-[(}-[{)
     ^^
====================
* Line 1, column 7: Missing open {
({]-[(}-[{)
      ^
====================
* Line 1, column 9: Missing close ], problem detected at line 1, column 11
({]-[(}-[{)
        ^ ^
====================
* Line 1, column 10: Missing close }, problem detected at line 1, column 11
({]-[(}-[{)
         ^^
====================
* Line 1, column 11: Missing open (
({]-[(}-[{)
          ^
</pre></blockquote>
    <p>Each time,
      <tt>bracket.pl</tt>
      corrects itself,
      and accurately reports the next set of problems.
    </p><h3>A difficult error report</h3>
    <p>
      To be 100% accurate,
      <tt>bracket.pl</tt>
      would have to guess the programmer's intent.
      This is, of course, not possible.
      Let's look at a text where
      <tt>bracket.pl</tt>'s guesses are not so good:
      <tt>{{]}</tt>.
      Here we will assume the closing square bracket is a typo for a closing parenthesis.
      Here's the result:
    </p><blockquote><pre>
* Line 1, column 1: Missing close }, problem detected at line 1, column 3
{{]}
^ ^
====================
* Line 1, column 2: Missing close }, problem detected at line 1, column 3
{{]}
 ^^
====================
* Line 1, column 3: Missing open [
{{]}
  ^
====================
* Line 1, column 4: Missing open {
{{]}
   ^
</pre></blockquote><p>
      Instead of one error,
      <tt>bracket.pl</tt>
      finds four.
    </p><p>
      But even in this case, the method is fairly good, especially when
      compared with current practice.
      The problem is at line 1, column 3,
      and the first three messages all identify this as one of their
      potential problem locations.
      It is reasonable to believe that a programmer, especially once
      he becomes used to this kind of mismatch reporting,
      will quickly find the first mismatch and fix it.
      For this difficult case,
      <tt>bracket.pl</tt> may not be much better than the state of the art,
      but it is certainly no worse.
    </p>
    <h3>How it works</h3>
    <p>
      For full details of the workings of
      <tt>bracket.pl</tt>
      there is
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">the code</a>,
      which is heavily commented.
      This section provides a conceptual overview.
    </p><p>
      <tt>bracket.pl</tt>
      uses two features of Marpa:
      left-eideticism and the Ruby Slippers.
      By left-eidetic, I mean that Marpa knows everything there is to know
      about the parse at, and to left of, the current position.
      As a consequence,
      Marpa
      also knows exactly which of its input symbols
      can lead to a successful parse,
      and is able to stop as soon as it knows that the parse cannot succeed.
    </p>
    <p>
      In the Ruby Slippers technique, we arrange for parsing to stop
      whenever we encounter an input which
      would cause parsing to fail.
      The application then
      asks Marpa, "OK.  What input would allow the
      parse to continue?"
      The application takes Marpa's answer to this
      question, and uses it to concoct
      an input that Marpa will accept.
    </p>
    <p>
      In this case,
      <tt>bracket.pl</tt>
      creates a virtual token which fixes the mismatch
      of brackets.
      Whatever the missing bracket may be,
      <tt>bracket.pl</tt>
      invents a bracket of that kind,
      and adds it to the virtual input.
      This done,
      parsing and error detection
      can proceed as if there was no problem.
      Of course,
      the error which made the Ruby Slippers token necessary
      is recorded, and those records are the source of the
      error reports we saw above.
    </p>
    <p>
      To make its error messages as informative as possible
      in the case of missing closing brackets,
      <tt>bracket.pl</tt>
      needs to report the exact location of
      the opening bracket.
      Left-eideticism again comes in handy here.
      Once the virtual closing bracket is supplied to Marpa,
      <tt>bracket.pl</tt>
      asks, "That bracketed text that I just closed -- where did it begin?"
      The Marpa parser tracks the start location
      of all symbol and rule instances,
      so it is able to provide the application
      with the exact location of
      the starting bracket.
    </p><p>
      When
      <tt>bracket.pl</tt>
      encounters a problem at a point where there are unclosed opening
      brackets, it has two choices.
      It can be optimistic or it can be pessimistic.
      "Optimistic" means it can hope that something later in the input will close
      the opening bracket.
      "Pessimistic" means it can decide that "all bets are off" and use
      Ruby Slippers tokens to close all the currently active open brackets.
    </p>
    <p>
      <tt>bracket.pl</tt>
      uses the pessimistic strategy.
      While the optimistic strategy sounds better, in practice
      the pessimistic one seems to provide better diagnostics.
      The pessimistic strategy does report some fixable problems
      as errors.
      But the optimistic one can introduce spurious fixes.
      These hide the real errors,
      and it is worse to miss errors
      than it is to overreport them.
      Even when the pessimistic strategy overreports,
      its first error message will always accurately identify
      the first problem location.
    </p>
    <p>
      While
      <tt>bracket.pl</tt>
      is already useable,
      I think of it as a prototype.
      Beyond that,
      the problem of matching delimiters
      is in fact very general, and I believe these techniques may have very wide application.
    </p>
    <h3>For more</h3>
    <p>
      The example script of this post is
      <a href="https://gist.github.com/jeffreykegler/b6bfeeadfcedeade6519">a Github gist</a>.
      For more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
        official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 11:11 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/11/delimiter.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Sun, 07 Sep 2014</h3>
<br />
<center><a name="chron"> <h2>Parsing: a timeline</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p>[ Revised 22 Oct 2014 ]
    </p>
    <p><b>1960</b>:
      The ALGOL 60 spec comes out.
      It specifies, for the first time, a block structured
      language.
      The ALGOL committee is well aware
      that
      nobody knows how to parse such a language.
      But they believe that,
      if they specify a block-structured
      language, a parser for it will be invented.
      Risky as this approach is, it pays off ...
    </p>
    <p><b>1961</b>: Ned Irons publishes his ALGOL parser.
      In fact, the Irons parser
      is the first parser of any kind to be described
      in print.
      Ned's algorithm is a left parser --
      a form of recursive descent.
      Unlike modern
      recursive descent,
      the Irons algorithm
      is general and syntax-driven.
      "General" means it can parse anything written in BNF.
      "Syntax-driven" (aka declarative) means that parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    </p>
    <p><b>1961</b>:
      Almost simultaneously, hand-coded approaches to left parsing
      appear.
      These we would now recognize as recursive descent.
      Over the following years, hand-coding approaches
      will become more popular for left parsers
      than syntax-driven algorithms.
      Three factors are at work:
      <ul>
      <li>
      In 1960's, memory and CPU are both extremely limited.
      Hand-coding pays off, even when the gains are small.
      <li>
      Pure left parsing is a very weak parsing technique.
      Hand-coding is often necessary
      to overcome its limits.
      This is
      as true today as it is in 1961.
      <li>
      Left parsing works well in combination with hand-coding --
      they are a very good fit.
      </ul>
    </p>
    <p><b>1965</b>:
    Don Knuth invents LR parsing.
      Knuth is primarily interested
      in the mathematics.
      Knuth describes a parsing algorithm,
      but it is not thought practical.
    </p>
    <p><b>1968</b>: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's core idea is to
      track everything about the parse in tables.
      Earley's algorithm is enticing, but it has three major issues:
      <ul>
      <li>First, there is a bug in the handling of zero-length rules.
      <li>Second, it is quadratic for right recursions.
      <li>Third, the bookkeeping required to set up the tables is,
      by the standards of 1968 hardware, daunting.
      </ul>
    <p><b>1969</b>:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
    </p>
    <p><b>1972</b>:
      Aho and Ullmann describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    <p><b>1975</b>:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    </p>
    <p><b>1977</b>:
      The first "Dragon book" comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters "LALR".
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    </p>
    <p><b>1979</b>: Bell Laboratories releases Version 7 UNIX.
	V7 includes what is, by far,
		the most comprehensive, useable and easily available
		compiler writing toolkit yet developed.
	 Central to the toolkit is
	 yacc, an LALR based parser generator.
	  With a bit of hackery,
	  yacc parses its own input language,
	  as well as the language of V7's main compiler,
	  the portable C compiler.
	  After two decades of research,
	  it seems that the parsing problem is solved.
    </p>
    <p><b>1987</b>:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    </p>
    <p><b>1991</b>:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of Earley algorithm.
      But Earley parsing is almost forgotten.
      It will be 20 years before anyone writes a practical
      implementation of Leo's algorithm.
    </p>
    <p><b>1990's</b>:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is "fast but stupid".
    </p><b>2000</b>:
    Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    </p>
    <p><b>2002</b>:
      Aycock&Horspool publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    </p>
    <p><b>2006</b>:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    </p>
    <p><b>2000 to today</b>:
    With the retreat from LALR comes a collapse in the
      prestige of parsing theory.
      After a half century,
      we seem to be back
      where we started.
      If you took Ned Iron's original 1961 algorithm,
      changed the names and dates,
      and translated the code from the mix of assembler and
      ALGOL into Haskell,
      you would easily republish it today,
      and bill it as 
      as revolutionary and new.
    </p>
    <p>
    <h3>Marpa</h3>
      Over the years,
      I had come back to Earley's algorithm again and again.
      Around 2010, I realized
      that the original, long-abandoned vision --
      an efficient, practical, general and syntax-driven parser --
      was now, in fact, quite possible.
      The necessary pieces had fallen into place.
    </p>
    <p>
      Aycock&Hospool has solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and probably no longer relevant in any case --
      caches misses are now the bottleneck.
    </p>
    <p>But while the original issues with Earley's disappeared,
      a new issue emerged.
      With a parsing algorithm as powerful as Earley's behind it,
      a syntax-driven approach can do much more than it can with
      a left parser.
      But with the experience with LALR in their collective consciousness,
      few modern programmers are prepared
      to trust a purely declarative parser.
      As Lincoln said, "Once a cat's been burned,
      he won't even sit on a cold stove."
    </p>
    <p>
      To be accepted, Marpa needed to allow
      procedure parsing,
      not just declarative parsing.
      So Marpa allows the user to specify events --
      occurrences of symbols and rules --
      at which declarative parsing pauses.
      While paused,
      the application can call procedural logic
      and single-step forward token by token.
      The procedural logic can hand control back
      over to syntax-driven parsing at any point it likes.
      The Earley tables can provide the procedural logic with
      full knowledge of the state of the
      parse so far:
      all rules recognized
      in all possible parses so far,
      and all symbols expected.
      Earley's algorithm is now a even better companion
      for hand-written procedural logic than recursive descent.
    </p>
    <h3>For more</h3>
    <p>
      For more about Marpa,
      there's
      <a href=http://savage.net.au/Marpa.html">the
      official web site maintained by Ron Savage</a>.
      I also have
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">a Marpa web site</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 16:50 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/09/chron.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 01 Sep 2014</h3>
<br />
<center><a name="website"> <h2>Marpa has a new web page</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Marpa has
      <a href="http://savage.net.au/Marpa.html">a
      new official public website</a>,
      which Ron Savage has generously agreed to manage.
      For those who have not heard of it,
      Marpa is a parsing algorithm.
      It is new, but very much based
      on earlier work by Jay Earley, Joop Leo, John Aycock and R. Nigel Horspool.
      Marpa is intended to replace, and to go well beyond,
      recursive descent and the yacc family of parsers.
    </p><ul>
      <li>
        Marpa is fast. It parses in linear time:
        <ul>
          <li>all the grammar classes that recursive descent parses;</li>
          <li>the grammar class that the yacc family parses;</li>
          <li>in fact, all unambiguous grammars, as long as they are free of unmarked middle recursions;
	  and</li>
	  <li>all
	  ambiguous grammars that are unions of a finite set of any of the above grammars.</li>
        </ul>
      </li>
      <li>
        Marpa is powerful. Marpa will parse anything that can be
	written in BNF.
	This includes any mixture of left, right and middle recursions.
      </li>
      <li>Marpa is convenient.
      Unlike recursive descent, you do not have to write a parser --
      Marpa generates one from BNF.
      Unlike PEG or yacc, parser generation is unrestricted and exact.
      Marpa converts any grammar which can be written as BNF
      into a parser which recognizes everything
      in the language described by that BNF, and which rejects everything that is
      not in that language.
      The programmer is not forced to make arbitrary choices while parsing.
      If a rule has several alternatives,
      all of the alternatives are considered for as long as they might yield a valid parse.
      </li>
      <li>
        Marpa is flexible. Like recursive descent, Marpa allows you to stop and
        do your own custom processing. Unlike recursive descent, Marpa makes available
        to you detailed information about the parse so far --
        which rules and symbols have been recognized, with their locations,
        and which rules and symbols are expected next.
      </li>
      </ul>
    <h3>Comments</h3>
    <p>
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>.
    </p>
  </body>
</html>
<br />
<p>posted at: 20:17 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2014/09/website.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
