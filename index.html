<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Fri, 05 Jul 2013</h3>
<br />
<center><a name="lovelace"> <h2>Parsing Ada Lovelace</h2> </a>
</center>
  <h3>The application</h3><!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      <p>Marpa, my parser project, has long allowed users to generate and examine all
      the parses of an ambiguous parse.
      This was more than enough for most applications.
      <p>
      </p>
      But not all.
      Some users need to manipulate very large sets of ambiguous parses,
      and do it efficiently.
      My new work with Marpa allows the creation of an Abstract Syntax Forest (ASF)
      An ASF is, as the name suggests, a collection of Abstract Syntax Trees.
      <p>
      </p>Writing an efficient ASF in not a simple matter.
      The naive implementation, as a set of fully expanded
      AST's, consumes resource that are
      exponential in the size of the input,
      and in practice quickly becomes unuseable.
      Marpa takes advantage of the fact that,
      in fully expanded AST's,
      many of the subtrees will be identical.
    <h3>Ada Lovelace's Quote</h3>
    <p>Natural Language Processing (NLP) is
    one application that requires working with highly ambiguous parses.
    I picked as an example to work with,
    an quote from Ada Lovelace.
    It have proven very interesting from a number of points of view.
    </p>
    <p>
    Ada, Countess of Lovelace, wrote the first paper
    that separates clearly separates computers and software, on one hand,
    from caculators and hardware, on the other.
    The paper makes clear she know the importance of this separation.
    Ada was working with Charles Babbage,
    who did all of the hardware design,
    and most of the actual coding in her paper.
    But the ideas in her paper are hers,
    not Babbage's.
    <p>
    Why would Babbage
    ignore obvious implications of his
    own invention?
    The answer is that,
    while these implications are obvious to us,
    they simply did not fit into Babbage's view of the world,
    and the view held by almost everyone in his society then.
    In 1843, the most significant aspect of the world was your duty to embrace
    Good and avoid Evil.
    In one form or another,
    pretty much everyone sees that as still the case today,
    But if you have the duty to avoid Evil,
    it is because you have Reason,
    which allows you to distinguish Good from Evil.
    Once once starts talking about machines which not only
    calculate, but manipulate the equations they use to calculate
    and compose music -- well, you were getting way too
    Mary Shelley for a lot of people.
    </p>
    <p>
    These days children, not long after they learn to talk,
    come to understand that games and game machines are two
    distinct things,
    and that while the game software is intangible,
    it is nonetheless a very real thing
    and necessary to "animate" the game machine.
    By the time modern children come to decide whether they has a soul
    or not, animated but soulless entities are part of their landscape.
    For Babbaage's generation, this was way too Mary Shelly.
    </p>
    <p>But Ada was the poet Byron's daughter.
    Pushing boundaries and shocking contemporaries
    was her inheritance.
    And mathematics were more her interest.
    Babbage was happy to delegate these complicated and unsettling matters
    to her.
    As <a href="http://www.fourmilab.ch/babbage/hpb.html">his son put it</a>,
    Babbage considered
    <blockquote>
    the Paper by Menabrea, translated with notes by Lady Lovelace,
    published in volume 3 of Taylor's 'Scientific Memoirs," as quite disposing of the mathematical aspect of the invention.
    </blockquote>
    <h3>On reading Ada</h3>
    <p>Ada's notes are worth reading and pondering,
    but they present many layers of difficulties to the modern reader.
    Among them:
    <ul>
    <li>They are in Victorian English.  Long, complex sentences were encouraged,
    and considered especially appropriate when making an difficult and important point.
    </li>
    <li>They are mathemetical
    </li>
    <li>Within math, their focus is on topics
    which are not important for modern computer programmers.
    </li>
    <li>Ada does not have her terminology ready-made.
    For example, she speaks of computing both symbolic results and numeric data,
    and attaching one to the other.
    She clearly understand that the symbolic results can represent operations.
    She also clearly understands that
    numeric data can
    represent not just numbers, but notes, positions in a loom, or computer operations.
    She does not speak of objects, classes or methods.
    Is this because she does not have the idea,
    or because the terminology does not yet exist?
    </li>
    <li>Where her terminology is ready-made, it is now archaic.
    </li>
    <li>She connects mathematics and philosophy in a way that was not just understood then,
    but expected and demanded.
    Unfortunately, modern readers now often see that sort of discussion as
    irrelevant, or even as a sign of inability to come to the point.
    </li>
    </ul>
    <p>
    <h3>The quote</h3>
<blockquote>
Those who view mathematical science,
not merely as a vast body of abstract and immutable truths,
whose intrinsic beauty,
symmetry and logical completeness,
when regarded in their connexion together as a whole,
entitle them to a prominent place in the interest of all profound
and logical minds,
but as possessing a yet deeper interest for the human race,
when it is remembered that this science constitutes the language
through which alone we can adequately express the great facts of
the natural world,
and those unceasing changes of mutual relationship which,
visibly or invisibly,
consciously or unconsciously to our immediate physical perceptions,
are interminably going on in the agencies of the creation we live amidst:
those who thus think on mathematical truth as the instrument through
which the weak mind of man can most effectually read his Creator's
works,
will regard with especial interest all that can tend to facilitate
the translation of its principles into explicit practical forms.
</blockquote>
<h3>Ada, the bullet point version</h3>
<p>158 words!  Ada's original sentence may look what happens
when two pickups taking
out-of-date dictionaries to the landfill collide.
There is, in fact, a good deal of structure and meaning in all those
words.
First, let take it as bullet points:
<ul>
<li>1. Math is awesome just for being itself.
<li>2. Math describes and predicts reality.
<li>3. No better way to figure out what is really behind our
existence.
<li>4. We have found a way to do more and better math.
Let's go for it.
</ul>
Ada is in fact quickly connecting her new science of software to
the history of philosophy in the West.
Bullet point 1 alludes to the Platonist view of mathematics.
Bullet point 2 alludes to the scientific one pioneered by
Galileo and Newton.
Bullet point 3 connects it with a post-Classical world view.
Ada's language is Christian but her idea here is one that Einstein
would have had no trouble with.
And bullet point 4 is the call to action.
<p>
When we come to discuss the parse in detail,
we'll see that it follows this structure.
As an aside,
note Ada' mention of "logical completeness" as one of the virtues of math.
G&ouml;del came along nearly a century later and showed this vision,
which went back to the Greeks, was an illusion.
So Ada did not predict everything.
On the other hand, 
G&ouml;del's result was also a complete surprise to Johnny von Neumann,
who was in the room that day.
    <h3>To learn more</h3>
    <p>
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2
        is available on CPAN</a>.
      A list of my Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
        here</a>.
      There is
      <a href="http://marpa-guide.github.io/chapter1.html">
        a new tutorial by Peter Stuifzand</a>.
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        The Ocean of Awareness blog</a>
      focuses on Marpa,
      and it has
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>.
      Marpa also has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page</a>.
      For questions, support and discussion, there is a
      Google Group:
      <code>marpa-parser@googlegroups.com</code>.
      Comments on this post can be made there.
    </p>
<br />
<p>posted at: 11:37 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/07/lovelace.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 17 Jun 2013</h3>
<br />
<center><a name="vs-prd-round-2"> <h2>Marpa v. Parse::RecDescent: a rematch</h2> </a>
</center>
  <h3>The application</h3><!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
--><p>
      In
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/06/mixing-procedural.html">
      a recent post</a>,
      I looked at an unusual language which serializes arrays and strings,
      using a mixture of counts and parentheses.  Here is an example:
    </p>
<blockquote><pre>
A2(A2(S3(Hey)S13(Hello, World!))S5(Ciao!))
</pre></blockquote>
    <p>
      The language is of special interest for comparison
      against recursive descent
      because, while simple, it requires procedural
      parsing -- a purely declarative BNF approach will not work.
      So it's a chance to find out if Marpa can play the game that is recursive descent's
      specialty.
      </p>
      <p>
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/06/mixing-procedural.html">
      The previous post</a>
      focused on how to use Marpa to mix
      procedural and declarative parsing together smoothly,
      from a coding point of view.
      It only hinted at another aspect: speed.
    Over the last year, Marpa has greatly improved its speed for this kind of application.
      The latest release of Marpa::R2 now clocks in almost 100 times faster than Parse::RecDescent for long inputs.
    </p>
    <h3>The benchmark</h3>
    <table align="center" cellpadding="5" border="1" width="100%">
      <tbody><tr><th rowspan="2">Length</th><th colspan="3">Seconds</th></tr>
        <tr>
          <th>Marpa::R2</th>
          <th>Marpa::XS</th>
          <th>Parse::RecDescent
          </th></tr>
        <tr><td>1000
          </td><td align="center">1.569
          </td><td align="center">2.938
          </td><td align="center">13.616
          </td></tr>
        <tr><td>2000
          </td><td align="center">2.746
          </td><td align="center">7.067
          </td><td align="center">62.083
          </td></tr>
        <tr><td>3000
          </td><td align="center">3.935
          </td><td align="center">13.953
          </td><td align="center">132.549
          </td></tr>
        <tr>
          <td>10000
          </td><td align="center">12.270
          </td><td align="center">121.654
          </td><td align="center">1373.171
          </td></tr>
      </tbody></table>
    <p>Parse::RecDescent is pure Perl, while Marpa is based on a parse
      engine in a library written in
      hand-optimized C.
      You'd expect Marpa to win this race and it did.
    </p>
    <p>
      And it is nice to see that the changes from Marpa::XS to Marpa::R2 have paid off.
      Included in the table are the Marpa numbers from my
      2012 benchmark of Marpa::XS.
      Marpa::R2 has a new interface
      and an internal lexer,
      and now beats Marpa::XS by a factor of up to 10.
      </p>
      <p>
      While the benchmarked language is ideally suited to show recursive descent to
      advantage, the input lengths were picked to emphasize Marpa's strengths.
      Marpa optimizes by doing a lot of precomputation,
      and is written with long inputs in mind.
      Though these days, a 500K source,
      longer than the longest tested, would not exactly set a new industry record.
    </p>
    <h3>To learn more</h3>
    <p>
      There are fuller descriptions of the language in
      <a href="http://blogs.perl.org/users/polettix/2012/04/parserecdescent-and-number-of-elements-read-on-the-fly.html">
        Flavio's post and code</a>,
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/06/mixing-procedural.html">
      and my recent post on how to write a parser for this language</a>.
      I talk more about the benchmark's methodology in
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2012/04/marpa-v-parserecdescent-some-numbers.html">
      my post on the 2012 benchmark</a>.
    </p>
    <p>
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2
        is available on CPAN</a>.
      A list of my Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
        here</a>.
      There is
      <a href="http://marpa-guide.github.io/chapter1.html">
        a new tutorial by Peter Stuifzand</a>.
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        The Ocean of Awareness blog</a>
      focuses on Marpa,
      and it has
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>.
      Marpa also has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page</a>.
      For questions, support and discussion, there is a
      Google Group:
      <code>marpa-parser@googlegroups.com</code>.
      Comments on this post can be made there.
    </p>
<br />
<p>posted at: 06:47 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/06/vs-prd-round-2.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 10 Jun 2013</h3>
<br />
<center><a name="mixing-procedural"> <h2>Mixing procedural and declarative parsing gracefully</h2> </a>
</center>
  <h3>Declarative and procedural parsing</h3>
    <p><!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
-->A declarative parser
      takes a description of your language and parses it
      for you.
      On the face of it, this sounds like the way you'd want
      to go,
      and Marpa offers that possibility -- it generates
      a parser from anything you can write in BNF and,
      if the parser is in one of the classes currently in
      practical use,
      that parser will run in linear time.
    </p>
    <p>
      But practical grammars often have context-sensitive parts --
      features which cannot be described in BNF.
      Nice as declarative parsing may sound,
      at least
      <b>some</b>
      procedural parsing
      can be a necessity
      in real-life.
      In this post, I take a problem for which procedural
      parsing is essential,
      and create a fast, short solution that
      mixes procedural and declarative.
    </p>
    <h3>The application</h3>
    <p>This is a sample of the language:
    </p>
    <blockquote>
      <pre>
A2(A2(S3(Hey)S13(Hello, World!))S5(Ciao!))
</pre>
    </blockquote>
    <p>
      It describes strings in nested arrays.
      The strings are introduced by the letter 'S', followed by a length count and then,
      in parentheses, the string itself.
      Arrays are introduced by the letter 'A' followed by an element count and, inside parentheses, the
      array's contents.
      These contents are a concatenated series of strings and other arrays.
      I call this a Dyck-Hollerith language because it
      combines
      <a href="http://en.wikipedia.org/wiki/Hollerith_constant">
        Hollerith constants</a>
      (strings preceded by a count),
      with balanced parentheses
      (what is called
      <a href="http://en.wikipedia.org/wiki/Dyck_language">
        a Dyck language</a>
      by mathematicians).
    </p>
    <p>
      The language is one I've dealt with before.
      It is apparently from "real life", and is described more fully
      in
      <a href="http://blogs.perl.org/users/polettix/2012/04/parserecdescent-and-number-of-elements-read-on-the-fly.html">
        a blog post by
        Flavio Poletti</a>.
      Several people, Gabor Szabo among them, prodded me to show how
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">
        Marpa</a>
      would do on this language.
      I did this
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2012/04/marpa-v-parserecdescent-some-numbers.html">a year ago, using Marpa's previous version, Marpa::XS</a>.
      The result was well-received and quite satisfactory.
    </p>
    <p>
      This time around, I used
      Marpa's latest version, Marpa::R2,
      and its new interface, the SLIF.
      The solution presented here was
      much easer to write,
      and will be easier to read.
      It is also several times faster.
    </p>
    <h3>The code</h3>
    <p>The full code for this example is in
      <a href="https://gist.github.com/jeffreykegler/5745272">
        a Github gist</a>.
      In what follows, I will assume the reader is
      interested in the ideas.
      Details of the interface,
      along with more detail-oriented tutorials,
      can be found
      <a href="https://metacpan.org/module/Marpa::R2">
        in Marpa's documentation</a>.
      Other tutorials are
      on
      <a href=http://jeffreykegler.github.io/Ocean-of-Awareness-blog/">
        the Ocean of Awareness blog</a>,
      and on
      <a href="http://marpa-guide.github.io/index.html">
        the Marpa Guide,
        a new website</a>
      being
      built due to the generosity of Peter Stuifzand
      and Ron Savage.
    </p>
    <h3>The DSL</h3>
    <p>First off, let's look at the declarative part.
      The core of the parser is the following lines,
      containing the BNF for the language's top-level structure.
    </p>
    <blockquote>
      <pre>
my $dsl = &lt;&lt;'END_OF_DSL';
# The BNF
:start ::= sentence
sentence ::= element
array ::= 'A' &lt;array count&gt; '(' elements ')'
    action =&gt; check_array
string ::= ( 'S' &lt;string length&gt; '(' ) text ( ')' )
elements ::= element+
  action =&gt; ::array
element ::= string | array
</pre>
    </blockquote>
    <p>Details of this syntax are in Marpa's documentation,
      but it's a dialect of EBNF.
      Adverbs like
      <tt>action =&gt; semantics</tt>
      tell Marpa what the semantics will be.
      The default (which will be set below) is for a rule to return its first child.
      <tt>::array</tt>
      semantics tell Marpa to return all every
      <tt>element</tt>
      of
      <tt>elements</tt>
      in an array.
      And
      <tt>check_array</tt>
      is the name of a function providing
      the semantics, as will be seen below.
    </p>
    <p>
      Single-quoted strings are looked for literally in the input.
      In the
      <tt>string</tt>
      declaration,
      you'll note some parentheses which are not in quotes.
      The unquoted parentheses are part of the Marpa DSL's own syntax,
      telling Marpa to "hide" the parenthesized symbols from the
      semantics.
      Here, the effect is that
      <tt>text</tt>
      is treated by the semantics as if it
      were the "first" symbol.
    </p>
    <p>Marpa's SLIF provides a lexer for the user,
      and this Marpa-internal lexer will handle most
      of the symbols in this example.
      The single-quoted strings we saw in the BNF are actually instructions
      to the internal lexer.
      The next lines tell Marpa how to recognize
      <tt>&lt;array count&gt;</tt>
      and
      <tt>&lt;string length&gt;</tt>.
    </p><blockquote>
      <pre>
&lt;array count&gt; ~ [\d]+
&lt;string length&gt; ~ [\d]+
text ~ [\d\D]
END_OF_DSL
</pre>
    </blockquote>
    <p>
      <tt>&lt;array_count&gt;</tt>
      and
      <tt>&lt;string length&gt;</tt>
      are both declared to be a series of digits.
      <tt>text</tt>
      is a stub.
      The length of
      <tt>text</tt>
      depends on the numeric value of
      <tt>&lt;string length&gt;</tt>, and dealing with that is beyond the power of
      the BNF.
      When it comes time to count out the symbols needed for
      <tt>text</tt>,
      we will hand control over to an external lexer.
      For the purposes of Marpa's lexer,
      <tt>text</tt>
      is described
      as a single character of any kind.
      Marpa's internal scanner uses a longest tokens match algorithm,
      and since we don't want the internal scanner to read
      <tt>text</tt>
      lexemes,
      describing
      <tt>text</tt>
      and other purely external lexemes
      as single characters is the right thing to do.
    </p>
    <p>Now comes the weld between declarative and procedural ...
    </p>
    <blockquote>
      <pre>
:lexeme ~ &lt;string length&gt; pause =&gt; after
:lexeme ~ text pause =&gt; before
</pre>
    </blockquote>
    <p>These two statements tell Marpa that
      <tt>&lt;string length&gt;</tt>
      and
      <tt>&lt;text&gt;</tt>
      are two lexicals at which Marpa's own parsing should "pause",
      handing over control to external procedural parsing logic.
      In the case of
      <tt>&lt;string length&gt;</tt>,
      the pause should be after it is read.
      In the case of
      <tt>&lt;text&gt;</tt>
      the pause should be before.
      What happens during the "pause", we will soon see.
    </p>
    <h3>Starting the parse</h3>
    <p>Next follows the code to read the DSL,
      and start the parser.
    </p><blockquote>
      <pre>
my $grammar = Marpa::R2::Scanless::G-&gt;new(
    {   action_object  =&gt; 'My_Actions',
        default_action =&gt; '::first',
        source         =&gt; \$dsl
    }
);

my $recce = Marpa::R2::Scanless::R-&gt;new( { grammar =&gt; $grammar } );
</pre>
    </blockquote>
    <p>The previous lines tell Marpa that when its semantics
      are provided by a Perl closure, it is to look for that closure in a package called
      <tt>My_Actions</tt>.
      The default semantics are
      <tt>::first</tt>, which means simply pass the value of
      the first RHS symbol of a rule upwards.
    </p>
    <h3>The main loop</h3>
    <p>We saw our input above:</p>
    <blockquote>
      <pre>
$input = 'A2(A2(S3(Hey)S13(Hello, World!))S5(Ciao!))';
</pre>
    </blockquote>
    <p>The block of code which follows is the main loop through the parse, including
      all the procedural parsing logic.
      Below, I will pull this
      procedural parsing logic out of the loop
      for separate examination.
    </p>
    <p>
      Here the
      <tt>$recce-&gt;read()</tt>
      method performs the first read
      and sets up the input string.
      The interior of the loop is entered whenever Marpa "pauses".
      Once the procedural parsing logic is done, Marpa resumes with
      the
      <tt>$recce-&gt;resume()</tt>
      call.
      Throughout,
      <tt>$pos</tt>
      is used to track the current character
      in the input stream.
      The loop ends when
      <tt>$pos</tt>
      is after the last character of
      <tt>$input</tt>.
    </p>
    <blockquote>
      <pre>
my $last_string_length;
my $input_length = length $input;
INPUT:
for (
    my $pos = $recce-&gt;read( \$input );
    $pos &lt; $input_length;
    $pos = $recce-&gt;resume($pos)
    )
{
    my $lexeme = $recce-&gt;pause_lexeme();
    die q{Parse exhausted in front of this string: "},
        substr( $input, $pos ), q{"}
        if not defined $lexeme;
    my ( $start, $lexeme_length ) = $recce-&gt;pause_span();
    if ( $lexeme eq 'string length' ) {
        $last_string_length = $recce-&gt;literal( $start, $lexeme_length ) + 0;
        $pos = $start + $lexeme_length;
        next INPUT;
    }
    if ( $lexeme eq 'text' ) {
        my $text_length = $last_string_length;
        $recce-&gt;lexeme_read( 'text', $start, $text_length );
        $pos = $start + $text_length;
        next INPUT;
    } ## end if ( $lexeme eq 'text' )
    die "Unexpected lexeme: $lexeme";
} ## end INPUT: for ( my $pos = $recce-&gt;read( \$input ); $pos &lt; $input_length...)
</pre>
    </blockquote>
    <h3>The procedural parsing</h3>
    <p>In this language,
      we need the procedural parsing logic to count the
      <tt>text</tt>
      strings properly.
      This is done in a very direct way.
      First we pull the count from
      <tt>&lt;string length&gt;</tt>:
    </p><blockquote>
      <pre>
    if ( $lexeme eq 'string length' ) {
        $last_string_length = $recce-&gt;literal( $start, $lexeme_length ) + 0;
        $pos = $start + $lexeme_length;
        next INPUT;
    }
</pre>
    </blockquote>
    <p>
      Above, we used
      <tt>pause_span()</tt>
      to set
      <tt>$start</tt>
      and
      <tt>$lexeme_length</tt>
      to the start and length of the lexeme that
      Marpa's internal scanner found.
      Passed to
      <tt>$recce-&gt;literal()</tt>, these two values
      return the "literal" string value of the lexeme, which will
      be the ASCII representation of a decimal number.
      We convert it to numeric, salt it away in
      <tt>$last_string_length</tt>,
      and set
      <tt>$pos</tt>
      to the location just after the
      <tt>&lt;string length&gt;</tt>
      lexeme.
    </p>
    <blockquote>
      <pre>
    if ( $lexeme eq 'text' ) {
        my $text_length = $last_string_length;
        $recce-&gt;lexeme_read( 'text', $start, $text_length );
        $pos = $start + $text_length;
        next INPUT;
    } ## end if ( $lexeme eq 'text' )
</pre>
    </blockquote>
    <p>Now we come to counting out the characters for the
      <tt>text</tt>
      lexeme.
      Recall that in the case of
      <tt>text</tt>, we pause
      <b>before</b>
      the lexeme, which means it will not have been read yet.
      With
      <tt>$recce-&gt;lexeme_read()</tt>, we tell Marpa
      that we want the next lexeme
    </p>
    <ul>
      <li>to be of type
        <tt>text</tt>,
      </li>
      <li>to start at the already decided
        <tt>$start</tt>
        position, and
      </li>
      <li>
        to be of the length that
        we saved in
        <tt>$last_string_length</tt>.
      </li>
    </ul>
    <p>
      We also set
      <tt>$pos</tt>
      to be just after the
      end of the lexeme.
    </p>
    <p>We've focused on the string lengths, but the Dyck-Hollerith language has
      a count of the number of elements in its array.
      Marpa's BNF-driven parsing logic has no trouble
      determining the number of elements from the array contents,
      and it does not need the count.
      What to do with it?
    </p>
    <blockquote>
      <pre>
package My_Actions;
</pre>
    </blockquote>
    <blockquote>
      <pre>
sub check_array {
    my ( undef, undef, $declared_size, undef, $array ) = @_;
    my $actual_size = @{$array};
    warn
        "Array size ($actual_size) does not match that specified ($declared_size)"
        if $declared_size != $actual_size;
    return $array;
} ## end sub check_array
</pre>
    </blockquote>
    <p>Recall that Marpa promised special semantics for the
      <tt>array</tt>
      rule
      in its DSL.
      Here they are.
      The first parameter to Marpa's semantic closures is a per-parse variable, here unused.
      The rest are the values of the RHS symbols, in order.
      We only care about the second (for
      <tt>&lt;array count&gt;</tt>),
      and the fourth (for
      <tt>elements</tt>).
      We determine a
      <tt>$declared_size</tt>
      from
      <tt>&lt;array count&gt;</tt>;
      and an
      <tt>$actual_size</tt>
      by looking at the array referenced by
      <tt>$array</tt>.
      If these differ, we choose to warn the user.
      Depending on your purposes,
      anything from ignoring the issue
      to throwing a fatal error may be equally or more reasonable.
    </p>
    <h3>The result of the the parse</h3>
    <p>And now we are ready to take the result of the parse.
    </p><blockquote>
      <pre>
my $result = $recce-&gt;value();
die 'No parse' if not defined $result;
</pre>
    </blockquote>
    <h3>For more about Marpa</h3>
    <p>The techniques described apply to problems considerably
      larger than the example of this post.
      Jean-Damien Durand is using them to create
      <a href="https://github.com/jddurand/MarpaX-Languages-C-AST">
        a C-to-AST tool</a>.
      This
      takes C language and converts it to an AST,
      following the C11
      specification carefully.
      The AST can then be manipulated
      as you wish.
    </p
    <p><p>
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2
        is available on CPAN</a>.
      A list of my Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
        here</a>.
      There is
      <a href="http://marpa-guide.github.io/chapter1.html">
        a new tutorial by Peter Stuifzand</a>.
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        The Ocean of Awareness blog</a>
      focuses on Marpa,
      and it has
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>.
      Marpa also has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page</a>.
      For questions, support and discussion, there is a
      Google Group:
      <code>marpa-parser@googlegroups.com</code>.
      Comments on this post can be made there.
    </p>
<br />
<p>posted at: 07:17 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/06/mixing-procedural.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Fri, 31 May 2013</h3>
<br />
<center><a name="design_of_4"> <h2>The Design of Four</h2> </a>
</center>
  <p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      In the Perl world at this moment,
      a lot is being said about the consequences of bad design.
      And it is useful to study design failures.
      But the exercise will come to nothing
      without a road to good design.
      This post will point out four
      Perl-centric
      projects that
      are worth study as models
      of good design.
    </p>
    <p>
      The projects are
      <a href="https://metacpan.org/release/ack">
      ack</a>,
      <a href="https://metacpan.org/release/App-cpanminus">
      cpanm</a>,
      <a href="https://metacpan.org/release/local-lib">local::lib</a> and
      <a href="https://metacpan.org/release/App-perlbrew">perlbrew</a>.
      Each of these is perfect
      in the older sense of "having all that is requisite to its nature and kind"
      (<a href="http://1828.mshaffer.com/d/word/perfect">Webster's 1828</a>).
	If you are into Perl,
	they are all widely useful,
	and looking at them as a potential or an actual user
	is the best way to gain an appreciation of the art behind them.
    </p>
    <p><tt>ack</tt> is a file search tool -- UNIX's grep with improvements.
      The improvements are influenced by a Perl sensibility,
      and <tt>ack</tt> is written in Perl.
      But while the other tools I list are of little interest unless
      you are into Perl,
      <tt>ack</tt> can help you out even if you otherwise shun Perl tools.
    </p>
    <p>
      <tt>cpanm</tt> is for installing CPAN packages from CPAN.
      If you don't know what that means, you aren't interested.
      If you do, you want to be using it.
      It does everything you need and importantly, nothing more.
      The interface is without clutter.
      Like I said, perfect.
    </p>
    <p>
      <tt>local::lib</tt> is for installing Perl modules in the directory of your choice.
      Even if you have root permission on a system,
      it is good practice to leave the delivered Perl on your system
      untouched except by vendor-sponsored patches and updates.
      <tt>local::lib</tt> allows you to do this easily and conveniently.
      It has every feature and convenience I want.
      And reading its documentation is again an encounter with
      perfection.
      Every feature described is
      <ul>
      <li>something that you need today,
      <li>something that you are worried you might need tomorrow, or
      <li>something that you are not worried you might need,
      but on reading the documentation will discover that you should be.
      </ul>
      Aside from that, there is nothing else.
      Clutter-free.  Perfect.
    </p>
    <p>
      Repeated perfection can be boring,
      a fact which
      I suspect plays no small role
      in making perfection an unusual thing in this world.
      So of <tt>perlbrew</tt>,
      I will simply say that it does for Perl versions and executables
      what <tt>local::lib</tt> does for Perl modules.
      <tt>perlbrew</tt> is the way to manage alternative Perl executables.
      And using <tt>perlbrew</tt>
      is a good way to study yet another perfect interface.
    </p>
    <p>
    How much relevance does the work of
      Andy Lester (<tt>ack</tt>),
      Tatsuhiko Miyagawa (<tt>cpanm</tt>),
      Matt S Trout (<tt>local::lib</tt>)
      and Kang-min Liu (<tt>perlbrew</tt>) have to other projects,
      including projects that now seem larger and more complex?
      Certainly
      these four applications all seem simple, well-defined,
      and self-contained.
      But I would argue that,
      if these problems seem simple and well-defined today,
      much of that impression is the result of the design skills
      of Andy, Tatsuhiko, Matt and Kang-min.
      And if, to an extent, they did benefit from
      having the good fortune to pick the the right problem at
      the right time,
      it is useful to recall
      Ben Hogan's comment on his profession:
    </p>
    <blockquote>Golf is a game of luck.
      The more I practice, the luckier I get.
    </blockquote>
<br />
<p>posted at: 07:20 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/05/design_of_4.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 27 May 2013</h3>
<br />
<center><a name="table"> <h2>Why Marpa works: table parsing</h2> </a>
</center>
  <p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Marpa works very differently from the parsers
      in wide use today.
      Marpa is a table parser.
      And Marpa is unusual among table parsers --
      its focus is on speed.
    </p>
    <p>
      The currently favored parsers use stacks,
      in some cases together with a state machine.
      These have the advantage that it is easy
      to see how they can be made
      to run fast.
      They have the disadvantage of severely limiting
      what the parser can do and how much it can know.
    </p>
    <h3>What is table parsing?</h3>
    <p>
      Table parsing means parsing by constructing a table of all the possible parses.
      This is pretty clearly something you want -- anything less means
      not completely knowing what you're doing.
      It's like walking across the yard blindfolded.
      It's fine if you can make sure there are
      no walls, open pits, or other dangerous obstacles.
      But for the general case,
      it's a bad idea.
    </p>
    <p>
      Where the analogy between walking blindfolded and parsing breaks
      down is that while taking off the blindfold has no cost,
      building a table of everything that is happening while you parse
      <b>does</b>
      have a cost.
      If you limit your parser to a stack and a state machine,
      you may be parsing with a blindfold on,
      but it is clear that your parser can be fast.
      How to make a table parser run fast
      is not so clear.
    </p>
    <h3>The advantages of table parsing</h3>
    <p>
      What are the advantages of taking off the blindfold?
      First, your parser can be completely general --
      anything you can write in BNF it can parse.
      And second,
      you always know exactly what is going on -- what rules
      are possible, what rules have been recognized,
      how far into a rule you have recognized it,
      what symbols you expect next, etc. etc.
    </p><p>
    </p><p>
      We programmers have gotten used to parsers which run blindfolded.
      When you bump into something while
      blindfolded you don't know
      what it was.
      When non-table parsers fail, they usually don't know why --
      they can only guess.
      If you have a full parse table,
      built from left to right,
      you know what you were looking for and what you already think you
      found.
      This means that you can pinpoint and identify errors precisely.
    </p>
    <p>
      Knowing where you are in a parse also allows certain tricks,
      like the one I call the Ruby Slippers.
      In this, you parse with an over-simplified grammar and,
      when it fails, you ask the parser what it was looking for.
      Then -- poof! -- you provide it.
    </p>
    <p>
      The Ruby Slippers work beautifully when dealing with
      missing tags in HTML.
      You can define a simplified HTML grammar,
      one that lives in a non-existent world --
      an ideal world where all start
      and end tags are always where they belong.
      Then you parse away.
      If, as will happen with real-world input, a tag is missing,
      you ask the table parser what it was looking for,
      and give it a virtual tag.
    </p>
    <h3>And as for fast ...</h3>
    <p>When I decided to write Marpa in 2007 my goal was to create a table parser
      that was as fast as possible.
      I was surprised to find that the academic literature contained a
      major improvement to table parsing by Joop Leo,
      an improvement which nobody had ever made a serious attempt to implement.
      Marpa is the first implementation of Joop Leo's 1991 improvement to table parsing which,
      as far as theory goes,
      makes Marpa as fast any parser
      in practical use today.
      Any class of grammar that
      recursive descent, bison, etc. will parse,
      Marpa will parse in linear time.
    </p>
    <p>
      Table parsing has had a reputation for being slow due to a
      bad "constant factor".
      Theoreticians, when looking at speed as time complexity,
      throw away constant factors.
      What's left once the constant factor is ignored is always more
      important.
      Surprisingly often,
      time complexity results which ignore
      constant factors are also the most
      meaningful results in practical terms.
    </p><p>
      But not 100% of the time.
      Sometimes the constant factor makes all the difference.
      And deciding when a constant factor does make a difference,
      and when it does not,
      is a tricky matter,
      one that lies in that murky zone between practice and theory
      where neither practitioner or theorist feels fully at home.
    </p><p>
    </p>
    <p>
      It's time to look again, for two reasons.
      First, Aycock and Horspool did a lot of careful work on
      reducing
      the constant factor for table parsing,
      work which Marpa incorporates
      and builds on.
      Second,
      the judgment about the constant factor dates from 1968,
      when computers were literally a billion
      times slower then they are now.
    </p><p>
    </p><p>
      Why has nobody re-examined this judgment as the years and the order
      of magnitude speed-ups marched by?
      When a judgment crosses sub-disciplines, it can be "sticky"
      beyond all reason,
      and this one is a good example.
      The decision that its "constant factor" made table parsing
      too slow for many practical applications
      is in part a practical take on a theoretical issue,
      and in part a theoretical call on a practical matter.
    </p><p>
    </p><p>
      Since 1968,
      the theoretical results have improved.
      But the theoreticians did not change
      their mind about the speed of table parsing
      because it was a judgment about the practical application
      of the theory.
      The practitioners were actually out there writing compilers.
      When it comes down to practice,
      you have to assume that practitioners know what they
      are talking about, right?
    </p><p>
    </p><p>
      Meanwhile the practice of writing software underwent
      revolution after revolution.
      But the practitioners continued to write off table parsing
      as impractical.
      Talking about the speed of table parsers quickly got you
      into some very heavy math.
      And some of the algorithms
      did not even exist except as mathematical
      notation on the pages
      of the journals and textbooks.
      When it comes down to theory about things
      that don't exist outside of theory,
      who do you listen to if not
      the theoreticians?
    </p>
    <p>
      I look carefully at the issue
      of the "constant factor" in
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/04/fast_enough.html">
        a previous post</a>.
      Forty-five years have passed and
      cost of CPU has fallen
      nine orders of magnitude.
      (Others say the cost of CPU has dropped by 50% a year,
      in which case it's over 14 orders of magnitude.
      But why quibble?)
      It's reasonable to suspect that
      the constant factor that practitioners
      and theoreticians were worried about in 1968
      stopped being a
      major obstacle many years ago.
    </p>
    <h3>For more about Marpa</h3>
    <p>
      Marpa's latest version is
      <a href="https://metacpan.org/module/Marpa::R2">Marpa::R2,
        which is available on CPAN</a>.
      A list of my Marpa tutorials can be found
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html#TUTORIAL">
        here</a>.
      There is
      <a href="http://marpa-guide.github.io/chapter1.html">
        a new tutorial by Peter Stuifzand</a>.
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/">
        This blog</a>
      focuses on Marpa,
      and it has
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/metapages/annotated.html">an annotated guide</a>.
      Marpa also has
      <a href="http://jeffreykegler.github.com/Marpa-web-site/">a web page</a>.
      For questions, support and discussion, there is a
      Google Group:
      <code>marpa-parser@googlegroups.com</code>.
      Comments on this post can be made there.
    </p>
<br />
<p>posted at: 01:37 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2013/05/table.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
