<html>
<head>
<link rel="alternate" title="Ocean of Awareness RSS" type="application/rss+xml" title="RSS" href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/index.rss" />
<title>Ocean of Awareness</title>
<style type="text/css">
   strong {font-weight: 700;}
</style>
</head>
<body>
<div
  style="color:white;background-color:#38B0C0;padding:1em;clear:left;text-align:center;">
<h1>Ocean of Awareness</h1>
</div>
  <div style="margin:0;padding:10px 30px 10px 10px;width:150px;float:left;border-right:2px solid #38B0C0">
  <p>
  <strong>Jeffrey Kegler's blog</strong>
  about Marpa, his new parsing algorithm,
    and other topics of interest</p>
  <p><a href="http://www.jeffreykegler.com/">Jeffrey's personal website</a></p>
      <p>
	<a href="https://twitter.com/jeffreykegler" class="twitter-follow-button" data-show-count="false">Follow @jeffreykegler</a>
      </p>
      <p style="text-align:center">
	<!-- Place this code where you want the badge to render. -->
	<a href="//plus.google.com/101567692867247957860?prsrc=3" rel="publisher" style="text-decoration:none;">
	<img src="//ssl.gstatic.com/images/icons/gplus-32.png" alt="Google+" style="border:0;width:32px;height:32px;"/></a>
      </p>
  <h3>Marpa resources</h3>
  <p><a href="http://jeffreykegler.github.com/Marpa-web-site/">The Marpa website</a></p>
  <p>The Ocean of Awareness blog: <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog">home page</a>,
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/chronological.html">chronological index</a>,
  and
  <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/metapages/annotated.html">annotated index</a>.
  </p>
  </div>
  <div style="margin-left:190px;border-left:2px solid #38B0C0;padding:25px;">
<h3>Sun, 03 Jun 2018</h3>
<br />
<center><a name="knuth_1965"> <h2>Why is parsing considered solved?</h2> </a>
</center>
<html>
  <head>
  </head>
  <body style="max-width:850px">
    <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    <p>It is often said that parsing is a "solved problem".
    Given the level of frustration with the state of the art,
    the underuse of the very powerful technique of
    Language-Oriented Programming due to problematic tools<a id="footnote-1-ref" href="#footnote-1">[1]</a>,
    and the vast superiority of human parsing ability
    over computers,
    this requires explanation.
    </p>
    <p>
    On what grounds would someone say that parsing is "solved"?
    To understand this,
    we need to look at the history of Parsing Theory.<a id="footnote-2-ref" href="#footnote-2">[2]</a>
    In fact, we'll have to start decades before computer Parsing Theory
    exists,
    with a now nearly-extinct school of linguistics,
    and its desire to put the field on strictly
    scientific basis.
    </p>
    <h2>"Language" as of 1929</h2>
    <p>In 1929 Leonard Bloomfield,
      as part of his effort to create a linguistics that
      would be taken seriously as a science,
      published his "Postulates".<a id="footnote-3-ref" href="#footnote-3">[3]</a>
      The "Postulates" include his definition of language:
    </p><blockquote>
      The totality of utterances that can be made in a speech
      community is the
      <b>language</b>
      of that speech-community.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </blockquote><p>
      There is no reference in this definition to the usual view,
      that the utterances of a language "mean" something.
      This omission is not accidental:
    </p><blockquote>
      The statement of meanings is therefore the weak point in
      language-study, and will remain so until human knowledge
      advances very far beyond its present state. In practice, we define the
      meaning of a linguistic form, wherever we can, in terms of some
      other science.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </blockquote><p>
      Bloomfield is passing the buck,
      because the behaviorist science of his time rejects
      any claims about mental states as
      unverifiable statements -- essentially,
      as claims to be able to read minds.
      "Hard" sciences like physics, chemistry and even
      biology avoid dealing with unverifiable mental states.
      Bloomfield and the behaviorists want to make the methods of linguistics
      as close to hard science as possible.
    </p>
    <p>
      Draconian as Bloomfield's exclusion of meaning is,
      it is a big success.
      Known as structural linguistics,
      Bloomfield's approach dominates lingustics for
      the next couple of decades.
    </p>
    <h2>1955: Noam Chomsky graduates</h2>
    <p>
      Noam Chomsky earns his PhD at the Universtity of Pennsylvania.
      His teacher, Zelig Harris, is a prominent Bloomfieldian,
      and Chomsky's early work is thought to be in the Bloomfield school.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
      Chomsky becomes a professor at MIT.
      MIT does not have a linguistics department,
      and Chomsky is free to teach his own approach to the subject.
    </p>
    <h2>The term "language" as of 1956</h2>
    <p>Chomsky publishes his "Three models" paper,
      one of the most important papers of all time.
      His definition of language now uses the terminology
      of set theory,
      but its substance comes from Bloomfield:
    </p><blockquote>
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of sysbols.  If A is an alphabet, we shall say that
      anything formed by concatenating the symbols of A is a string in
      A. By a grammar of the language L we mean a device of some sort that
      produces all of the strings that are sentences of L and only these.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </blockquote>
    <p>
      But already in "Three Models",
      Chomsky readily brings in semantics,
      when it serves his purposes.
      For a semantically ambiguous utterance,
      Chomsky's new model produces multiple syntactic derivations.
      Each of these syntactic derivations
      "look" like the natural representation
      of one of the meanings,
      and Chomsky points out that this is a very
      desirable property for a model to have.<a id="footnote-8-ref" href="#footnote-8">[8]</a>
    </p>
    <h2>Chomsky 1959</h2>
    <p>In 1959, Chomsky reviews a book by B.F. Skinner's on linguistics.<a id="footnote-9-ref" href="#footnote-9">[9]</a>
    Skinner is the most prominent behaviorist of the time.
    </p>
    <p>
    Chomsky's review removes all doubt about where he stands
    on behaviorism
    or on the relevance of linguistics to the study of meaning.<a id="footnote-10-ref" href="#footnote-10">[10]</a>
    His review galvanizes the opposition to behaviorism, and
    Chomsky establishes himself as behavorism's most
    prominent and effective critic.
    </p>
    <p>
      In later years,
      Chomsky will make it clear that he had had no intention of
      following in the behaviorist tradition,
      by avoiding considerations of meaning, aka semantics:
    </p><blockquote>
      [...] it would be absurd to develop
      a general syntactic theory
      without assigning an absolutely
      crucial role to semantic considerations,
      since obviously the necessity to support
      semantic interpretation is one of the primary
      requirements
      that the structures
      generated by the syntactic component of a grammar
      must meet.<a id="footnote-11-ref" href="#footnote-11">[11]</a>
    </blockquote>
    <h2>Oettinger 1961</h2>
    <p>
      While the stack itself goes back to Turing<a id="footnote-12-ref" href="#footnote-12">[12]</a>,
      its significance for parsing becomes an object
      of interest in itself with
      Samuelson and Bauer's 1959 paper<a id="footnote-13-ref" href="#footnote-13">[13]</a>.
      Mathematical study of stacks as models of computing begins with Anthony Oettinger's 1961 paper.<a id="footnote-14-ref" href="#footnote-14">[14]</a></p>
    <p>Oettinger 1961 is full of evidence that stacks
      (which he calls "pushdown stores") are very new.
      Oettinger, for example, does not use the terms "push" or "pop",
      but instead describes operations on his pushdown stores using
      a set of vector operations which will later form the basis
      of the APL language.
    </p>
    <p>
      As of 1961, all algorithms with acceptable speed are using
      stacks with various modifications.
      Oettinger expresses a hope:
    </p>
    <blockquote>
      The development of a theory of pushdown algorithms should
      hopefully lead to systematic techniques for generating
      algorithms satisfying given requirements to replace
      the ad hoc invention of each new algorithm.<a id="footnote-15-ref" href="#footnote-15">[15]</a>
    </blockquote>
    <p>Oettinger defines 4 languages, all of sets of strings.<a id="footnote-16-ref" href="#footnote-16">[16]</a>
      Oettinger's pushdown stores
      will eventually be called
      deterministic pushdown automata (DPDA's) and
      become the basis of a model of language.
      Oettinger hopes this model will
      be an adequate basis both for natural language
      (Russian translation is Oettinger's area of research)
      and for computing languages like ALGOL.
      For Russian translation,
      DPDA's will prove totally inadequate.
    <p>
    </p>
      But as a hoped-for basis for a theory of computer language parsing,
      Oettinger DPDA's have a much longer life.
      The focus of Parsing Theory research over the next ten years will be
      discovering a theory of DPDA-based parsing.
      And once discovered,
      this theory will dominate the academic literature
      on parsing for a much longer time.
    </p>
    <h2>Knuth 1965</h2>
    <p>DPDA-based parsing theory soon attracts the attention of Computer Science's
    best technical mathematician, Donald Knuth.
    In his pivotal LR(k) paper,<a id="footnote-17-ref" href="#footnote-17">[17]</a>
      Knuth sets out a theory that explains
      all the "tricks"<a id="footnote-18-ref" href="#footnote-18">[18]</a>
      used for efficient parsing up to that time.
      Knuth sets out a comprehensive theory of stack-based
      parsing algorithms.
      For a start, Knuth shows that stack-based parsing is
      equivalent to a new and unexpected class of grammars
      LR(k), and he provides a parsing algorithm for them.
    </p>
    <p>
      Knuth's new algorithm might be expected to be "the one to rule
      them all".
      Unfortunately, while deterministic and linear,
      is not practical -- it requires huge tables well beyond
      the memory capabilities of the time.
      This does not suggest to Knuth that the DPDA-based model
      is inappropriate as a model of practical parsing --
      instead it suggest to him, and to the field,
      that the boundary of practical parsing lies inside the
      LR(k) grammars.
    </p>
    <p>
    The idea that the solution to the parsing problem must be
    DPDA-based is not without foundation.
    In 1965, the limits of computer technology are severe.
    For practitioners,
    any parsing technique that required more than a DPDA --
    that is, more than state
    machine and a stack,
    was not likely to happen.
    After all,
    four years earlier, stacks had been bleeding edge.
    </p>
    <p>
    To be sure,
      Knuth, in his program for further research<a id="footnote-19-ref" href="#footnote-19">[19]</a>,
      does suggests investigation of parsers for superclasses
      of LR(k).
      He even describes his own superclass of LR(k):
      LR(k,t), which is LR(k) more aggressive lookahead.
      But he is clearly unenthusiastic about LR(k,t)<a id="footnote-20-ref" href="#footnote-20">[20]</a>
      It is reasonable to suppose,
      that Knuth is even more negative about more general approaches that
      he does not bother to mention.<a id="footnote-21-ref" href="#footnote-21">[21]</a>
    </p>
    <p>
      In any case, those reading Knuth's LR(k) focused almost
      exclusively on his suggestions for research within the DPDA-based
      model.
      These included grammars rewrites;
      streamlining of the LR(k) tables;
      or research into LR(k) subclasses.
      It is LR(k) subclassing that will receive the most attention.
    </p>
    <p>
      Knuth is certainly aware that DPDA determinism and
      linear time behavior are not the same thing.<a id="footnote-22-ref" href="#footnote-22">[22]</a>
      An algorithm can be more powerful than a DPDA,
      while still being linear.
      But linearity is a stand-in for "practical",
      and, with his discovery that even DPDA-based
      algorithms can be impractical,
      Knuth, and the research community,
      decide that it is extremely unlikely than more
      powerful computing models will also be faster in
      practice.
    </p>
    <p>
       Why was such a powerful skepticism based on the results for one
       computing model of computing?
       Stacks, as we now call them, are a natural model of computing,
       so it is reasonable to think they form a step on the hierarchy
       of tradeoffs of power against practical speed.
       But a very important was the proof that LR(k) grammars were
       "equivalent" to DPDA's.
       And central to the acceptance of this proof as relevant
       was a confusion about the use of the term "language".
    </p>
    <p>With his 1965, Knuth, Computer Science's greatest mathematician 
    disposes of the DPDA problem.
    Knuth's exhausting and exhilarating 39-page
    demonstration of mathematical virtuousity
    almost "runs the board"
    of open problems in parsing,
    and his section on "open problems" is read
    as a definitive program for further research.
    </p>
    <p>
    Knuth does not quite solve the problem,
    but he sets the framework within which a solution
    will be found --
    or so it seems.
    Because, while Knuth's math is correct,
    a confusion about the term "language"
    makes his conclusions unreliable.
    </p>
    <h2>The term "language" as of 1965</h2>
    <p>
    Knuth defines language as follows:
    </p>
    <blockquote>
    The language defined by G is<br>
    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
    { &alpha; | S => &alpha; and &alpha; is a string over T }<br>
    namely, the set of all terminal string derivable from S by using
    the productions of G as substitution rules.<a id="footnote-23-ref" href="#footnote-23">[23]</a>
    </blockquote>
    (Here G is a grammar whose start symbol is S and whose set
    of terminals is T.)
    This is clearly the behavorist definition of language
    translated into set-theoretic terms.
    </p>
    <p>Knuth proves, to the satisfaction of the profession,
    the "equivalence" of LR(k) and DPDA's.
    LR(k) is a class of grammars and the DPDA model is of
    a language -- a set of strings.
    At first glance, this is an "apples and oranges" comparison --
    how do you prove the equivalence of a language and a grammar.
    </p>
    <p>Knuth does this by reducing the language and the class grammar
    of grammars to a lowest common denominator --
    a grammar defines a language, so he compares the LR(k) language
    to the DPDA language.
    It takes some impressive mathematics,
    but Knuth is able to show that the two languages are equivalent.
    But note that the question whether LR(k) is an impassable
    barrier for parsing grammars -- not languages.
    </p>
    Punning a class of grammars as a class of languages does not work --
    in fact, as Knuth shows, it produces a considerable amount of magical
    thinking. 
    Using the Knuth algorithm
    <ul>
    <li>Parsing LR(k) grammars for an arbitrary is hopelessly impractical.
    </li>
    <li>Parsing LR(1) grammars is almost practical, but not quite.<a id="footnote-24-ref" href="#footnote-24">[24]</a>
    </li>
    <li>Parsing LR(0) grammars is quite practical.
    </li>
    </ul>
    </p>The problem for Knuth's proof of equivalence is that,
    if you consider languages, LR(1) and LR(k) are equivalent.
    And in fact, both are almost equivalent to LR(0) -- if you add
    an explicit end marker to a language
    (which in most applications is easy to do<a id="footnote-25-ref" href="#footnote-25">[25]</a>)
    then LR(k) = LR(1) = LR(0).
    </p>
    <p>
    That is, in language terms, the hopelessly impractical
    is equivalent to the borderline impractical.
    And these, for most applications, are equivalent to the
    very practical LR(0).
    When thinking in terms of languages,
    we can transport ourselves across the
    same practical/impractical boundary that we are claiming
    to show is, in practice, impassable for grammars.
    </p>
    <p>
    What for languages is reasonable thinking,
    is magical thinking when it comes to grammars.
    This suggests that reasoning based on the equivalence
    of languages may not be helpful for deciding
    what is practical for parsing grammars --
    in fact, it suggests that this kind of reasoning
    could be seriously misleading.
    </p>
    <p>In that light,
    it should be no surprise that,
    in 1991, Joop Leo showed how to extend practical
    parsing well beyond the LR(k) and DPDA models.<a id="footnote-26-ref" href="#footnote-26">[26]</a>
    </p>
    <h2>Comments, etc.</h2>
    <p>
      The background material for this post is in my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3>
    Parsing: a timeline 3.0</a>,
    and this post may be considered a supplement to "Timelime".
    I encourage
    those who want to know more about the story of Parsing Theory
    to look at my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3>
    Parsing: a timeline 3.0</a>.
    For example, "Timeline 3.0" tells the story of the search for a good
    LR(k) subclass,
    and what happened afterwards.
    </p>
    <p>
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
    The well-known <a href="https://en.wikipedia.org/wiki/Design_Patterns">
    <cite>Design Patterns</cite> book</a>
    (aka "the Gang of 4 book")
    has a section on this which call Language-oriented programmer
    its "Interpreter pattern".
    This amply illustrates the main obstacle to use
    of the pattern -- lack of adequate parsing tools.
    I talk much more about this in my two blog posts on 
    the Interpreter pattern:
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/bnf_to_ast.html">
    BNF to AST</a>
    and 
    <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2013/03/interpreter.html">
    The Interpreter Design Pattern</a>.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
      This post takes the form of a timeline, and
      is intended to be incorporated in my
      <a href="https://jeffreykegler.github.io/personal/timeline_v3>.
      Parsing: a timeline</a>.
      The earlier entires in this post borrow heavily from
      <a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html">
	    a previous blog post</a>.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
        Bloomfield, Leonard,
        "A set of Postulates
        for the Science of Language",
        <cite>Language</cite>, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
        Bloomfield 1926, definition 4 on p. 154.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
        Bloomfield, Leonard.
        <cite>Language</cite>.
        Holt, Rinehart and Winston, 1933, p. 140.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
        Harris, Randy Allen,
        <cite>The Linguistics Wars</cite>,
        Oxford University Press, 1993,
        pp 31-34, p. 37.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
        The quote is on p. 114 of
        Chomsky, Noam.
        "Three models for the description of language."
        <cite>IRE Transactions on information theory</cite>,
        vol. 2, issue 3, September 1956, pp. 113-124.
        In case there is any doubt Chomsky's "strings"
        are Bloomfield's utterances,
        Chomsky also calls his strings,
        "utterances".
        For example in Chomsky, Noam,
        <cite>Syntactic Structures</cite>,
        2nd ed.,
        Mouton de Gruyter, 2002, on p. 15:
        "Any grammar of a language will project the finite and somewhat accidental
        corpus of observed utterances to a set (presumably infinite)
        of grammatical utterances."
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
        Chomsky 1956, p. 118, p. 123.
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9">9.
    Chomsky, Noam.
    “A Review of B. F. Skinner’s Verbal Behavior”. <cite>Language</cite>,
    Volume 35, No. 1, 1959, 26-58.
    <a href="https://chomsky.info/1967____/">
    https://chomsky.info/1967____/</a> accessed on 3 June 2018.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10">10.
    See in particular, Section IX of Chomsky 1959.
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11">11.
        Chomsky, Noam.
        <cite>Topics in the Theory of Generative Grammar</cite>.
        De Gruyter, 1978, p. 20.
        (The quote occurs in footnote 7 starting on p. 19.)
 <a href="#footnote-11-ref">&#8617;</a></p>
<p id="footnote-12">12.
        Carpenter, Brian E., and Robert W. Doran.
        "The other Turing machine."
        <cite>The Computer Journal</cite>, vol. 20, issue 3, 1 January 1977, pp. 269-279.
 <a href="#footnote-12-ref">&#8617;</a></p>
<p id="footnote-13">13.
        Samelson, Klaus, and Friedrich L. Bauer. "Sequentielle formelübersetzung." it-Information Technology 1.1-4 (1959): 176-182.
 <a href="#footnote-13-ref">&#8617;</a></p>
<p id="footnote-14">14.
          Oettinger, Anthony.
          "Automatic Syntactic Analysis and the Pushdown Store"
          <cite>Proceedings of Symposia in Applied Mathematics</cite>,
          Volume 12,
          American Mathematical Society, 1961.
 <a href="#footnote-14-ref">&#8617;</a></p>
<p id="footnote-15">15.
        Oettinger 1961, p. 127.
 <a href="#footnote-15-ref">&#8617;</a></p>
<p id="footnote-16">16.
    Oettinger 1961, p. 106.
 <a href="#footnote-16-ref">&#8617;</a></p>
<p id="footnote-17">17.
    Knuth, Donald E.
    "On the translation of languages from left to right."
    <cite>Information and Control</cite>, Volume 8, Issue 6, December 1965, pp. 607-639.
    <a href="https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d">
    https://ac.els-cdn.com/S0019995865904262/1-s2.0-S0019995865904262-main.pdf?_tid=dcf0f8a0-d312-475e-a559-be7714206374&acdnat=1524066529_64987973992d3a5fffc1b0908fe20b1d</a>, accessed 24 April 2018.
 <a href="#footnote-17-ref">&#8617;</a></p>
<p id="footnote-18">18.
      Knuth 1965, p. 607, in the abstract.
 <a href="#footnote-18-ref">&#8617;</a></p>
<p id="footnote-19">19.
      Knuth 1961, pp. 637-639.
 <a href="#footnote-19-ref">&#8617;</a></p>
<p id="footnote-20">20.
      "Finally, we might mention another generalization of LR(k)"
      (Knuth 1965, p. 638); and
      "One might choose to call this left-to-right translation,
      although we had to back up a finite amount."
      (p. 639).
 <a href="#footnote-20-ref">&#8617;</a></p>
<p id="footnote-21">21.
      Knuth's skepticism for more general Chomskyan approaches
      is suggested by his own plans for his (not yet released) Chapter
      12 of the <cite>Art of Computer Programming</cite>,
      in which he planned to use pre-Chomskyan bottom-up methods. (See
      Knuth, Donald E., "The genesis of attribute grammars",
      <cite>Attribute Grammars and Their Applications</cite>,
      Springer, September 1990, p. 3.)
 <a href="#footnote-21-ref">&#8617;</a></p>
<p id="footnote-22">22.
        Knuth 1965, p. 607: "execution time at worst
        proportional to the length of the string being parsed."
 <a href="#footnote-22-ref">&#8617;</a></p>
<p id="footnote-23">23.
    Knuth 1965, p. 608.
 <a href="#footnote-23-ref">&#8617;</a></p>
<p id="footnote-24">24.
    Given the capacity of computer memories in 1965,
    LR(1) was clearly impractical.
    Today, that could be reconsidered, but LR(1) is still restrictive
    and has poor error-handling,
    so few practitioners have bothered with it.
 <a href="#footnote-24-ref">&#8617;</a></p>
<p id="footnote-25">25.
    Some parsing applications, such as those which receive their input "on-line",
    can not determine the size of their input in advance.
    For these applications adding an end marker to their input is
    inconvenient or impossible.
 <a href="#footnote-25-ref">&#8617;</a></p>
<p id="footnote-26">26.
 Joop M. I. M.
 "A general context-free parsing algorithm running in linear time on every LR (k) grammar without using lookahead."
 <cite>Theoretical computer science</cite>, Volume 82, Issue 1, 22 May 1991, pp. 165-176.
 <a href="https://www.sciencedirect.com/science/article/pii/030439759190180A">
 https://www.sciencedirect.com/science/article/pii/030439759190180A</a>, accessed 24 April 2018.
 <a href="#footnote-26-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 11:35 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/knuth_1965.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 28 May 2018</h3>
<br />
<center><a name="chomsky_1956"> <h2>Is a language just a set of strings?</h2> </a>
</center>
<html>
  <head>
  </head>
  <body>
    <blockquote>
	But to my mind, though I am native here<br>
	And to the manner born, it is a custom<br>
	More honor’d in the breach than the observance.<a id="footnote-1-ref" href="#footnote-1">[1]</a>
    </blockquote>
    <h2>Chomsky's "Three Models" paper</h2>
    <p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      Important papers produce important mistakes.
      A paper can contain a great many errors,
      and they will have no effect if the paper is ignored.
      On the other hand,
      even the good methods of
      a great paper can go badly wrong
      when its methods
      outlive the reasons for using them.
    </p>
    <p>
      Chomsky's "Three Models" paper<a id="footnote-2-ref" href="#footnote-2">[2]</a>
      is about as influential
      as a paper can get.
      Just 12 pages,
      it's the paper in which the most-cited scholar of our
      time first outlined his ideas.
      Even at the time,
      linguists described its effect on their field as
      "Copernician".<a id="footnote-3-ref" href="#footnote-3">[3]</a>
      Bringing new rigor into what had been seen as a "soft"
      science, it turned lots of heads outside linguistics.
      It belongs on anyone's list of the most important scientific papers ever.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
    </p>
    <p>
      Given its significance,
      it is almost incidental that
      "Three models" is also the foundation paper of computer Parsing Theory,
      the subject of these blog posts.
      Chomsky does not consider himself a computer scientist
      and, after founding our field,
      has paid little attention to it.
      But in fact,
      the Chomskyan model has been even more dominant
      in computer parsing than in Chomsky's own
      field of linguistics.
    </p>
    <p>
      "Three Models" places Chomksy among the great mathematicians
      of all time.
      True, the elegance and rigor of Chomsky's proofs
      better befit a slumming linguist
      than they would a professional mathematician.
      But at its heart,
      mathematics is not a technical field,
      or even about problem-solving --
      at its most fundamental,
      it is about framing problems so that they
      <b>can</b> be solved.
      And Chomsky's skill at framing problems is astonishing.
    </p>
    <h2>A brilliant simplification</h2>
    <p>
      In 1956,
      Chomsky had a new approach to linguistics,
      and wanted to prove that his approach to language
      did things that
      the previous approach,
      based on finite-state models,
      could not.
      ("Finite-state" models, also known as Markov chains,
      are the predecessors of the regular expressions
      of today.)
      Brilliantly,
      Chomsky sets out to do this with extremely minimal definition of what
      a language is.
    </p>
    <blockquote>
      By a language then, we shall mean a set (finite or infinite) of
      sentences, each of finite length, all constructed from a finite
      alphabet of sysbols.  If A is an alphabet, we shall say that
      anything formed by concatenating the symbols of A is a string in
      A. By a grammar of the language L we mean a device of some sort that
      produces all of the strings that are sentences of L and only these.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </blockquote>
    <p>Yes, you read that right --
    Chomsky uses a definition of language which has nothing to
    do with language actually meaning anything.
    A language, for the purposes of the math in "Three Models",
    is nothing but a list of strings.
    Similarly, a grammar is just something that enumerates
    those strings.
    The grammar does not have to provide any clue as to what
    the strings might mean.
    </p>
    <p>
        For example, Chomsky would require of a French grammar that one of the
	strings that it lists be
    </p>
    <blockquote>
      (42) Ceci n'est pas une phrase vraie.
    </blockquote>
    <p>But for the purposes of his demonstration,
    Chomsky does not require of his "grammar" that it
    give us any guidance as to what sentence (42) might mean.
    <p>
    Chomsky shows that there are English sentences that his "grammar" would
    list,
    and which a finite-state "grammar" would not list.
    Clearly if the finite-state grammar cannot even produce a sentence
    as one of a list,
    it is not adequate as a model of that language,
    at least as far as that sentence goes.
    <p>
    </p>
    Chomsky shows that there is,
    in fact,
    a large, significant class of sentences that
    his "grammars" can list,
    but which the finite-state grammars cannot list.
    Chomsky presents this as
    very strong evidence that
    his grammars will make better models
    of language
    than finite-state grammars can.
    </p>
    <h2>Other considerations</h2>
    <p>
    In addition to simplifying the math, Chomsky has two other good
    reasons to avoid dealing with meaning.
    A second reason is that
    semantics is a treacherously dangerous field of study.
    If you can make your point,
    and don't have to drag in semantics,
    you are crazy to do otherwise.
    Sentence (42), above,
    is just one example of the pitfalls
    that await those tackle who semantic issues.
    It echoes
    <a href="https://en.wikipedia.org/wiki/The_Treachery_of_Images">a famous Magritte<a>
    and translates to "This is not a true sentence".
    </p>
    <p>
    A third reason is that most linguists of Chomsky's time
    were Bloomfieldians.
    Bloomfield defined language as follows:
    </p>
    <blockquote>
    The totality of utterances that can be made in a speech
    community is the <b>language</b>
    of that speech-community.<a id="footnote-6-ref" href="#footnote-6">[6]</a>
    </blockquote>
    <p>
    Bloomfield says "totality" instead of "set"
    and "utterances" instead of "strings",
    but for our purposes in this post the idea is the same --
    the definition is without regard to the meaning
    of the members of the set.<a id="footnote-7-ref" href="#footnote-7">[7]</a>
    </p>
    <p>
    Bloomfield's omission of semantics is not accidental.
    Bloomfield wanted to establish linguistics as a
    science, and for Bloomfield
    claiming to know the meaning of
    a sentence was dangerously close to
    claiming to be able to read minds.
    You cannot base your work on mind-reading and expect people to
    believe that you are doing science.
    Bloomfield therefore suggested avoiding,
    totally if possible,
    any discussion of semantics.
    Most readers of Chomsky's paper in 1956 were Bloomfieldians --
    Chomsky has studied under a Bloomfieldian,
    and originally was seen as one.<a id="footnote-8-ref" href="#footnote-8">[8]</a>
    By excluding semantics from his own model of language,
    Chomsky was making his paper maximally acceptable to
    his readership.
    </p>
    <h2>Semantics sneaks back in</h2>
    <p>
    But you did not have to read Chomsky's mind,
    or predict the future,
    to see that Chomsky
    was a lot more interested in semantics than
    Bloomfield was.
    Already in "Three Models",
    he is suggesting that his model is superior to
    its predecessors,
    because his model,
    when an utterance is ambiguous,
    produces multiple derivations to reflect that.
    Even better, these multiple derivations "look" like natural representations
    of the difference between meanings.<a id="footnote-9-ref" href="#footnote-9">[9]</a>
    These insights,
    which dropped effortlessly out of Chomsky's grammars,
    were well beyond what the finite-state models were providing.
    </p>
    <p>
    By itself,
    Chomsky's argument, that his grammars were better
    because they could list more sentences,
    might have carried the day.
    With the demonstration that his grammars could do
    more than list sentences,
    but also could proivde insight into the structure and semantics
    of sentences,
    Chomsky's case was compelling.
    Young linguists wanted theoretical tools with this
    kind of power and those few
    older linguists not convinced struggled to find
    reasons why the young linguists could not
    have what they wanted.
    </p>
    <p>
    In later years,
    Chomsky made it quite clear what his position was:
    <blockquote>
    [...] it would be absurd to develop
    a general syntactic theory
    without assigning an absolutely
    crucial role to semantic considerations,
    since obviously the necessity to support
    semantic interpretation is one of the primary
    requirements
    that the structures
    generated by the syntactic component of a grammar
    must meet.<a id="footnote-10-ref" href="#footnote-10">[10]</a>
    </blockquote>
    Compare this to Bloomfield:
    <blockquote>
    The statement of meanings is therefore the weak point in
    language-study, and will remain so until human knowledge
    advances very far beyond its present state. In practice, we define the
    meaning of a linguistic form, wherever we can, in terms of some
    other science.<a id="footnote-11-ref" href="#footnote-11">[11]</a>
    </blockquote>
    It is easy to see why linguists found Chomsky's
    expansion of their horizons irresistable.
    <p>
    </p>
    <h2>The tradition</h2>
    <p>Given the immense prestige of "Three models",
    it is unsurprising that it was closely studied by
    the pioneers of parsing theory.
    Unfortunately, what they picked up was not
    Chomsky's emphasis on the overriding importance
    of semantics,
    but the narrow definition of language
    that Chomsky had adopted from Bloomfield
    for tactical purposes.
    In the classic Aho and Ullman 1972 textbook, we have
    </p>
    <blockquote>
    A language over an alphabet &Sigma;
    is a set of strings over an alphabet &Sigma;.
    This definition encompasses almost everyone's notion of a language.<a id="footnote-12-ref" href="#footnote-12">[12]</a>
    </blockquote>
    If this "encompasses" my notion of a language,
    it does so only in the sense that an avalanche encompasses
    a skier.
    </p>
    <p>
    From 1988, thirty years after Chomsky,
    here is another authoritative textbook of Parsing Theory
    defining "language":
    </p>
    <blockquote>
      A set V is an alphabet (or a vocabulary) if it is finite and
      nonempty.
      The elements
      of an alphabet V are called the symbols (or letters or characters) of
      V.
      A language L over V is any subset of the free monoid V*.
      The elements
      of a language L are called sentences of L.<a id="footnote-13-ref" href="#footnote-13">[13]</a>
    </blockquote>
    <p>The language is now that of abstract algebra,
    but the idea is the same -- pure Bloomfield.<a id="footnote-14-ref" href="#footnote-14">[14]</a>
    </p>
    <h2>The problem</h2>
    <p>
    Interesting, you might be saying, that
    some textbook definitions are not everything they could be,
    but is there any effect on the daily practice of
    programming?
    </p>
    <p>
    The languages human beings use with each other
    are powerful,
    varied, flexible and endlessly retargetable.
    The parsers we use to communicate with computers
    are restrictive, repetitive in form,
    difficult to reprogram,
    and prohibitively hard to retarget.
    Is this because humans have a preternatural language ability?
    </p>
    <p>
    Or is there something wrong with the way we
    go about talking to computers?
    How the Theory of Parsing literature defines the term
    "language" may seem
    of only pedantic interest.
    But I will argue that it is a mistake which has everything
    to do with the limits of modern computer languages.
    <p>
    What is the problem with defining a language as a set of strings?
    Here is one example of how the textbook definition
    affects daily practice.
    Call one grammar <tt>SENSE</tt>:
    </p>
    <pre id="g-structure-op"><tt>
      SENSE ::= E
      E ::= E + T
      E ::= T
      T ::= T * P
      T ::= P
      P ::= number
    </tt></pre>
    <p>
    And call another grammar <tt>STRING</tt>:
    </p>
    <pre id="g-string-op"><tt>
      STRING ::= E
      E  ::= P OP E
      OP ::= '*'
      OP ::= '+'
      P  ::= number
    </tt></pre>
    <p>
    If you define a language as a set of strings,
    both
    <tt>SENSE</tt>
    and <tt>STRING</tt>
    recognize the same language.
    But it's a very different story if you
    take the intended meaning as
    that of traditional arithmetic expressions,
    and consider
    the meaning of the two grammars.
    </p>
    <p>
    <tt>SENSE</tt>
    recognizes the associativity and precedence of the two operators --
    the parse tree it produces could be used directly to evaluate an arithmetic
    expression and the answer would always be correct.
    The parse tree that <tt>STRING</tt> produces, if evaluated directly,
    will very often produce a wrong answer -- it does not capture
    the structure of an arithmetic expression.
    In order to produce correct results,
    the output of <tt>STRING</tt> could be put through a second phase,
    but that is the point --
    <tt>STRING</tt> left crucial parts of the job of parsing undone,
    and either some other logic does the job <tt>STRING</tt> did not do,
    or a wrong answer results.
    <p>
    It is much easier to write a parser for <tt>STRING</tt>
    than it is for <tt>SENSE</tt>.
    Encouraged by a theory that minimizes the
    difference,
    many implementations attempt to make do with <tt>STRING</tt>.
    </p>
    <p>
    But that is not the worst of it.
    The idea that a language is a set of strings
    has guided research,
    and steered it away from the most promising lines.
    How, I hope to explain in the future.<a id="footnote-15-ref" href="#footnote-15">[15]</a>
    </p>
    <h2>Comments, etc.</h2>
    <p>
      The background material for this post is in my
    <a href="https://jeffreykegler.github.io/personal/timeline_v3">
    Parsing: a timeline 3.0</a>,
    and this post may be considered a supplement to "Timelime".
      To learn about Marpa,
      my Earley/Leo-based parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
	<cite>Hamlet</cite>, Act I, scene iv.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
      Chomsky, Noam.
      "Three models for the description of language."
      <cite>IRE Transactions on information theory</cite>,
      vol. 2, issue 3, September 1956, pp. 113-124.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
	Harris, Randy Allen,
	<cite>The Linguistics Wars</cite>,
	Oxford University Press, 1993,
	pp 33.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
      In this post I am treating the "Three models" paper
      as the "first"
      work of Chomskyan linguistics.
      Other choices can be justified.
      The next year, 1957,
      Chomsky published a book covering the same material: <cite>Syntactic Structures</cite>.
      <cite>Syntactic Structures</cite>
      was much more accessible,
      and attracted much more attention --
      the Chomskyan revolution did not really begin before
      it came out.
      On the other hand,
      both of these draw their material from Chomsky's
      1000-page
      <cite>Logical Structure of Linguistic Theory</cite>,
      which was completed in June 1955.
      But <cite>Logical Structure of Linguistic Theory</cite>
      was not published until 1975
      and then only in part.
      (See
      <a href="https://www.journals.uchicago.edu/doi/full/10.1086/686177">
      Radick, Gregory,
      "The Unmaking of a Modern Synthesis: Noam Chomsky, Charles Hockett, and the Politics of Behaviorism, 1955–1965"
      </a>.)
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
      Chomsky 1956, p. 114.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
    Bloomfield, Leonard,
    "A set of Postulates
    for the Science of Language",
    <cite>Language</cite>, Vol. 2, No. 3 (Sep., 1926), pp. 153-164.
    The quote is definition 4 on p. 154.
 <a href="#footnote-6-ref">&#8617;</a></p>
<p id="footnote-7">7.
    In case there is any doubt as to the link between
    the Chomsky and Bloomfield definitions,
    Chomsky also calls his strings,
    "utterances".
    See Chomsky, Noam, <cite>Syntactic Structures</cite>,
    2nd ed.,
    Mouton de Gruyter, 2002, p. 49
 <a href="#footnote-7-ref">&#8617;</a></p>
<p id="footnote-8">8.
    Harris 1993, pp 31-34, p. 37.
 <a href="#footnote-8-ref">&#8617;</a></p>
<p id="footnote-9">9.
    Chomsky 1956, p. 118, p. 123.
 <a href="#footnote-9-ref">&#8617;</a></p>
<p id="footnote-10">10.
    Chomsky, Noam.
    <cite>Topics in the Theory of Generative Grammar</cite>.
    De Gruyter, 1978, p. 20.
    (The quote occurs in footnote 7 starting on p. 19.)
 <a href="#footnote-10-ref">&#8617;</a></p>
<p id="footnote-11">11.
    Bloomfield, Leonard.
    <cite>Language</cite>.
    Holt, Rinehart and Winston, 1933, p. 140.
 <a href="#footnote-11-ref">&#8617;</a></p>
<p id="footnote-12">12.
    Aho, Alfred V., and Jeffrey D. Ullman.
    <cite>The theory of parsing, translation, and compiling</cite>.
    Vol. 1. Prentice-Hall, 1972, p. 16.
 <a href="#footnote-12-ref">&#8617;</a></p>
<p id="footnote-13">13.
      Sippu, Seppo and Soisalon-Soininen, Eljas.
      <cite>Parsing Theory</cite>, Volume I,
      Springer-Verlag, 1988,
      p. 11.
 <a href="#footnote-13-ref">&#8617;</a></p>
<p id="footnote-14">14.
    A welcome errancy from tradition, however, arrives with
    Grune, D. and Jacobs, C. J. H., <cite>Parsing Techniques: A Practical Guide</cite>,
    2nd edition, Springer, 2008.
    On pp. 5-7, they attribute the traditional "set of strings" definition
    to "formal linguistics".
    They
    point out that the computer scientist requires a grammar to
    not only list a set of strings, but provide a
    "structure" for each of them.<br><br>
    As an aside,
    Grune and Jacobs often depart from the "just stick to the math"
    approach taken by other textbooks parsing theory.
    They often give the history and motivation behind the math.
    My own work owes much to them.
 <a href="#footnote-14-ref">&#8617;</a></p>
<p id="footnote-15">15.
    Readers who want to peek ahead can look at my
    <a href="https://jeffreykegler.github.io/personal/timeline_v3">
    Parsing: a timeline 3.0</a>.
    The tale is told there is from a somewhat different point of view,
    but no reader of "Timeline" will be much surprised by where
    I take this line of thought.
 <a href="#footnote-15-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 19:59 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/chomsky_1956.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Mon, 14 May 2018</h3>
<br />
<center><a name="fast_power"> <h2>Parsers and Useful Power</h2> </a>
</center>
<html>
  <head>
  </head>
  <body>
    <p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      What do parser
      users want?
      What makes a parser<a id="footnote-1-ref" href="#footnote-1">[1]</a>
      successful?
      In this post I will look at
      one aspect of
      that question,
      in light of an episode in
      the history of parsing.
    </p>
    <h2>Irons 1961</h2>
    <p>
      The first paper
      fully describing a parser was Irons 1961<a id="footnote-2-ref" href="#footnote-2">[2]</a>.
      The Irons parser was what is called "general",
      meaning that it can parse all of
      the "context-free grammars".
      That makes it
      far more powerful than most parsers
      in practical use today.
    </p>
    <p>
      But the Irons algorithm was not always fast in the general case.
      Irons 1961 used backtracking
      to achieve its power,
      so it would go exponential for many useful grammars.
    </p>
    <p>
      Among the grammars Irons 1961 could not parse quickly
      were those containing the all-important arithmetic expressions.
      Irons 1961 gave way to recursive descent.
    </p>
    <p>
      Recursive descent (RD) in its pure form,
      could not parse arithmetic expressions at all,
      but it could be customized with procedural code.
      That is, it could call specialized parsers which were
      reliably fast for specific sections of the input.
      The Irons parser was declarative,
      and not easy to cusomtize.
    </p>
    <h2>Raw power versus useful power</h2>
    <p>
      The contest between Irons parsing and recursive descent took place
      before the theory for analyzing algorithms was fully formed.<a id="footnote-3-ref" href="#footnote-3">[3]</a>
      In retrospect, we can say that,
      except in specialized uses,
      an acceptable parser for most practical uses
      must be linear or quasi-linear.<a id="footnote-4-ref" href="#footnote-4">[4]</a>
      That is,
      the "useful power" of a parser is the class
      of grammars that it will parse in quasi-linear time.<a id="footnote-5-ref" href="#footnote-5">[5]</a>
    </p>
    <p>
      Useful power turns out to be more important,
      in practice,
      than raw power.
      Recursive descent won out over the
      Irons algorithm because,
      while the Irons algorithm had vastly more raw power,
      RD had slightly more "useful power".
    <p>
      It is nice to have raw power as well -- it means an algorithm can take on some specialized tasks.
      And raw power provides a kind of "soft failure" debugging mode for grammars with,
      for example, unintended ambiguities.
      But, in the eyes of the programming community, the more important measure of a parser
      is its useful power -- the class of grammars that it will parse at quasi-linear speed.
    </p>
    <h2>Stating the obvious?</h2>
    <p>
    That useful power is more important than raw power may seem,
    in retrospect,
    obvious.
    But in fact, it remains a live issue.
    In practice raw power and useful power are often confused.
    The parsing literature is not always as helpful as it could be:
    it can be hard to determine what the
    useful power of an algorithm is.
    </p>
    <p>
      And the Irons experiment with raw power is often repeated,
      in hopes of a different result.
      Very often,
      a new algorithm is a hybrid of two others:
      an algorithm with a lot of raw power,
      but which can go quadratic or worse;
      and a fast algorithm which lacks power.
      When the power of the fast algorithm fails,
      the hybrid algorithm switches over
      to the algorithm with raw power.
    </p>
    <p>
      It is a sort of
      cross-breeding of algorithms.
      The hope is that the hybrid algorithm has the best
      features of each of its parents.
      This works a lot better in botany than it does in parsing.
      Once you have a successful cross in a plant,
      you can breed from the successful hybrid
      and expect good things to happen.
      In botany,
      the individual crosses can have an extremely high
      failure rate,
      and cross-breeding can still succeed.
      But it's different when you cross algorithms:
      Even after you've succeeded with one parse,
      the next parse from your hybrid is a fresh new toss of the dice.
    </p>
    <h2>References, comments, etc.</h2>
    <p>
      To learn about
      my own parsing project,
      Marpa<a id="footnote-6-ref" href="#footnote-6">[6]</a>,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
    <h2>Footnotes</h2>
<p id="footnote-1">1.
      By "parser" in this post,
      I will mean a programmer's
      most powerful toolbox parser --
      what might be called the "flagship" parser.
      No parser will ever be the right one for all uses.
 <a href="#footnote-1-ref">&#8617;</a></p>
<p id="footnote-2">2.
	For the reference to Irons, see
        <a href="https://jeffreykegler.github.io/personal/timeline_v3">
          V3 of my "Parsing: A Timeline"</a>.
	  The "Timeline" contains the background material for this post.
 <a href="#footnote-2-ref">&#8617;</a></p>
<p id="footnote-3">3.
      Even the term "analysis of algorithms" did not exist until 1969:
      see <a href="https://web.archive.org/web/20160828152021/http://www-cs-faculty.stanford.edu/~uno/news.html">
      Knuth, "Recent News"</a>.
 <a href="#footnote-3-ref">&#8617;</a></p>
<p id="footnote-4">4.
        For more about "linear" and "quasi-linear",
	including definitions,
	see
        <a href="https://jeffreykegler.github.io/personal/timeline_v3">
          V3 of my "Parsing: A Timeline"</a>,
        in particular its 'Term: linear' section.
 <a href="#footnote-4-ref">&#8617;</a></p>
<p id="footnote-5">5.
	While it is clearly the consensus among practitioners and theoreticians
	that, for parsing,
	practical time is quasi-linear or better,
	there are those who argue that worse-than-quasi-linear parsers
	are often the right ones for the job,
	and that research on them has been unwisely neglected.
	The dissenters are not without a case:
	For example, in natural language, while sentences are in theory
	infinite in length, in practice their average size is fixed.
	And while very long difficult-to-parse sentences do occur in some texts, such as
	older ones, it is normal for a human reader
	to have to spend extra time on them.
	So it may be unreasonable to insist that a parsing algorithm be
	quasi-linear in this application.
 <a href="#footnote-5-ref">&#8617;</a></p>
<p id="footnote-6">6.
      Marpa's useful power is LR-regular,
      which properly contains
      every class of grammar in practical use: regular expressions,
      LALR,
      LL(k) for all k,
      LR(k) for all k,
      and the LL-regular grammars.
      <!-- For LLR, see Theorem 3, p. 448 in
      https://ris.utwente.nl/ws/portalfiles/portal/6126669,
      Nijholt, "On the parsing of LL-regular grammars" -->
 <a href="#footnote-6-ref">&#8617;</a></p>
  </body>
</html>
<br />
<p>posted at: 06:01 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/05/fast_power.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Tue, 10 Apr 2018</h3>
<br />
<center><a name="timeline_v3"> <h2>Version 3 of "Parsing: a timeline"</h2> </a>
</center>
<html>
  <head>
  </head>
  <body>
    <p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
      My most popular blog posts by far have been my two versions of "Parsing: a timeline".
      I have just created
      <a href="https://jeffreykegler.github.io/personal/timeline_v3">a
        3rd version</a>,
      which has so many changes
      that it might be considered a new work.
      The new version is less Marpa-centric and several times as long.
      It covers new topics, including combinator and monadic
      parsing, and operator expression parsing.
      And sources are now provided for all material.
    </p>
    <h2>References, comments, etc.</h2>
    <p>
      For more about
      Marpa, my own parsing project,
      there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
<br />
<p>posted at: 15:08 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2018/04/timeline_v3.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
<h3>Tue, 23 Aug 2016</h3>
<br />
<center><a name="timeline2"> <h2>Parsing: an expanded timeline</h2> </a>
</center>
<html>
  <head>
  </head>
  <body><p>
      <!--
      marpa_r2_html_fmt --no-added-tag-comment --no-ws-ok-after-start-tag
      -->
    </p>
    <p><b>The fourth century BCE</b>:
      In India, Pannini creates
      a sophisticated description of the Sanskrit language,
      exact and complete, and including pronunciation.
      Sanskrit
      could be recreated using nothing but Pannini's grammar.
      Pannini's grammar is probably the first formal system of any kind, predating Euclid.
      Even today, nothing like it exists for any other natural language
      of comparable size or corpus.
      Pannini is the object of serious study today.
      But in the 1940's and 1950's Pannini is almost unknown in the West.
      His work has no direct effect on the other events in this timeline.
    </p>
    <p><b>1943</b>:
      Emil Post defines and studies a formal rewriting system using
      productions.
      With this, the process of reinventing Pannini in the West begins.
    </p><p><b>1948</b>:
      Claude Shannon publishes the foundation paper of information theory.
      Andrey Markov's finite state processes are used heavily.
    </p><p><b>1952</b>:
      Grace Hopper writes a linker-loader and
      <a href="https://en.wikipedia.org/wiki/History_of_compiler_construction#First_compilers">
        describes it
        as a "compiler"</a>.
      She seems to be the first person to use this term for a computer program.
      Hopper uses the term
      "compiler" in its original sense:
      "something or someone that brings other things together".
    </p>
    <p><b>1954</b>:
      At IBM, a team under John Backus begins working
      on the language which will be called FORTRAN.
      The term "compiler" is still being used in Hopper's looser sense,
      instead of its modern one.
      In particular, there is no implication that the output of a "compiler"
      is ready for execution by a computer.
      <!-- "http://www.softwarepreservation.org/projects/FORTRAN/paper/Backus-ProgrammingInAmerica-1976.pdf
        pp. 133-134
      -->
      The output of one 1954 "compiler",
      for example, produces relative addresses,
      which need to be translated by hand before a machine can execute them.
    </p>
    <p><b>1955</b>:
      Noam Chomsky is awarded a Ph.D. in linguistics and accepts a teaching post at MIT.
      MIT does not have a linguistics department and
      Chomsky, in his linguistics course, is free to teach his own approach,
      highly original and very mathematical.
    </p>
    <p><b>1956</b>:
      <!-- "Three models" -->
      Chomsky publishes the paper which
      is usually considered the foundation of Western formal language theory.
      The paper advocates a natural language approach that involves
    </p><ul>
      <li>a bottom layer, using Markov's finite state processes;
      </li><li>a middle, syntactic layer, using context-free grammars and
        context-sensitive grammars; and
      </li><li>a top layer, which involves mappings or "transformations"
        of the output of the syntactic layer.
      </li></ul><p>
      These layers resemble, and will inspire,
      the lexical, syntactic and AST transformation phases
      of modern parsers.
      For finite state processes, Chomsky acknowledges Markov.
      The other layers seem to be Chomsky's own formulations --
      Chomsky does not cite Post's work.
    </p>
    <p><b>1957</b>:
      Steven Kleene discovers regular expressions,
      a very handy notation for Markov's processes.
      Regular expressions turn out to describe exactly the mathematical
      objects being studied as
      finite state automata,
      as well as some of the objects being studied as
      neural nets.
    </p>
    <p><b>1957</b>:
      Noam Chomsky publishes
      <b>Syntactic Structures</b>,
      one of the most influential books of all time.
      The orthodoxy in 1957 is structural linguistics
      which argues, with Sherlock Holmes, that
      "it is a capital mistake to theorize in advance of the facts".
      Structuralists start with the utterances in a language,
      and build upward.
    </p>
    <p>
      But Chomsky claims that without a theory there
      are no facts: there is only noise.
      The Chomskyan approach is to start with a grammar, and use the corpus of
      the language to check its accuracy.
      Chomsky's approach will soon come to dominate linguistics.
    </p>
    <p><b>1957</b>:
      Backus's team makes the first FORTRAN compiler
      available to IBM customers.
      FORTRAN is the first high-level language
      that will find widespread implementation.
      As of this writing,
      it is the oldest language that survives in practical use.
      FORTRAN is a line-by-line language
      and its parsing is primitive.
    </p>
    <p><b>1958</b>:
      John McCarthy's LISP appears.
      LISP goes beyond the line-by-line syntax --
      it is recursively structured.
      But the LISP interpreter does not find the
      recursive structure:
      the programmer must explicitly
      indicate the structure herself,
      using parentheses.
    </p><p><b>1959</b>:
      Backus invents a new notation to describe
      the IAL language (aka ALGOL).
      Backus's notation is influenced by his study of Post --
      he seems not to have read Chomsky until later.
      <!-- http://archive.computerhistory.org/resources/text/Oral_History/Backus_John/Backus_John_1.oral_history.2006.102657970.pdf
      p. 25 -->
    </p>
    <p><b>1960</b>:
      Peter Naur
      improves the Backus notation
      and uses it to describe ALGOL 60.
      The improved notation will become known as Backus-Naur Form (BNF).
    </p><p><b>1960</b>:
      The ALGOL 60 report
      specifies, for the first time, a block structured
      language.
      ALGOL 60 is recursively structured but the structure is
      implicit -- newlines are not semantically significant,
      and parentheses indicate syntax only in a few specific cases.
      The ALGOL compiler will have to find the structure.
      It is a case of 1960's optimism at its best.
      As the ALGOL committee is well aware, a parsing
      algorithm capable
      of handling ALGOL 60 does not yet exist.
      But the risk they are taking will soon pay off.
    </p>
    <p><b>1960</b>:
      A.E. Gleenie publishes his description of a compiler-compiler.
      <!-- http://www.chilton-computing.org.uk/acl/literature/reports/p024.htm -->
      Glennie's "universal compiler" is more of a methodology than
      an implementation -- the compilers must be written by hand.
      Glennie credits both Chomsky and Backus, and observes that the two
      notations are "related".
      He also mentions Post's productions.
      Glennie may have been the first to use BNF as a description of a
      <b>procedure</b>
      instead of as the description of a
      <b>Chomsky grammar</b>.
      Glennie points out that the distinction is "important".
    </p>
    <p><b>Chomskyan BNF and procedural BNF</b>:
      BNF, when used as a Chomsky grammar, describes a set of strings,
      and does
      <b>not</b>
      describe how to parse strings according to the grammar.
      BNF notation, if used to describe a procedure, is a set of instructions, to be
      tried in some order, and used to process a string.
      Procedural BNF describes a procedure first, and a language only indirectly.
    </p>
    <p>
      Both procedural and Chomskyan BNF describe languages,
      but usually
      <b>not the same</b>
      language.
      That is,
    </p><ul>
      <li>Suppose D is some BNF description.
      </li>
      <li>Let P(D) be D interpreted as a procedure,
      </li>
      <li>Let L(P(D)) be the language which the procedure P(D) parses.
      </li>
      <li>Let G(D) be D interpreted as a Chomsky grammar.
      </li>
      <li>Let L(G(D)) be the language which the grammar G(D) describes.
      </li>
      <li>Then, usually, L(P(D)) != L(G(D)).
      </li>
    </ul>
    <p>
      The pre-Chomskyan approach,
      using procedural BNF,
      is far more natural
      to someone trained as a computer programmer.
      The parsing problem appears to the programmer in the form of
      strings to be parsed,
      exactly the starting point of procedural BNF
      and pre-Chomsky parsing.
    </p>
    <p>
      Even when the Chomskyan approach is pointed out,
      it does not at first seem very attractive.
      With the pre-Chomskyan approach,
      the examples of the language
      more or less naturally lead to a parser.
      In the Chomskyan approach
      the programmer has to search for
      an algorithm to parse strings according to his grammar --
      and the search for good algorithms to parse
      Chomskyan grammars has proved surprisingly
      long and difficult.
      Handling
      semantics is more natural with a Chomksyan approach.
      But, using captures, semantics
      can be added to a pre-Chomskyan parser
      and, with practice, this seems natural enough.
    </p>
    <p>
      Despite the naturalness of the pre-Chomskyan approach
      to parsing, we will find that the first fully-described
      automated parsers are Chomskyan.
      This is a testimony to Chomsky's influence at the time.
      We will also see that Chomskyan parsers
      have been dominant ever since.
    </p>
    <p><b>1961</b>:
      In January,
      Ned Irons publishes a paper describing his ALGOL 60
      parser.
      It is the first paper to fully describe any parser.
      The Irons algorithm is Chomskyan and top-down
      with a "left corner" element.
      The Irons algorithm
      is general,
      meaning that it can parse anything written in BNF.
      It is syntax-driven (aka declarative),
      meaning that the parser is
      actually created from the BNF --
      the parser does not need
      to be hand-written.
    </p>
    <p><b>1961</b>:
      Peter Lucas publishes the first
      description of a purely top-down parser.
      This can be considered to be recursive descent,
      though in Lucas's
      paper the algorithm has a
      syntax-driven implementation, useable only for
      a restricted class of grammars.
      Today we think of recursive descent as a methodology for
      writing parsers by hand.
      Hand-coded approaches became more popular
      in the 1960's due to three factors:
    </p>
    <ul>
      <li>
        Memory and CPU were both extremely limited.
        Hand-coding paid off, even when the gains were small.
      </li>
      <li>
        Non-hand coded top-down parsing,
        of the kind Lucas's syntax-driven
        approach allowed, is a very weak parsing technique.
        It was (and still is) often necessary
        to go beyond its limits.
      </li>
      <li>
        Top-down parsing is intuitive -- it essentially means calling
        subroutines.
        It therefore requires little or
        no knowledge of parsing theory.
        This makes it a good fit for hand-coding.
      </li>
    </ul>
    <p><b>1963</b>:
      L. Schmidt, Howard Metcalf, and Val Schorre present papers
      on syntax-directed compilers at a Denver conference.
      <!-- Schorre 1964, p. D1.3-1 -->
    </p>
    <p><b>1964</b>:
      Schorre publishes a paper on the Meta II
      "compiler writing language",
      summarizing the papers of the 1963 conference.
      Schorre cites both Backus and Chomsky as sources
      for Meta II's notation.
      Schorre notes
      that his parser
      is "entirely different" from that of Irons 1961 --
      in fact it is pre-Chomskyan.
      Meta II is a template, rather
      than something that readers can use,
      but in principle it can be turned
      into a fully automated compiler-compiler.
      <!-- Schorre 1964, p. D1.3-1
    http://ibm-1401.info/Meta-II-schorre.pdf
    -->
    </p><p><b>1965</b>:
      Don Knuth invents LR parsing.
      The LR algorithm is deterministic,
      Chomskyan and bottom-up,
      but it is not thought to be practical.
      Knuth is primarily interested
      in the mathematics.
    </p>
    <p><b>1968</b>: Jay Earley invents the algorithm named after him.
      Like the Irons algorithm,
      Earley's algorithm is Chomskyan, syntax-driven and fully general.
      Unlike the Irons algorithm, it does not backtrack.
      Earley's algorithm is both top-down and bottom-up at once --
      it uses dynamic programming and keeps track of the parse
      in tables.
      Earley's approach makes a lot of sense
      and looks very promising indeed,
      but there are three serious issues:
    </p>
    <ul>
      <li>First, there is a bug in the handling of zero-length rules.
      </li>
      <li>Second, it is quadratic for right recursions.
      </li>
      <li>Third, the bookkeeping required to set up the tables is,
        by the standards of 1968 hardware, daunting.
      </li>
    </ul>
    <p><b>1969</b>:
      Frank DeRemer describes a new variant of Knuth's LR
      parsing.
      DeRemer's LALR algorithm requires only
      a stack and a state table of quite
      manageable size.
      LALR looks practical.
    </p>
    <p><b>1969</b>:
      Ken Thompson writes the "ed" editor as one of the first components
      of UNIX.
      At this point, regular expressions are an esoteric mathematical formalism.
      Through the "ed" editor and its descendants,
      regular expressions will become
      an everyday
      part of the working programmer's toolkit.
    </p>
    <p><b>Recognizers</b>:
      In comparing algorithms, it can be important to keep in mind whether
      they are recognizers or parsers.
      A
      <b>recognizer</b>
      is a program which takes a string and produces a "yes"
      or "no" according to whether a string is in part of a language.
      Regular expressions are typically used as recognizers.
      A
      <b>parser</b>
      is a program which takes a string and produces a tree reflecting
      its structure according to a grammar.
      The algorithm for a compiler clearly must be a parser, not a recognizer.
      Recognizers can be, to some extent,
      used as parsers by introducing captures.
    </p><p><b>1972</b>:
      Alfred Aho and Jeffrey Ullman
      publish a two volume textbook summarizing the theory
      of parsing.
      This book is still important.
      It is also distressingly up-to-date --
      progress in parsing theory slowed dramatically
      after 1972.
      Aho and Ullman describe
      a straightforward fix to the zero-length rule bug in Earley's original algorithm.
      Unfortunately, this fix involves adding even more bookkeeping to Earley's.
    </p>
    <p><b>1972</b>:
      Under the names TDPL and GTDPL,
      Aho and Ullman investigate
      the non-Chomksyan parsers in
      the Schorre lineage.
      They note that
      "it can be quite difficult to determine
      what language is defined by a TDPL parser".
      That is,
      GTDPL parsers do whatever they do,
      and that whatever is something
      the programmer in general will not be able to describe.
      The best a programmer can usually do
      is to create a test suite and fiddle with the GTDPL description
      until it passes.
      Correctness cannot be established in any stronger sense.
      GTDPL is an extreme form of
      the old joke that "the code is the documentation" --
      with GTDPL nothing
      documents the language of the parser,
      not even the code.
    </p>
    <p>
      GTDPL's obscurity buys nothing in the way of additional parsing
      power.
      Like all non-Chomskyan parsers,
      GTDPL is basically a extremely powerful recognizer.
      Pressed into service as a parser, it is comparatively weak.
      As a parser, GTDPL
      is essentially equivalent to Lucas's 1961 syntax-driven
      algorithm,
      which was in turn a restricted form of recursive descent.
    </p>
    <p>
      At or around this time,
      rumor has it
      that the main line of development for GTDPL parsers
      is classified secret by the US government.
      <!-- http://www.wikiwand.com/en/Talk:Metacompiler/Archive_2 -->
      GTDPL parsers have the property that even small changes
      in GTDPL parsers can be very labor-intensive.
      For some government contractors,
      GTDPL parsing provides steady work for years to come.
      Public interest in GTDPL fades.
    </p>
    <p><b>1975</b>:
      Bell Labs converts its C compiler from hand-written recursive
      descent to DeRemer's LALR algorithm.
    </p>
    <p><b>1977</b>:
      The first "Dragon book" comes out.
      This soon-to-be classic textbook is nicknamed after
      the drawing on the front cover,
      in which a knight takes on a dragon.
      Emblazoned on the knight's lance are the letters "LALR".
      From here on out,
      to speak lightly of LALR will be to besmirch the escutcheon
      of parsing theory.
    </p>
    <p><b>1979</b>: Bell Laboratories releases Version 7 UNIX.
      V7 includes what is, by far,
      the most comprehensive, useable and easily available
      compiler writing toolkit yet developed.
    </p>
    <p><b>1979</b>:
      Part of the V7 toolkit is Yet Another Compiler Compiler (YACC).
      YACC is LALR-powered.
      Despite its name, YACC is the first compiler-compiler
      in the modern sense.
      For some useful languages, the process of going from
      Chomskyan specification to executable is fully automated.
      Most practical languages,
      including
      the C language
      and YACC's own input language,
      still require manual hackery.
      Nonetheless,
      after two decades of research,
      it seems that the parsing problem is solved.
    </p>
    <p><b>1987</b>:
      Larry Wall introduces Perl 1.
      Perl embraces complexity like no previous language.
      Larry uses YACC and LALR very aggressively --
      to my knowledge more aggressively than anyone before
      or since.
    </p>
    <p><b>1991</b>:
      Joop Leo discovers a way of speeding up right
      recursions in Earley's algorithm.
      Leo's algorithm
      is linear for just about every unambiguous grammar of
      practical interest, and many ambiguous ones as well.
      In 1991 hardware is six orders of magnitude faster
      than 1968 hardware, so that the
      issue of bookkeeping overhead had receded
      in importance.
      This is a major discovery.
      When it comes to speed,
      the game has changed in favor of the Earley algorithm.
    </p>
    <p>
      But Earley parsing is almost forgotten.
      Twenty years will pass
      before anyone writes a practical
      implementation of Leo's algorithm.
    </p>
    <p><b>1990's</b>:
      Earley's is forgotten.
      So everyone in LALR-land is content, right?
      Wrong. Far from it, in fact.
      Users of LALR are making unpleasant discoveries.
      While LALR automatically
      generates their parsers,
      debugging them
      is so hard they could just as easily
      write the parser by hand.
      Once debugged, their LALR parsers are fast for correct inputs.
      But almost all they tell the users about incorrect inputs
      is that they are incorrect.
      In Larry's words, LALR is "fast but stupid".
    </p><p><b>2000</b>:
      Larry Wall decides on a radical reimplementation
      of Perl -- Perl 6.
      Larry does not even consider using LALR again.
    </p>
    <p><b>2002</b>:
      John Aycock and R. Nigel Horspool
      publish their attempt at a fast, practical Earley's parser.
      Missing from it is Joop Leo's improvement --
      they seem not to be aware of it.
      Their own speedup is limited in what it achieves
      and the complications it introduces
      can be counter-productive at evaluation time.
      But buried in their paper is a solution to the zero-length rule bug.
      And this time the solution requires no additional bookkeeping.
    </p>
    <p><b>2004</b>:
      Bryan Ford publishes his paper on PEG.
      Implementers by now are avoiding YACC,
      and it seems
      as if there might soon be no syntax-driven algorithms in practical
      use.
      Ford fills this gap by repackaging the nearly-forgotten GTDPL.
      Ford adds packratting, so that PEG is always linear,
      and provides PEG with an attractive new syntax.
      But nothing has been done to change
      <a href="http://jeffreykegler.github.io/Ocean-of-Awareness-blog/individual/2015/03/peg.html">
        the problematic behaviors</a>
      of GTDPL.
    </p><p><b>2006</b>:
      GNU announces that the GCC compiler's parser has been rewritten.
      For three decades,
      the industry's flagship C compilers have used
      LALR as their parser --
      proof of the claim that LALR and serious
      parsing are equivalent.
      Now, GNU replaces
      LALR with the technology that
      it replaced a quarter century earlier:
      recursive descent.
    </p>
    <p><b>Today</b>:
      After five decades of parsing theory,
      the state of the art seems to be back
      where it started.
      We can imagine someone taking
      Ned Iron's original 1961 algorithm
      from the first paper ever published describing a parser,
      and republishing it today.
      True, he would have to
      translate its code from the mix of assembler and
      ALGOL into something more fashionable, say Haskell.
      But with that change,
      it might look like a breath of fresh air.
    </p>
    <p>
    </p><h3>Marpa: an afterword</h3><p>
      The recollections of my teachers cover most of
      this timeline.
      My own begin around 1970.
      Very early on, as a graduate student,
      I became unhappy with the way
      the field was developing.
      Earley's algorithm looked interesting,
      and it was something I returned to on and off.
    </p>
    <p>
      The original vision of the 1960's was a parser that
      was
    </p>
    <ul>
      <li>efficient,
      </li>
      <li>practical,
      </li>
      <li>general, and
      </li>
      <li>syntax-driven.
      </li>
    </ul>
    <p>
      By 2010 this vision
      seemed to have gone the same way as many other 1960's dreams.
      The rhetoric stayed upbeat, but
      parsing practice had become a series of increasingly desperate
      compromises.
    </p>
    <p>
      But,
      while nobody was looking for them,
      the solutions to the problems encountered in the 1960's
      had appeared in the literature.
      Aycock and Horspool had solved the zero-length rule bug.
      Joop Leo had found the speedup for right recursion.
      And the issue of bookkeeping overhead had pretty much evaporated on its
      own.
      Machine operations are now a billion times faster than in 1968,
      and are probably no longer relevant in any case --
      cache misses are now the bottleneck.
    </p>
    <p>
      The programmers of the 1960's would have been prepared
      to trust a fully declarative Chomskyan parser.
      With the experience with LALR in their collective consciousness,
      modern programmers might be more guarded.
      As Lincoln said, "Once a cat's been burned,
      he won't even sit on a cold stove."
      But I found it straightforward to rearrange the Earley parse engine
      to allow efficient
      event-driven handovers between procedural and syntax-driven
      logic.
      And Earley tables provide the procedural logic with
      full knowledge of the state of the
      parse so far,
      so that
      Earley's algorithm is a better platform
      for hand-written procedural logic than recursive descent.
    </p>
    <h2>References, comments, etc.</h2>
    <p>
      My implementation of Earley's algorithm is called Marpa.
      For more about Marpa, there is the
      <a href="http://savage.net.au/Marpa.html">semi-official web site, maintained by Ron Savage</a>.
      The official, but more limited, Marpa website
      <a href="http://jeffreykegler.github.io/Marpa-web-site/">is my personal one</a>.
      Comments on this post can be made in
      <a href="http://groups.google.com/group/marpa-parser">
        Marpa's Google group</a>,
      or on our IRC channel: #marpa at freenode.net.
    </p>
  </body>
</html>
<br />
<p>posted at: 11:38 |
<a href="http://jeffreykegler.github.com/Ocean-of-Awareness-blog/individual/2016/08/timeline2.html">direct link to this entry</a>
</p>
<div style="color:#38B0C0;padding:1px;text-align:center;">
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
&sect;
</div>
</div>
</div>
<div id="footer" style="border-top:thick solid #38B0C0;clear:left;padding:1em;">
<p>This is Ocean of Awareness's
  new home.  This blog has been hosted at
  <a href="http://blogs.perl.org/users/jeffrey_kegler/">blogs.perl.org</a>
  but I have succumbed to the lure of static blogging.
</div>
	<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-33430331-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>
</body></html>
